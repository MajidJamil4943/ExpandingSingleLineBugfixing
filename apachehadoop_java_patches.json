[
    {
        "commit_id": "37151167ddf12a3fc8da8213187456db1d94c2bb",
        "commit_message": "HDFS-17760. Fix MoveToTrash throws ParentNotDirectoryException when there is a file inode with the same name in the trash (#7514). Contributed by liuguanghua.\n\nSigned-off-by: He Xiaoqiao <hexiaoqiao@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/37151167ddf12a3fc8da8213187456db1d94c2bb",
        "buggy_code": "} catch (FileAlreadyExistsException e) {",
        "fixed_code": "} catch (FileAlreadyExistsException | ParentNotDirectoryException e) {",
        "patch": "@@ -162,7 +162,7 @@ public boolean moveToTrash(Path path) throws IOException {\n           LOG.warn(\"Can't create(mkdir) trash directory: \" + baseTrashPath);\n           return false;\n         }\n-      } catch (FileAlreadyExistsException e) {\n+      } catch (FileAlreadyExistsException | ParentNotDirectoryException e) {\n         // find the path which is not a directory, and modify baseTrashPath\n         // & trashPath, then mkdirs\n         Path existsFilePath = baseTrashPath;"
    },
    {
        "commit_id": "431f29e9100b15014d1207e11541a8f91a0c1a9e",
        "commit_message": "HADOOP-19494: [ABFS][FnsOverBlob] Fix Case Sensitivity Issue for hdi_isfolder metadata (#7496)\n\nContributed by Manish Bhatt\nReviewed by Anmol, Manika, Anuj\n\nSigned off by: Anuj Modi<anujmodi@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/431f29e9100b15014d1207e11541a8f91a0c1a9e",
        "buggy_code": "if (AbfsHttpConstants.XML_TAG_HDI_ISFOLDER.equals(currentNode)) {",
        "fixed_code": "if (AbfsHttpConstants.XML_TAG_HDI_ISFOLDER.equalsIgnoreCase(currentNode)) {",
        "patch": "@@ -207,7 +207,7 @@ public void endElement(final String uri,\n     if (parentNode.equals(AbfsHttpConstants.XML_TAG_METADATA)) {\n       currentBlobEntry.addMetadata(currentNode, value);\n       // For Marker blobs hdi_isFolder will be present as metadata\n-      if (AbfsHttpConstants.XML_TAG_HDI_ISFOLDER.equals(currentNode)) {\n+      if (AbfsHttpConstants.XML_TAG_HDI_ISFOLDER.equalsIgnoreCase(currentNode)) {\n         currentBlobEntry.setIsDirectory(Boolean.valueOf(value));\n       }\n     }"
    },
    {
        "commit_id": "f52faeb6389a575cfde24eea17002024dc36beda",
        "commit_message": "HADOOP-19485. S3A: Test Failures after SDK 2.29.52 Upgrade\n\nCode changes related to HADOOP-19485.\n\nAwsSdkWorkarounds no longer needs to cut back on transfer manager logging\n(HADOOP-19272) :\n- Remove log downgrade and change assertion to expect nothing to be logged.\n- remove false positives from log.\n\nITestS3AEndpointRegion failure:\n- Change in state of AwsExecutionAttribute.ENDPOINT_OVERRIDDEN\n  attribute requires test tuning to match.\n\nSome tests against third-party stores fail\n- Includes fix for the assumeStoreAwsHosted() logic.\n- Documents how to turn off multipart uploads with third-party stores.\n\nContributed by Steve Loughran.",
        "commit_url": "https://github.com/apache/hadoop/commit/f52faeb6389a575cfde24eea17002024dc36beda",
        "buggy_code": "!NetworkBinding.isAwsEndpoint(fs.getConf()",
        "fixed_code": "NetworkBinding.isAwsEndpoint(fs.getConf()",
        "patch": "@@ -1162,7 +1162,7 @@ public static void assumeNotS3ExpressFileSystem(final FileSystem fs) {\n    */\n   public static void assumeStoreAwsHosted(final FileSystem fs) {\n     assume(\"store is not AWS S3\",\n-        !NetworkBinding.isAwsEndpoint(fs.getConf()\n+        NetworkBinding.isAwsEndpoint(fs.getConf()\n             .getTrimmed(ENDPOINT, DEFAULT_ENDPOINT)));\n   }\n "
    },
    {
        "commit_id": "f52faeb6389a575cfde24eea17002024dc36beda",
        "commit_message": "HADOOP-19485. S3A: Test Failures after SDK 2.29.52 Upgrade\n\nCode changes related to HADOOP-19485.\n\nAwsSdkWorkarounds no longer needs to cut back on transfer manager logging\n(HADOOP-19272) :\n- Remove log downgrade and change assertion to expect nothing to be logged.\n- remove false positives from log.\n\nITestS3AEndpointRegion failure:\n- Change in state of AwsExecutionAttribute.ENDPOINT_OVERRIDDEN\n  attribute requires test tuning to match.\n\nSome tests against third-party stores fail\n- Includes fix for the assumeStoreAwsHosted() logic.\n- Documents how to turn off multipart uploads with third-party stores.\n\nContributed by Steve Loughran.",
        "commit_url": "https://github.com/apache/hadoop/commit/f52faeb6389a575cfde24eea17002024dc36beda",
        "buggy_code": "String awsSdkPrefix = \"software/amazon/awssdk\";",
        "fixed_code": "String awsSdkPrefix = \"software/amazon/\";",
        "patch": "@@ -57,7 +57,7 @@ public void testShadedClasses() throws IOException {\n     assertThat(v2ClassPath)\n             .as(\"AWS V2 SDK should be present on the classpath\").isNotNull();\n     List<String> listOfV2SdkClasses = getClassNamesFromJarFile(v2ClassPath);\n-    String awsSdkPrefix = \"software/amazon/awssdk\";\n+    String awsSdkPrefix = \"software/amazon/\";\n     List<String> unshadedClasses = new ArrayList<>();\n     for (String awsSdkClass : listOfV2SdkClasses) {\n       if (!awsSdkClass.startsWith(awsSdkPrefix)) {"
    },
    {
        "commit_id": "2dd658252bd2ec9831c0984823e173fca11f9051",
        "commit_message": "HADOOP-19492. S3A: Some tests failing on third-party stores\n\n* Includes fix for the assumeStoreAwsHosted() logic.\n* Documents how to turn off multipart uploads with third-party stores\n\nChange-Id: Iae344b372dceaca981426035e062b542af25f0cd",
        "commit_url": "https://github.com/apache/hadoop/commit/2dd658252bd2ec9831c0984823e173fca11f9051",
        "buggy_code": "!NetworkBinding.isAwsEndpoint(fs.getConf()",
        "fixed_code": "NetworkBinding.isAwsEndpoint(fs.getConf()",
        "patch": "@@ -1162,7 +1162,7 @@ public static void assumeNotS3ExpressFileSystem(final FileSystem fs) {\n    */\n   public static void assumeStoreAwsHosted(final FileSystem fs) {\n     assume(\"store is not AWS S3\",\n-        !NetworkBinding.isAwsEndpoint(fs.getConf()\n+        NetworkBinding.isAwsEndpoint(fs.getConf()\n             .getTrimmed(ENDPOINT, DEFAULT_ENDPOINT)));\n   }\n "
    },
    {
        "commit_id": "f9f9aeefc6c2efa430b7341b8003ef447041ef4c",
        "commit_message": "HADOOP-19506. Fix TestThrottledInputStream when bandwidth is equal to throttle limit (#7517) Contributed by Istvan Toth.\n\nSigned-off-by: Shilun Fan <slfan1989@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/f9f9aeefc6c2efa430b7341b8003ef447041ef4c",
        "buggy_code": "assertTrue(bytesPerSec < maxBPS);",
        "fixed_code": "assertTrue(bytesPerSec <= maxBPS);",
        "patch": "@@ -216,7 +216,7 @@ private void copyAndAssert(File tmpFile, File outFile, long maxBPS)\n       assertEquals(in.getTotalBytesRead(), tmpFile.length());\n \n       long bytesPerSec = in.getBytesPerSec();\n-      assertTrue(bytesPerSec < maxBPS);\n+      assertTrue(bytesPerSec <= maxBPS);\n     } finally {\n       IOUtils.closeStream(in);\n       IOUtils.closeStream(out);"
    },
    {
        "commit_id": "fba01e695567693e3fb59820af5af4f19401940d",
        "commit_message": "YARN-11797. Fix tests trying to connect to 0.0.0.0 (#7515) Contributed by Istvan Toth.\n\nSigned-off-by: Shilun Fan <slfan1989@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/fba01e695567693e3fb59820af5af4f19401940d",
        "buggy_code": "setConfForRM(rmId, confKey, \"0.0.0.0:\" + ServerSocketUtil.getPort(base +",
        "fixed_code": "setConfForRM(rmId, confKey, \"localhost:\" + ServerSocketUtil.getPort(base +",
        "patch": "@@ -29,7 +29,7 @@ public class HATestUtil {\n   public static void setRpcAddressForRM(String rmId, int base,\n       Configuration conf) throws IOException {\n     for (String confKey : YarnConfiguration.getServiceAddressConfKeys(conf)) {\n-      setConfForRM(rmId, confKey, \"0.0.0.0:\" + ServerSocketUtil.getPort(base +\n+      setConfForRM(rmId, confKey, \"localhost:\" + ServerSocketUtil.getPort(base +\n           YarnConfiguration.getRMDefaultPortNumber(confKey, conf), 10), conf);\n     }\n   }"
    },
    {
        "commit_id": "e6144531de80ccca7a17281be34eab4ea0e57266",
        "commit_message": "YARN-11764. yarn tests have stopped running. (#7345)\n\nAfter completing HADOOP-15984, we added JUnit5 test dependencies to some modules. These dependencies caused maven-surefire to fail to recognize JUnit4 tests, leading to the cessation of unit tests in some YARN modules. As a result, some YARN unit tests failed and were not detected in time. This JIRA will track and resolve these issues, including the stopped unit tests and test errors.\n\nCo-authored-by: Chris Nauroth <cnauroth@apache.org>\nCo-authored-by: He Xiaoqiao <hexiaoqiao@apache.org>\nReviewed-by: Chris Nauroth <cnauroth@apache.org>\nReviewed-by: Steve Loughran <stevel@apache.org>\nReviewed-by: He Xiaoqiao <hexiaoqiao@apache.org>\nSigned-off-by: Shilun Fan <slfan1989@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/e6144531de80ccca7a17281be34eab4ea0e57266",
        "buggy_code": "@JsonIgnoreProperties(ignoreUnknown = true)",
        "fixed_code": "@JsonIgnoreProperties(ignoreUnknown = true, value = {\"eventInfoJAXB\"})",
        "patch": "@@ -42,7 +42,7 @@\n @XmlAccessorType(XmlAccessType.NONE)\n @Public\n @Evolving\n-@JsonIgnoreProperties(ignoreUnknown = true)\n+@JsonIgnoreProperties(ignoreUnknown = true, value = {\"eventInfoJAXB\"})\n public class TimelineEvent implements Comparable<TimelineEvent> {\n \n   private long timestamp;"
    },
    {
        "commit_id": "e6144531de80ccca7a17281be34eab4ea0e57266",
        "commit_message": "YARN-11764. yarn tests have stopped running. (#7345)\n\nAfter completing HADOOP-15984, we added JUnit5 test dependencies to some modules. These dependencies caused maven-surefire to fail to recognize JUnit4 tests, leading to the cessation of unit tests in some YARN modules. As a result, some YARN unit tests failed and were not detected in time. This JIRA will track and resolve these issues, including the stopped unit tests and test errors.\n\nCo-authored-by: Chris Nauroth <cnauroth@apache.org>\nCo-authored-by: He Xiaoqiao <hexiaoqiao@apache.org>\nReviewed-by: Chris Nauroth <cnauroth@apache.org>\nReviewed-by: Steve Loughran <stevel@apache.org>\nReviewed-by: He Xiaoqiao <hexiaoqiao@apache.org>\nSigned-off-by: Shilun Fan <slfan1989@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/e6144531de80ccca7a17281be34eab4ea0e57266",
        "buggy_code": "@JsonIgnoreProperties(ignoreUnknown = true)",
        "fixed_code": "@JsonIgnoreProperties(ignoreUnknown = true, value = {\"valuesJAXB\"})",
        "patch": "@@ -39,7 +39,7 @@\n @XmlAccessorType(XmlAccessType.NONE)\n @InterfaceAudience.Public\n @InterfaceStability.Unstable\n-@JsonIgnoreProperties(ignoreUnknown = true)\n+@JsonIgnoreProperties(ignoreUnknown = true, value = {\"valuesJAXB\"})\n public class TimelineMetric {\n \n   /**"
    },
    {
        "commit_id": "e6144531de80ccca7a17281be34eab4ea0e57266",
        "commit_message": "YARN-11764. yarn tests have stopped running. (#7345)\n\nAfter completing HADOOP-15984, we added JUnit5 test dependencies to some modules. These dependencies caused maven-surefire to fail to recognize JUnit4 tests, leading to the cessation of unit tests in some YARN modules. As a result, some YARN unit tests failed and were not detected in time. This JIRA will track and resolve these issues, including the stopped unit tests and test errors.\n\nCo-authored-by: Chris Nauroth <cnauroth@apache.org>\nCo-authored-by: He Xiaoqiao <hexiaoqiao@apache.org>\nReviewed-by: Chris Nauroth <cnauroth@apache.org>\nReviewed-by: Steve Loughran <stevel@apache.org>\nReviewed-by: He Xiaoqiao <hexiaoqiao@apache.org>\nSigned-off-by: Shilun Fan <slfan1989@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/e6144531de80ccca7a17281be34eab4ea0e57266",
        "buggy_code": "if (yarnClient != null) {",
        "fixed_code": "if (yarnClient != null && attemptId != null) {",
        "patch": "@@ -196,7 +196,7 @@ Collections.<String, LocalResource> emptyMap(),\n \n   @After\n   public void teardown() throws YarnException, IOException {\n-    if (yarnClient != null) {\n+    if (yarnClient != null && attemptId != null) {\n       yarnClient.killApplication(attemptId.getApplicationId());\n     }\n     attemptId = null;"
    },
    {
        "commit_id": "e6144531de80ccca7a17281be34eab4ea0e57266",
        "commit_message": "YARN-11764. yarn tests have stopped running. (#7345)\n\nAfter completing HADOOP-15984, we added JUnit5 test dependencies to some modules. These dependencies caused maven-surefire to fail to recognize JUnit4 tests, leading to the cessation of unit tests in some YARN modules. As a result, some YARN unit tests failed and were not detected in time. This JIRA will track and resolve these issues, including the stopped unit tests and test errors.\n\nCo-authored-by: Chris Nauroth <cnauroth@apache.org>\nCo-authored-by: He Xiaoqiao <hexiaoqiao@apache.org>\nReviewed-by: Chris Nauroth <cnauroth@apache.org>\nReviewed-by: Steve Loughran <stevel@apache.org>\nReviewed-by: He Xiaoqiao <hexiaoqiao@apache.org>\nSigned-off-by: Shilun Fan <slfan1989@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/e6144531de80ccca7a17281be34eab4ea0e57266",
        "buggy_code": "@Test(timeout = 5000)",
        "fixed_code": "@Test(timeout = 8000)",
        "patch": "@@ -197,7 +197,7 @@ public void testSplitBasedOnHeadroom() throws Exception {\n     checkTotalContainerAllocation(response, 100);\n   }\n \n-  @Test(timeout = 5000)\n+  @Test(timeout = 8000)\n   public void testStressPolicy() throws Exception {\n \n     // Tests how the headroom info are used to split based on the capacity"
    },
    {
        "commit_id": "e6144531de80ccca7a17281be34eab4ea0e57266",
        "commit_message": "YARN-11764. yarn tests have stopped running. (#7345)\n\nAfter completing HADOOP-15984, we added JUnit5 test dependencies to some modules. These dependencies caused maven-surefire to fail to recognize JUnit4 tests, leading to the cessation of unit tests in some YARN modules. As a result, some YARN unit tests failed and were not detected in time. This JIRA will track and resolve these issues, including the stopped unit tests and test errors.\n\nCo-authored-by: Chris Nauroth <cnauroth@apache.org>\nCo-authored-by: He Xiaoqiao <hexiaoqiao@apache.org>\nReviewed-by: Chris Nauroth <cnauroth@apache.org>\nReviewed-by: Steve Loughran <stevel@apache.org>\nReviewed-by: He Xiaoqiao <hexiaoqiao@apache.org>\nSigned-off-by: Shilun Fan <slfan1989@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/e6144531de80ccca7a17281be34eab4ea0e57266",
        "buggy_code": "for (String fileType : new String[]{\"stdout\", \"stderr\", \"syslog\"}) {",
        "fixed_code": "for (String fileType : new String[]{\"stdout\", \"stderr\", \"syslog\", \"zero\"}) {",
        "patch": "@@ -252,7 +252,7 @@ private void verifyLocalFileDeletion(\n \n       String containerIdStr = container11.toString();\n       File containerLogDir = new File(app1LogDir, containerIdStr);\n-      for (String fileType : new String[]{\"stdout\", \"stderr\", \"syslog\"}) {\n+      for (String fileType : new String[]{\"stdout\", \"stderr\", \"syslog\", \"zero\"}) {\n         File f = new File(containerLogDir, fileType);\n         GenericTestUtils.waitFor(() -> !f.exists(), 1000, 1000 * 50);\n         Assert.assertFalse(\"File [\" + f + \"] was not deleted\", f.exists());"
    },
    {
        "commit_id": "1ba30d6ca63391bb8bffd61ca089f889d234437b",
        "commit_message": "YARN-11754. [JDK17] Fix SpotBugs Issues in YARN. (#7317) Contributed by Shilun Fan.\n\nReviewed-by: Chris Nauroth <cnauroth@apache.org>\r\nSigned-off-by: Shilun Fan <slfan1989@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/1ba30d6ca63391bb8bffd61ca089f889d234437b",
        "buggy_code": "Long ts = supplementTs ? TimestampGenerator.",
        "fixed_code": "long ts = supplementTs ? TimestampGenerator.",
        "patch": "@@ -330,7 +330,7 @@ public static <K> Map<K, Object> readResults(Result result,\n               for (Map.Entry<Long, byte[]> cell : cells.entrySet()) {\n                 V value =\n                     (V) valueConverter.decodeValue(cell.getValue());\n-                Long ts = supplementTs ? TimestampGenerator.\n+                long ts = supplementTs ? TimestampGenerator.\n                     getTruncatedTimestamp(cell.getKey()) : cell.getKey();\n                 cellResults.put(ts, value);\n               }"
    },
    {
        "commit_id": "91535fa7b7dee6c94dc61ab13500fe35dcf06cae",
        "commit_message": "YARN-11759: Fix log statement in RMAppImpl#processNodeUpdate\n\nCloses #7328\n\nSigned-off-by: Chris Nauroth <cnauroth@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/91535fa7b7dee6c94dc61ab13500fe35dcf06cae",
        "buggy_code": "LOG.debug(\"Received node update event:{} for node:{} with state:\",",
        "fixed_code": "LOG.debug(\"Received node update event:{} for node:{} with state:{}\",",
        "patch": "@@ -1018,7 +1018,7 @@ private void createNewAttempt(ApplicationAttemptId appAttemptId) {\n   private void processNodeUpdate(RMAppNodeUpdateType type, RMNode node) {\n     NodeState nodeState = node.getState();\n     updatedNodes.put(node, RMAppNodeUpdateType.convertToNodeUpdateType(type));\n-    LOG.debug(\"Received node update event:{} for node:{} with state:\",\n+    LOG.debug(\"Received node update event:{} for node:{} with state:{}\",\n         type, node, nodeState);\n   }\n "
    },
    {
        "commit_id": "1f0d9df8875f23e456c0950d2c5489bd1b7973be",
        "commit_message": "HDFS-17637. Fix spotbugs in HttpFSFileSystem#getXAttr (#7099) Contributed by Hualong Zhang.\n\nReviewed-by: Shilun Fan <slfan1989@apache.org>\r\nSigned-off-by: Shilun Fan <slfan1989@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/1f0d9df8875f23e456c0950d2c5489bd1b7973be",
        "buggy_code": "return xAttrs != null ? xAttrs.get(name) : null;",
        "fixed_code": "return xAttrs.get(name);",
        "patch": "@@ -1370,7 +1370,7 @@ public byte[] getXAttr(Path f, String name) throws IOException {\n     JSONObject json = (JSONObject) HttpFSUtils.jsonParse(conn);\n     Map<String, byte[]> xAttrs = createXAttrMap(\n         (JSONArray) json.get(XATTRS_JSON));\n-    return xAttrs != null ? xAttrs.get(name) : null;\n+    return xAttrs.get(name);\n   }\n \n   /** Convert xAttrs json to xAttrs map */"
    },
    {
        "commit_id": "6bcc2541235486d30be5b5438327673039d07951",
        "commit_message": "HADOOP-19279. ABFS: Disabling Apache Http Client as Default Http Client for ABFS Driver(#7055)\n\nAs part of work done under HADOOP-19120 [ABFS]: ApacheHttpClient adaptation as network library - ASF JIRA\r\nApache HTTP Client was introduced as an alternative Network Library that can be used with ABFS Driver. Earlier JDK Http Client was the only supported network library.\r\n\r\nApache HTTP Client was found to be more helpful in terms of controls and knobs it provides to manage the Network aspects of the driver better. Hence, the default Network Client was made to be used with the ABFS Driver.\r\n\r\nRecently while running scale workloads, we observed a regression where some unexpected wait time was observed while establishing connections. A possible fix has been identified and we are working on getting it fixed.\r\nThere was also a possible NPE scenario was identified on the new network client code.\r\n\r\nUntil we are done with the code fixes and revalidated the whole Apache client flow, we would like to make JDK Client as default client again. The support will still be there, but it will be disabled behind a config.\r\n\r\nContributed by: manika137",
        "commit_url": "https://github.com/apache/hadoop/commit/6bcc2541235486d30be5b5438327673039d07951",
        "buggy_code": "= HttpOperationType.APACHE_HTTP_CLIENT;",
        "fixed_code": "= HttpOperationType.JDK_HTTP_URL_CONNECTION;",
        "patch": "@@ -169,7 +169,7 @@ public final class FileSystemConfigurations {\n   public static final long THOUSAND = 1000L;\n \n   public static final HttpOperationType DEFAULT_NETWORKING_LIBRARY\n-      = HttpOperationType.APACHE_HTTP_CLIENT;\n+      = HttpOperationType.JDK_HTTP_URL_CONNECTION;\n \n   public static final int DEFAULT_APACHE_HTTP_CLIENT_MAX_IO_EXCEPTION_RETRIES = 3;\n "
    },
    {
        "commit_id": "ea6e0f7cd58d0129897dfc7870aee188be80a904",
        "commit_message": "HADOOP-19221. S3A: Unable to recover from failure of multipart block upload attempt (#6938)\n\n\r\nThis is a major change which handles 400 error responses when uploading\r\nlarge files from memory heap/buffer (or staging committer) and the remote S3\r\nstore returns a 500 response from a upload of a block in a multipart upload.\r\n\r\nThe SDK's own streaming code seems unable to fully replay the upload;\r\nat attempts to but then blocks and the S3 store returns a 400 response\r\n\r\n    \"Your socket connection to the server was not read from or written to\r\n     within the timeout period. Idle connections will be closed.\r\n     (Service: S3, Status Code: 400...)\"\r\n\r\nThere is an option to control whether or not the S3A client itself\r\nattempts to retry on a 50x error other than 503 throttling events\r\n(which are independently processed as before)\r\n\r\nOption:  fs.s3a.retry.http.5xx.errors\r\nDefault: true\r\n\r\n500 errors are very rare from standard AWS S3, which has a five nines\r\nSLA. It may be more common against S3 Express which has lower\r\nguarantees.\r\n\r\nThird party stores have unknown guarantees, and the exception may\r\nindicate a bad server configuration. Consider setting\r\nfs.s3a.retry.http.5xx.errors to false when working with\r\nsuch stores.\r\n\r\nSignification Code changes:\r\n\r\nThere is now a custom set of implementations of\r\nsoftware.amazon.awssdk.http.ContentStreamProvidercontent in\r\nthe class org.apache.hadoop.fs.s3a.impl.UploadContentProviders.\r\n\r\nThese:\r\n\r\n* Restart on failures\r\n* Do not copy buffers/byte buffers into new private byte arrays,\r\n  so avoid exacerbating memory problems..\r\n\r\nThere new IOStatistics for specific http error codes -these are collected\r\neven when all recovery is performed within the SDK.\r\n  \r\nS3ABlockOutputStream has major changes, including handling of\r\nThread.interrupt() on the main thread, which now triggers and briefly\r\nawaits cancellation of any ongoing uploads.\r\n\r\nIf the writing thread is interrupted in close(), it is mapped to\r\nan InterruptedIOException. Applications like Hive and Spark must\r\ncatch these after cancelling a worker thread.\r\n\r\nContributed by Steve Loughran",
        "commit_url": "https://github.com/apache/hadoop/commit/ea6e0f7cd58d0129897dfc7870aee188be80a904",
        "buggy_code": "return new S3ADataBlocks.ByteBufferBlockFactory(fileSystem);",
        "fixed_code": "return new S3ADataBlocks.ByteBufferBlockFactory(fileSystem.createStoreContext());",
        "patch": "@@ -27,7 +27,7 @@ protected String getBlockOutputBufferName() {\n   }\n \n   protected S3ADataBlocks.BlockFactory createFactory(S3AFileSystem fileSystem) {\n-    return new S3ADataBlocks.ByteBufferBlockFactory(fileSystem);\n+    return new S3ADataBlocks.ByteBufferBlockFactory(fileSystem.createStoreContext());\n   }\n \n }"
    },
    {
        "commit_id": "ea6e0f7cd58d0129897dfc7870aee188be80a904",
        "commit_message": "HADOOP-19221. S3A: Unable to recover from failure of multipart block upload attempt (#6938)\n\n\r\nThis is a major change which handles 400 error responses when uploading\r\nlarge files from memory heap/buffer (or staging committer) and the remote S3\r\nstore returns a 500 response from a upload of a block in a multipart upload.\r\n\r\nThe SDK's own streaming code seems unable to fully replay the upload;\r\nat attempts to but then blocks and the S3 store returns a 400 response\r\n\r\n    \"Your socket connection to the server was not read from or written to\r\n     within the timeout period. Idle connections will be closed.\r\n     (Service: S3, Status Code: 400...)\"\r\n\r\nThere is an option to control whether or not the S3A client itself\r\nattempts to retry on a 50x error other than 503 throttling events\r\n(which are independently processed as before)\r\n\r\nOption:  fs.s3a.retry.http.5xx.errors\r\nDefault: true\r\n\r\n500 errors are very rare from standard AWS S3, which has a five nines\r\nSLA. It may be more common against S3 Express which has lower\r\nguarantees.\r\n\r\nThird party stores have unknown guarantees, and the exception may\r\nindicate a bad server configuration. Consider setting\r\nfs.s3a.retry.http.5xx.errors to false when working with\r\nsuch stores.\r\n\r\nSignification Code changes:\r\n\r\nThere is now a custom set of implementations of\r\nsoftware.amazon.awssdk.http.ContentStreamProvidercontent in\r\nthe class org.apache.hadoop.fs.s3a.impl.UploadContentProviders.\r\n\r\nThese:\r\n\r\n* Restart on failures\r\n* Do not copy buffers/byte buffers into new private byte arrays,\r\n  so avoid exacerbating memory problems..\r\n\r\nThere new IOStatistics for specific http error codes -these are collected\r\neven when all recovery is performed within the SDK.\r\n  \r\nS3ABlockOutputStream has major changes, including handling of\r\nThread.interrupt() on the main thread, which now triggers and briefly\r\nawaits cancellation of any ongoing uploads.\r\n\r\nIf the writing thread is interrupted in close(), it is mapped to\r\nan InterruptedIOException. Applications like Hive and Spark must\r\ncatch these after cancelling a worker thread.\r\n\r\nContributed by Steve Loughran",
        "commit_url": "https://github.com/apache/hadoop/commit/ea6e0f7cd58d0129897dfc7870aee188be80a904",
        "buggy_code": "Assume.assumeTrue(\"mark/reset nopt supoprted\", false);",
        "fixed_code": "Assume.assumeTrue(\"mark/reset not supported\", false);",
        "patch": "@@ -36,7 +36,7 @@ protected String getBlockOutputBufferName() {\n    * @return null\n    */\n   protected S3ADataBlocks.BlockFactory createFactory(S3AFileSystem fileSystem) {\n-    Assume.assumeTrue(\"mark/reset nopt supoprted\", false);\n+    Assume.assumeTrue(\"mark/reset not supported\", false);\n     return null;\n   }\n }"
    },
    {
        "commit_id": "ea6e0f7cd58d0129897dfc7870aee188be80a904",
        "commit_message": "HADOOP-19221. S3A: Unable to recover from failure of multipart block upload attempt (#6938)\n\n\r\nThis is a major change which handles 400 error responses when uploading\r\nlarge files from memory heap/buffer (or staging committer) and the remote S3\r\nstore returns a 500 response from a upload of a block in a multipart upload.\r\n\r\nThe SDK's own streaming code seems unable to fully replay the upload;\r\nat attempts to but then blocks and the S3 store returns a 400 response\r\n\r\n    \"Your socket connection to the server was not read from or written to\r\n     within the timeout period. Idle connections will be closed.\r\n     (Service: S3, Status Code: 400...)\"\r\n\r\nThere is an option to control whether or not the S3A client itself\r\nattempts to retry on a 50x error other than 503 throttling events\r\n(which are independently processed as before)\r\n\r\nOption:  fs.s3a.retry.http.5xx.errors\r\nDefault: true\r\n\r\n500 errors are very rare from standard AWS S3, which has a five nines\r\nSLA. It may be more common against S3 Express which has lower\r\nguarantees.\r\n\r\nThird party stores have unknown guarantees, and the exception may\r\nindicate a bad server configuration. Consider setting\r\nfs.s3a.retry.http.5xx.errors to false when working with\r\nsuch stores.\r\n\r\nSignification Code changes:\r\n\r\nThere is now a custom set of implementations of\r\nsoftware.amazon.awssdk.http.ContentStreamProvidercontent in\r\nthe class org.apache.hadoop.fs.s3a.impl.UploadContentProviders.\r\n\r\nThese:\r\n\r\n* Restart on failures\r\n* Do not copy buffers/byte buffers into new private byte arrays,\r\n  so avoid exacerbating memory problems..\r\n\r\nThere new IOStatistics for specific http error codes -these are collected\r\neven when all recovery is performed within the SDK.\r\n  \r\nS3ABlockOutputStream has major changes, including handling of\r\nThread.interrupt() on the main thread, which now triggers and briefly\r\nawaits cancellation of any ongoing uploads.\r\n\r\nIf the writing thread is interrupted in close(), it is mapped to\r\nan InterruptedIOException. Applications like Hive and Spark must\r\ncatch these after cancelling a worker thread.\r\n\r\nContributed by Steve Loughran",
        "commit_url": "https://github.com/apache/hadoop/commit/ea6e0f7cd58d0129897dfc7870aee188be80a904",
        "buggy_code": "private void resetStatistics() {",
        "fixed_code": "protected void resetStatistics() {",
        "patch": "@@ -301,7 +301,7 @@ protected int directoriesInPath(Path path) {\n   /**\n    * Reset all the metrics being tracked.\n    */\n-  private void resetStatistics() {\n+  protected void resetStatistics() {\n     costValidator.resetMetricDiffs();\n   }\n "
    },
    {
        "commit_id": "1655acc5e2d5fe27e01f46ea02bd5a7dea44fe12",
        "commit_message": "HADOOP-19250. [Addendum] Fix test TestServiceInterruptHandling.testRegisterAndRaise. (#7008)\n\n\r\nContributed by Chenyu Zheng",
        "commit_url": "https://github.com/apache/hadoop/commit/1655acc5e2d5fe27e01f46ea02bd5a7dea44fe12",
        "buggy_code": "String name = IrqHandler.CONTROL_C;",
        "fixed_code": "String name = \"USR2\";",
        "patch": "@@ -38,7 +38,7 @@ public class TestServiceInterruptHandling\n   @Test\n   public void testRegisterAndRaise() throws Throwable {\n     InterruptCatcher catcher = new InterruptCatcher();\n-    String name = IrqHandler.CONTROL_C;\n+    String name = \"USR2\";\n     IrqHandler irqHandler = new IrqHandler(name, catcher);\n     irqHandler.bind();\n     assertEquals(0, irqHandler.getSignalCount());"
    },
    {
        "commit_id": "8ca4627a0da91f61a8015589a886ecbe9c949de5",
        "commit_message": "HDFS-17557. Fix bug for TestRedundancyMonitor#testChooseTargetWhenAllDataNodesStop (#6897). Contributed by Haiyang Hu.\n\nSigned-off-by: Ayush Saxena <ayushsaxena@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/8ca4627a0da91f61a8015589a886ecbe9c949de5",
        "buggy_code": "doAnswer(delayer).when(spyClusterMap).getNumOfRacks();",
        "fixed_code": "doAnswer(delayer).when(spyClusterMap).getNumOfNonEmptyRacks();",
        "patch": "@@ -72,7 +72,7 @@ public void testChooseTargetWhenAllDataNodesStop() throws Throwable {\n       NetworkTopology clusterMap = replicator.clusterMap;\n       NetworkTopology spyClusterMap = spy(clusterMap);\n       replicator.clusterMap = spyClusterMap;\n-      doAnswer(delayer).when(spyClusterMap).getNumOfRacks();\n+      doAnswer(delayer).when(spyClusterMap).getNumOfNonEmptyRacks();\n \n       ExecutorService pool = Executors.newFixedThreadPool(2);\n "
    },
    {
        "commit_id": "ae76e9475cdafbe4c00f37a0d94f13b772b4d10d",
        "commit_message": "HDFS-17564. EC: Fix the issue of inaccurate metrics when decommission mark busy DN. (#6911). Contributed by Haiyang Hu.\n\nSigned-off-by: He Xiaoqiao <hexiaoqiao@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/ae76e9475cdafbe4c00f37a0d94f13b772b4d10d",
        "buggy_code": "abstract void addTaskToDatanode(NumberReplicas numberReplicas);",
        "fixed_code": "abstract boolean addTaskToDatanode(NumberReplicas numberReplicas);",
        "patch": "@@ -145,5 +145,5 @@ abstract void chooseTargets(BlockPlacementPolicy blockplacement,\n    *\n    * @param numberReplicas replica details\n    */\n-  abstract void addTaskToDatanode(NumberReplicas numberReplicas);\n+  abstract boolean addTaskToDatanode(NumberReplicas numberReplicas);\n }"
    },
    {
        "commit_id": "4b1b16a846b59c6499a923dfe7f59e48d8a5a3d4",
        "commit_message": "HDFS-17551. Fix unit test failure caused by HDFS-17464. (#6883). Contributed by farmmamba.",
        "commit_url": "https://github.com/apache/hadoop/commit/4b1b16a846b59c6499a923dfe7f59e48d8a5a3d4",
        "buggy_code": "+ \"should be monotonically increased.\",",
        "fixed_code": "+ \"should be monotonically increased\",",
        "patch": "@@ -1173,7 +1173,7 @@ private void testMoveBlockFailure(Configuration config) {\n           .getReplicaInfo(block.getBlockPoolId(), newReplicaInfo.getBlockId())\n           .getGenerationStamp());\n       LambdaTestUtils.intercept(IOException.class, \"Generation Stamp \"\n-              + \"should be monotonically increased.\",\n+              + \"should be monotonically increased\",\n           () -> fsDataSetImpl.finalizeNewReplica(newReplicaInfo, block));\n       assertFalse(newReplicaInfo.blockDataExists());\n "
    },
    {
        "commit_id": "f4fde40524d73d855bb5ea6375834dce24cd4688",
        "commit_message": "HADOOP-19184. S3A Fix TestStagingCommitter.testJobCommitFailure (#6843)\n\n\r\nFollow up on HADOOP-18679\r\n\r\nContributed by: Mukund Thakur",
        "commit_url": "https://github.com/apache/hadoop/commit/f4fde40524d73d855bb5ea6375834dce24cd4688",
        "buggy_code": "Pair.of(results, errors));",
        "fixed_code": "Pair.of(results, errors), mockClient);",
        "patch": "@@ -158,7 +158,7 @@ public void setupCommitter() throws Exception {\n     this.errors = new StagingTestBase.ClientErrors();\n     this.mockClient = newMockS3Client(results, errors);\n     this.mockFS = createAndBindMockFSInstance(jobConf,\n-        Pair.of(results, errors));\n+        Pair.of(results, errors), mockClient);\n     this.wrapperFS = lookupWrapperFS(jobConf);\n     // and bind the FS\n     wrapperFS.setAmazonS3Client(mockClient);"
    },
    {
        "commit_id": "87fb97777745b2cefed6bef57490b84676d2343d",
        "commit_message": "HADOOP-19098. Vector IO: Specify and validate ranges consistently. #6604\n\nClarifies behaviour of VectorIO methods with contract tests as well as\nspecification.\n\n* Add precondition range checks to all implementations\n* Identify and fix bug where direct buffer reads was broken\n  (HADOOP-19101; this surfaced in ABFS contract tests)\n* Logging in VectoredReadUtils.\n* TestVectoredReadUtils verifies validation logic.\n* FileRangeImpl toString() improvements\n* CombinedFileRange tracks bytes in range which are wanted;\n   toString() output logs this.\n\nHDFS\n* Add test TestHDFSContractVectoredRead\n\nABFS\n* Add test ITestAbfsFileSystemContractVectoredRead\n\nS3A\n* checks for vector IO being stopped in all iterative\n  vector operations, including draining\n* maps read() returning -1 to failure\n* passes in file length to validation\n* Error reporting to only completeExceptionally() those ranges\n  which had not yet read data in.\n* Improved logging.\n\nreadVectored()\n* made synchronized. This is only for the invocation;\n  the actual async retrieves are unsynchronized.\n* closes input stream on invocation\n* switches to random IO, so avoids keeping any long-lived connection around.\n\n+ AbstractSTestS3AHugeFiles enhancements.\n+ ADDENDUM: test fix in ITestS3AContractVectoredRead\n\nContains: HADOOP-19101. Vectored Read into off-heap buffer broken in fallback\nimplementation\n\nContributed by Steve Loughran\n\nChange-Id: Ia4ed71864c595f175c275aad83a2ff5741693432",
        "commit_url": "https://github.com/apache/hadoop/commit/87fb97777745b2cefed6bef57490b84676d2343d",
        "buggy_code": "String.format(\"fs.s3a.bucket.%s.endpoint\", host), \"\");",
        "fixed_code": "String.format(\"fs.s3a.bucket.%s.endpoint\", host), \"us-east-1\");",
        "patch": "@@ -175,7 +175,7 @@ protected YarnConfiguration createConfiguration() {\n     String host = jobResourceUri.getHost();\n     // and fix to the main endpoint if the caller has moved\n     conf.set(\n-        String.format(\"fs.s3a.bucket.%s.endpoint\", host), \"\");\n+        String.format(\"fs.s3a.bucket.%s.endpoint\", host), \"us-east-1\");\n \n     // set up DTs\n     enableDelegationTokens(conf, tokenBinding);"
    },
    {
        "commit_id": "ba7faf90c80476c79e6bfc7c02749dfc031337eb",
        "commit_message": "HADOOP-19098. Vector IO: Specify and validate ranges consistently.\n\n\r\nClarifies behaviour of VectorIO methods with contract tests as well as specification.\r\n\r\n* Add precondition range checks to all implementations\r\n* Identify and fix bug where direct buffer reads was broken\r\n  (HADOOP-19101; this surfaced in ABFS contract tests)\r\n* Logging in VectoredReadUtils.\r\n* TestVectoredReadUtils verifies validation logic.\r\n* FileRangeImpl toString() improvements\r\n* CombinedFileRange tracks bytes in range which are wanted;\r\n   toString() output logs this.\r\n\r\nHDFS\r\n* Add test TestHDFSContractVectoredRead\r\n\r\nABFS\r\n* Add test ITestAbfsFileSystemContractVectoredRead\r\n\r\nS3A\r\n* checks for vector IO being stopped in all iterative\r\n  vector operations, including draining\r\n* maps read() returning -1 to failure\r\n* passes in file length to validation\r\n* Error reporting to only completeExceptionally() those ranges\r\n  which had not yet read data in.\r\n* Improved logging.  \r\n\r\nreadVectored()\r\n* made synchronized. This is only for the invocation;\r\n  the actual async retrieves are unsynchronized.\r\n* closes input stream on invocation\r\n* switches to random IO, so avoids keeping any long-lived connection around.\r\n\r\n+ AbstractSTestS3AHugeFiles enhancements.\r\n\r\nContains: HADOOP-19101. Vectored Read into off-heap buffer broken in fallback implementation\r\n\r\nContributed by Steve Loughran",
        "commit_url": "https://github.com/apache/hadoop/commit/ba7faf90c80476c79e6bfc7c02749dfc031337eb",
        "buggy_code": "String.format(\"fs.s3a.bucket.%s.endpoint\", host), \"\");",
        "fixed_code": "String.format(\"fs.s3a.bucket.%s.endpoint\", host), \"us-east-1\");",
        "patch": "@@ -175,7 +175,7 @@ protected YarnConfiguration createConfiguration() {\n     String host = jobResourceUri.getHost();\n     // and fix to the main endpoint if the caller has moved\n     conf.set(\n-        String.format(\"fs.s3a.bucket.%s.endpoint\", host), \"\");\n+        String.format(\"fs.s3a.bucket.%s.endpoint\", host), \"us-east-1\");\n \n     // set up DTs\n     enableDelegationTokens(conf, tokenBinding);"
    },
    {
        "commit_id": "f1f2abe6418308a8124cdb12aa98bd35168ba379",
        "commit_message": "YARN-11668. Fix RM crash for potential concurrent modification exception when updating node attributes (#6681) Contributed by Junfan Zhang.\n\nReviewed-by: Dinesh Chitlangia <dineshc@apache.org>\r\nSigned-off-by: Shilun Fan <slfan1989@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/f1f2abe6418308a8124cdb12aa98bd35168ba379",
        "buggy_code": "this(hostName, new HashMap<NodeAttribute, AttributeValue>());",
        "fixed_code": "this(hostName, new ConcurrentHashMap<NodeAttribute, AttributeValue>());",
        "patch": "@@ -613,7 +613,7 @@ public void setHostName(String hostName) {\n     }\n \n     public Host(String hostName) {\n-      this(hostName, new HashMap<NodeAttribute, AttributeValue>());\n+      this(hostName, new ConcurrentHashMap<NodeAttribute, AttributeValue>());\n     }\n \n     public Host(String hostName,"
    },
    {
        "commit_id": "12a26d8b1987e883efab00c25a0594512527bd1f",
        "commit_message": "HDFS-17431. Fix log format for BlockRecoveryWorker#recoverBlocks (#6643)",
        "commit_url": "https://github.com/apache/hadoop/commit/12a26d8b1987e883efab00c25a0594512527bd1f",
        "buggy_code": "LOG.warn(\"recover Block: {} FAILED: {}\", b, e);",
        "fixed_code": "LOG.warn(\"recover Block: {} FAILED: \", b, e);",
        "patch": "@@ -628,7 +628,7 @@ public void run() {\n                 new RecoveryTaskContiguous(b).recover();\n               }\n             } catch (IOException e) {\n-              LOG.warn(\"recover Block: {} FAILED: {}\", b, e);\n+              LOG.warn(\"recover Block: {} FAILED: \", b, e);\n             }\n           }\n         } finally {"
    },
    {
        "commit_id": "095dfcca306289e8f676de89d6a054a193593d5d",
        "commit_message": "HADOOP-18088. Replace log4j 1.x with reload4j. (#4052) \n\n\r\n\r\nCo-authored-by: Wei-Chiu Chuang <weichiu@apache.org>\r\n\r\n\r\nIncludes HADOOP-18354. Upgrade reload4j to 1.22.2 due to XXE vulnerability (#4607). \r\n\r\nLog4j 1.2.17 has been replaced by reloadj 1.22.2\r\nSLF4J is at 1.7.36",
        "commit_url": "https://github.com/apache/hadoop/commit/095dfcca306289e8f676de89d6a054a193593d5d",
        "buggy_code": "private static final String SLF4J_LOG4J_ADAPTER_CLASS = \"org.slf4j.impl.Log4jLoggerAdapter\";",
        "fixed_code": "private static final String SLF4J_LOG4J_ADAPTER_CLASS = \"org.slf4j.impl.Reload4jLoggerAdapter\";",
        "patch": "@@ -34,7 +34,7 @@\n @InterfaceStability.Unstable\n public class GenericsUtil {\n \n-  private static final String SLF4J_LOG4J_ADAPTER_CLASS = \"org.slf4j.impl.Log4jLoggerAdapter\";\n+  private static final String SLF4J_LOG4J_ADAPTER_CLASS = \"org.slf4j.impl.Reload4jLoggerAdapter\";\n \n   /**\n    * Set to false only if log4j adapter class is not found in the classpath. Once set to false,"
    },
    {
        "commit_id": "095dfcca306289e8f676de89d6a054a193593d5d",
        "commit_message": "HADOOP-18088. Replace log4j 1.x with reload4j. (#4052) \n\n\r\n\r\nCo-authored-by: Wei-Chiu Chuang <weichiu@apache.org>\r\n\r\n\r\nIncludes HADOOP-18354. Upgrade reload4j to 1.22.2 due to XXE vulnerability (#4607). \r\n\r\nLog4j 1.2.17 has been replaced by reloadj 1.22.2\r\nSLF4J is at 1.7.36",
        "commit_url": "https://github.com/apache/hadoop/commit/095dfcca306289e8f676de89d6a054a193593d5d",
        "buggy_code": "jarFile.getName().matches(\"log4j.*[.]jar\"));",
        "fixed_code": "jarFile.getName().matches(\"reload4j.*[.]jar\"));",
        "patch": "@@ -35,6 +35,6 @@ public void testFindContainingJar() {\n     Assert.assertTrue(\"Containing jar does not exist on file system \",\n         jarFile.exists());\n     Assert.assertTrue(\"Incorrect jar file \" + containingJar,\n-        jarFile.getName().matches(\"log4j.*[.]jar\"));\n+        jarFile.getName().matches(\"reload4j.*[.]jar\"));\n   }\n }"
    },
    {
        "commit_id": "8261229daab58529b17c1dc7b37ed5d0be9f5def",
        "commit_message": "HADOOP-18830. Cut S3 Select (#6144)\n\n\r\n\r\nCut out S3 Select\r\n* leave public/unstable constants alone\r\n* s3guard tool will fail with error\r\n* s3afs. path capability will fail\r\n* openFile() will fail with specific error\r\n* s3 select doc updated\r\n* Cut eventstream jar\r\n* New test: ITestSelectUnsupported verifies new failure\r\n  handling above\r\n\r\nContributed by Steve Loughran",
        "commit_url": "https://github.com/apache/hadoop/commit/8261229daab58529b17c1dc7b37ed5d0be9f5def",
        "buggy_code": "package org.apache.hadoop.fs.s3a.select;",
        "fixed_code": "package org.apache.hadoop.fs.s3a.tools;",
        "patch": "@@ -16,7 +16,7 @@\n  * limitations under the License.\n  */\n \n-package org.apache.hadoop.fs.s3a.select;\n+package org.apache.hadoop.fs.s3a.tools;\n \n import java.io.Closeable;\n import java.io.IOException;"
    },
    {
        "commit_id": "6da1a19a8367fa7e3eee999256638ad6ea03aebc",
        "commit_message": "HADOOP-19045. S3A: Validate CreateSession Timeout Propagation (#6470)\n\n\r\n\r\nNew test ITestCreateSessionTimeout to verify that the duration set\r\nin fs.s3a.connection.request.timeout is passed all the way down.\r\n\r\nThis is done by adding a sleep() in a custom signer and verifying\r\nthat it is interrupted and that an AWSApiCallTimeoutException is\r\nraised.\r\n\r\n+ Fix testRequestTimeout()\r\n* doesn't skip if considered cross-region\r\n* sets a minimum duration of 0 before invocation\r\n* resets the minimum afterwards\r\n\r\nContributed by Steve Loughran",
        "commit_url": "https://github.com/apache/hadoop/commit/6da1a19a8367fa7e3eee999256638ad6ea03aebc",
        "buggy_code": "public final class CustomHttpSigner implements HttpSigner<AwsCredentialsIdentity> {",
        "fixed_code": "public class CustomHttpSigner implements HttpSigner<AwsCredentialsIdentity> {",
        "patch": "@@ -40,7 +40,7 @@\n  *   fs.s3a.http.signer.class = org.apache.hadoop.fs.s3a.auth.CustomHttpSigner\n  * </pre>\n  */\n-public final class CustomHttpSigner implements HttpSigner<AwsCredentialsIdentity> {\n+public class CustomHttpSigner implements HttpSigner<AwsCredentialsIdentity> {\n   private static final Logger LOG = LoggerFactory\n       .getLogger(CustomHttpSigner.class);\n "
    },
    {
        "commit_id": "7272a3d960708863146b40681a7d2e07749078d4",
        "commit_message": "YARN-11641. Fix getCapacityConfigurationTypeForQueues for absolute mode with zero resource. (#6435)",
        "commit_url": "https://github.com/apache/hadoop/commit/7272a3d960708863146b40681a7d2e07749078d4",
        "buggy_code": "Guice.createInjector(new TestWebServiceUtil.WebServletModule(rm)));",
        "fixed_code": "Guice.createInjector(new TestWebServiceUtil.WebServletModule(rm, false)));",
        "patch": "@@ -115,7 +115,7 @@ public void setUp() throws Exception {\n         createConfig(new CapacitySchedulerConfiguration(new Configuration(false)));\n     rm = createMockRM(config);\n     GuiceServletConfig.setInjector(\n-        Guice.createInjector(new TestWebServiceUtil.WebServletModule(rm)));\n+        Guice.createInjector(new TestWebServiceUtil.WebServletModule(rm, false)));\n   }\n \n   public static MockRM createMockRM(CapacitySchedulerConfiguration csConf) {"
    },
    {
        "commit_id": "cc4c4be1b7bda8f5869241a50197699da0f99f4d",
        "commit_message": "HDFS-17331:Fix Blocks are always -1 and DataNode version are always UNKNOWN in federationhealth.html (#6429). Contributed by lei w.\n\nSigned-off-by: Shuyan Zhang <zhangshuyan@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/cc4c4be1b7bda8f5869241a50197699da0f99f4d",
        "buggy_code": "innerinfo.put(\"numBlocks\", -1); // node.numBlocks()",
        "fixed_code": "innerinfo.put(\"numBlocks\", node.getNumBlocks());",
        "patch": "@@ -481,7 +481,7 @@ private String getNodesImpl(final DatanodeReportType type) {\n         innerinfo.put(\"adminState\", node.getAdminState().toString());\n         innerinfo.put(\"nonDfsUsedSpace\", node.getNonDfsUsed());\n         innerinfo.put(\"capacity\", node.getCapacity());\n-        innerinfo.put(\"numBlocks\", -1); // node.numBlocks()\n+        innerinfo.put(\"numBlocks\", node.getNumBlocks());\n         innerinfo.put(\"version\", (node.getSoftwareVersion() == null ?\n                         \"UNKNOWN\" : node.getSoftwareVersion()));\n         innerinfo.put(\"used\", node.getDfsUsed());"
    },
    {
        "commit_id": "36198b5edf5b761b46fa5d1696ad6aa85b35b72a",
        "commit_message": "HADOOP-19027. S3A: S3AInputStream doesn't recover from HTTP/channel exceptions (#6425)\n\n\r\n\r\nDifferentiate from \"EOF out of range/end of GET\" from\r\n\"EOF channel problems\" through\r\ntwo different subclasses of EOFException and input streams to always\r\nretry on http channel errors; out of range GET requests are not retried.\r\nCurrently an EOFException is always treated as a fail-fast call in read()\r\n\r\nThis allows for all existing external code catching EOFException to handle\r\nboth, but S3AInputStream to cleanly differentiate range errors (map to -1)\r\nfrom channel errors (retry)\r\n\r\n- HttpChannelEOFException is subclass of EOFException, so all code\r\n  which catches EOFException is still happy.\r\n  retry policy: connectivityFailure\r\n- RangeNotSatisfiableEOFException is the subclass of EOFException\r\n  raised on 416 GET range errors.\r\n  retry policy: fail\r\n- Method ErrorTranslation.maybeExtractChannelException() to create this\r\n  from shaded/unshaded NoHttpResponseException, using string match to\r\n  avoid classpath problems.\r\n- And do this for SdkClientExceptions with OpenSSL error code WFOPENSSL0035.\r\n  We believe this is the OpenSSL equivalent.\r\n- ErrorTranslation.maybeExtractIOException() to perform this translation as\r\n  appropriate.\r\n\r\nS3AInputStream.reopen() code retries on EOF, except on\r\n RangeNotSatisfiableEOFException,\r\n which is converted to a -1 response to the caller\r\n as is done historically.\r\n\r\nS3AInputStream knows to handle these with\r\n read(): HttpChannelEOFException: stream aborting close then retry\r\n lazySeek(): Map RangeNotSatisfiableEOFException to -1, but do not map\r\n  any other EOFException class raised.\r\n\r\nThis means that\r\n* out of range reads map to -1\r\n* channel problems in reopen are retried\r\n* channel problems in read() abort the failed http connection so it\r\n  isn't recycled\r\n\r\nTests for this using/abusing mocking.\r\n\r\nTesting through actually raising 416 exceptions and verifying that\r\nreadFully(), char read() and vector reads are all good.\r\n\r\nThere is no attempt to recover within a readFully(); there's\r\na boolean constant switch to turn this on, but if anyone does\r\nit a test will spin forever as the inner PositionedReadable.read(position, buffer, len)\r\ndowngrades all EOF exceptions to -1.\r\nA new method would need to be added which controls whether to downgrade/rethrow\r\nexceptions.\r\n\r\nWhat does that mean? Possibly reduced resilience to non-retried failures\r\non the inner stream, even though more channel exceptions are retried on.\r\n\r\nContributed by Steve Loughran",
        "commit_url": "https://github.com/apache/hadoop/commit/36198b5edf5b761b46fa5d1696ad6aa85b35b72a",
        "buggy_code": "translated = S3AUtils.translateException(text, \"\",",
        "fixed_code": "translated = S3AUtils.translateException(text, \"/\",",
        "patch": "@@ -478,7 +478,7 @@ public <T> T retryUntranslated(\n       if (caught instanceof IOException) {\n         translated = (IOException) caught;\n       } else {\n-        translated = S3AUtils.translateException(text, \"\",\n+        translated = S3AUtils.translateException(text, \"/\",\n             (SdkException) caught);\n       }\n "
    },
    {
        "commit_id": "2369f0cddbb5e2ce20882d819760a58deeb13583",
        "commit_message": "HDFS-17309. RBF: Fix Router Safemode check condition error (#6390) Contributed by liuguanghua.\n\nReviewed-by: Inigo Goiri <inigoiri@apache.org>\r\nReviewed-by: Simbarashe Dzinamarira <sdzinamarira@linkedin.com>\r\nSigned-off-by: Shilun Fan <slfan1989@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/2369f0cddbb5e2ce20882d819760a58deeb13583",
        "buggy_code": "boolean isCacheStale = (now - cacheUpdateTime) > this.staleInterval;",
        "fixed_code": "boolean isCacheStale = (cacheUpdateTime == 0) || (now - cacheUpdateTime) > this.staleInterval;",
        "patch": "@@ -169,7 +169,7 @@ public void periodicInvoke() {\n     }\n     StateStoreService stateStore = router.getStateStore();\n     long cacheUpdateTime = stateStore.getCacheUpdateTime();\n-    boolean isCacheStale = (now - cacheUpdateTime) > this.staleInterval;\n+    boolean isCacheStale = (cacheUpdateTime == 0) || (now - cacheUpdateTime) > this.staleInterval;\n \n     // Always update to indicate our cache was updated\n     if (isCacheStale) {"
    },
    {
        "commit_id": "e07e445326ace76a0237692a8b28fdc481e3def7",
        "commit_message": "HDFS-17215. RBF: Fix some method annotations about @throws . (#6136). Contributed by xiaojunxiang.",
        "commit_url": "https://github.com/apache/hadoop/commit/e07e445326ace76a0237692a8b28fdc481e3def7",
        "buggy_code": "public void setProto(Message p) {",
        "fixed_code": "public void setProto(Message p) throws IllegalArgumentException {",
        "patch": "@@ -54,7 +54,7 @@ public FederationProtocolPBTranslator(Class<P> protoType) {\n    * the proto handler this translator holds.\n    */\n   @SuppressWarnings(\"unchecked\")\n-  public void setProto(Message p) {\n+  public void setProto(Message p) throws IllegalArgumentException {\n     if (protoClass.isInstance(p)) {\n       if (this.builder != null) {\n         // Merge with builder"
    },
    {
        "commit_id": "0417c1c633da188b4acf53f2ec9bf69c06c6d185",
        "commit_message": "HDFS-17263. RBF: Fix client ls trash path cannot get except default nameservices trash path (#6291) Contributed by liuguanghua.\n\nReviewed-by: Inigo Goiri <inigoiri@apache.org>\r\nReviewed-by: He Xiaoqiao <hexiaoqiao@apache.org>\r\nSigned-off-by: Shilun Fan <slfan1989@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/0417c1c633da188b4acf53f2ec9bf69c06c6d185",
        "buggy_code": "\"^\" + getTrashRoot() + TRASH_PATTERN + \"/\");",
        "fixed_code": "\"^\" + getTrashRoot() + TRASH_PATTERN);",
        "patch": "@@ -357,7 +357,7 @@ public void refreshEntries(final Collection<MountTable> entries) {\n   @VisibleForTesting\n   public static boolean isTrashPath(String path) throws IOException {\n     Pattern pattern = Pattern.compile(\n-        \"^\" + getTrashRoot() + TRASH_PATTERN + \"/\");\n+        \"^\" + getTrashRoot() + TRASH_PATTERN);\n     return pattern.matcher(path).find();\n   }\n "
    },
    {
        "commit_id": "0417c1c633da188b4acf53f2ec9bf69c06c6d185",
        "commit_message": "HDFS-17263. RBF: Fix client ls trash path cannot get except default nameservices trash path (#6291) Contributed by liuguanghua.\n\nReviewed-by: Inigo Goiri <inigoiri@apache.org>\r\nReviewed-by: He Xiaoqiao <hexiaoqiao@apache.org>\r\nSigned-off-by: Shilun Fan <slfan1989@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/0417c1c633da188b4acf53f2ec9bf69c06c6d185",
        "buggy_code": "if (!namenodeListingExists && nnListing.size() == 0) {",
        "fixed_code": "if (!namenodeListingExists && nnListing.size() == 0 && children == null) {",
        "patch": "@@ -943,7 +943,7 @@ public DirectoryListing getListing(String src, byte[] startAfter,\n       }\n     }\n \n-    if (!namenodeListingExists && nnListing.size() == 0) {\n+    if (!namenodeListingExists && nnListing.size() == 0 && children == null) {\n       // NN returns a null object if the directory cannot be found and has no\n       // listing. If we didn't retrieve any NN listing data, and there are no\n       // mount points here, return null."
    },
    {
        "commit_id": "a079f6261d77512a4eeb9a1d10e667caaecde29c",
        "commit_message": "HADOOP-18917. Addendum. Fix deprecation issues after commons-io upgrade. (#6228). Contributed by PJ Fanning.",
        "commit_url": "https://github.com/apache/hadoop/commit/a079f6261d77512a4eeb9a1d10e667caaecde29c",
        "buggy_code": "PrintStream o = new PrintStream(NullOutputStream.NULL_OUTPUT_STREAM);",
        "fixed_code": "PrintStream o = new PrintStream(NullOutputStream.INSTANCE);",
        "patch": "@@ -257,7 +257,7 @@ public void testOfflineImageViewer() throws Exception {\n         FSImageTestUtil.getFSImage(\n         cluster.getNameNode()).getStorage().getStorageDir(0));\n     assertNotNull(\"Didn't generate or can't find fsimage\", originalFsimage);\n-    PrintStream o = new PrintStream(NullOutputStream.NULL_OUTPUT_STREAM);\n+    PrintStream o = new PrintStream(NullOutputStream.INSTANCE);\n     PBImageXmlWriter v = new PBImageXmlWriter(new Configuration(), o);\n     v.visit(new RandomAccessFile(originalFsimage, \"r\"));\n   }"
    },
    {
        "commit_id": "a079f6261d77512a4eeb9a1d10e667caaecde29c",
        "commit_message": "HADOOP-18917. Addendum. Fix deprecation issues after commons-io upgrade. (#6228). Contributed by PJ Fanning.",
        "commit_url": "https://github.com/apache/hadoop/commit/a079f6261d77512a4eeb9a1d10e667caaecde29c",
        "buggy_code": "PrintStream output = new PrintStream(NullOutputStream.NULL_OUTPUT_STREAM);",
        "fixed_code": "PrintStream output = new PrintStream(NullOutputStream.INSTANCE);",
        "patch": "@@ -405,7 +405,7 @@ private static FileStatus pathToFileEntry(FileSystem hdfs, String file)\n   @Test(expected = IOException.class)\n   public void testTruncatedFSImage() throws IOException {\n     File truncatedFile = new File(tempDir, \"truncatedFsImage\");\n-    PrintStream output = new PrintStream(NullOutputStream.NULL_OUTPUT_STREAM);\n+    PrintStream output = new PrintStream(NullOutputStream.INSTANCE);\n     copyPartOfFile(originalFsimage, truncatedFile);\n     try (RandomAccessFile r = new RandomAccessFile(truncatedFile, \"r\")) {\n       new FileDistributionCalculator(new Configuration(), 0, 0, false, output)"
    },
    {
        "commit_id": "8b974bcc1f084ae77dccf99ebc243e7a571f2e11",
        "commit_message": "HADOOP-18889. Third party storage followup. (#6186)\n\n\r\nFollowup to HADOOP-18889 third party store support;\r\n\r\nFix some minor review comments which came in after the merge.",
        "commit_url": "https://github.com/apache/hadoop/commit/8b974bcc1f084ae77dccf99ebc243e7a571f2e11",
        "buggy_code": "once(\"getBucketLocation()\", bucketName, () ->",
        "fixed_code": "invoker.retry(\"getBucketLocation()\", bucketName, true, () ->",
        "patch": "@@ -1357,7 +1357,7 @@ public String getBucketLocation() throws IOException {\n     public String getBucketLocation(String bucketName) throws IOException {\n       final String region = trackDurationAndSpan(\n           STORE_EXISTS_PROBE, bucketName, null, () ->\n-              once(\"getBucketLocation()\", bucketName, () ->\n+              invoker.retry(\"getBucketLocation()\", bucketName, true, () ->\n                   // If accessPoint then region is known from Arn\n                   accessPoint != null\n                       ? accessPoint.getRegion()"
    },
    {
        "commit_id": "615a2a42cf428dc687809508ef36a1fc5cba0099",
        "commit_message": "HDFS-17220. Fix same available space policy in AvailableSpaceVolumeChoosingPolicy (#6174)\n\nReviewed-by: He Xiaoqiao <hexiaoqiao@apache.org>\r\nReviewed-by: zhangshuyan <zqingchai@gmail.com>\r\nSigned-off-by: Tao Li <tomscut@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/615a2a42cf428dc687809508ef36a1fc5cba0099",
        "buggy_code": "return (mostAvailable - leastAvailable) < balancedSpaceThreshold;",
        "fixed_code": "return (mostAvailable - leastAvailable) <= balancedSpaceThreshold;",
        "patch": "@@ -209,7 +209,7 @@ public boolean areAllVolumesWithinFreeSpaceThreshold() {\n         leastAvailable = Math.min(leastAvailable, volume.getAvailable());\n         mostAvailable = Math.max(mostAvailable, volume.getAvailable());\n       }\n-      return (mostAvailable - leastAvailable) < balancedSpaceThreshold;\n+      return (mostAvailable - leastAvailable) <= balancedSpaceThreshold;\n     }\n     \n     /**"
    },
    {
        "commit_id": "9bc159f4ac3cbc3d3aa253c2e591dec17cd81648",
        "commit_message": "HADOOP-18487. Make protobuf 2.5 an optional runtime dependency. (#4996)\n\n\r\nProtobuf 2.5 JAR is no longer needed at runtime. \r\n\r\nThe option common.protobuf.scope defines whether the protobuf 2.5.0\r\ndependency is marked as provided or not.\r\n\r\n* New package org.apache.hadoop.ipc.internal for internal only protobuf classes\r\n  ...with a ShadedProtobufHelper in there which has shaded protobuf refs\r\n  only, so guaranteed not to need protobuf-2.5 on the CP\r\n* All uses of org.apache.hadoop.ipc.ProtobufHelper have\r\n  been replaced by uses of org.apache.hadoop.ipc.internal.ShadedProtobufHelper\r\n* The scope of protobuf-2.5 is set by the option common.protobuf2.scope\r\n  In this patch is it is still \"compile\"\r\n* There is explicit reference to it in modules where it may be needed.\r\n*  The maven scope of the dependency can be set with the common.protobuf2.scope\r\n   option. It can be set to \"provided\" in a build:\r\n       -Dcommon.protobuf2.scope=provided\r\n* Add new ipc(callable) method to catch and convert shaded protobuf\r\n  exceptions raised during invocation of the supplied lambda expression\r\n* This is adopted in the code where the migration is not traumatically\r\n  over-complex. RouterAdminProtocolTranslatorPB is left alone for this\r\n  reason.\r\n\r\nContributed by Steve Loughran",
        "commit_url": "https://github.com/apache/hadoop/commit/9bc159f4ac3cbc3d3aa253c2e591dec17cd81648",
        "buggy_code": "return (E) caught;",
        "fixed_code": "return (E) cause;",
        "patch": "@@ -819,7 +819,7 @@ public static <E extends Throwable> E verifyCause(\n     if (cause == null || !clazz.isAssignableFrom(cause.getClass())) {\n       throw caught;\n     } else {\n-      return (E) caught;\n+      return (E) cause;\n     }\n   }\n "
    },
    {
        "commit_id": "9bc159f4ac3cbc3d3aa253c2e591dec17cd81648",
        "commit_message": "HADOOP-18487. Make protobuf 2.5 an optional runtime dependency. (#4996)\n\n\r\nProtobuf 2.5 JAR is no longer needed at runtime. \r\n\r\nThe option common.protobuf.scope defines whether the protobuf 2.5.0\r\ndependency is marked as provided or not.\r\n\r\n* New package org.apache.hadoop.ipc.internal for internal only protobuf classes\r\n  ...with a ShadedProtobufHelper in there which has shaded protobuf refs\r\n  only, so guaranteed not to need protobuf-2.5 on the CP\r\n* All uses of org.apache.hadoop.ipc.ProtobufHelper have\r\n  been replaced by uses of org.apache.hadoop.ipc.internal.ShadedProtobufHelper\r\n* The scope of protobuf-2.5 is set by the option common.protobuf2.scope\r\n  In this patch is it is still \"compile\"\r\n* There is explicit reference to it in modules where it may be needed.\r\n*  The maven scope of the dependency can be set with the common.protobuf2.scope\r\n   option. It can be set to \"provided\" in a build:\r\n       -Dcommon.protobuf2.scope=provided\r\n* Add new ipc(callable) method to catch and convert shaded protobuf\r\n  exceptions raised during invocation of the supplied lambda expression\r\n* This is adopted in the code where the migration is not traumatically\r\n  over-complex. RouterAdminProtocolTranslatorPB is left alone for this\r\n  reason.\r\n\r\nContributed by Steve Loughran",
        "commit_url": "https://github.com/apache/hadoop/commit/9bc159f4ac3cbc3d3aa253c2e591dec17cd81648",
        "buggy_code": "DFSUtil.addPBProtocol(confCopy, InterQJournalProtocolPB.class,",
        "fixed_code": "DFSUtil.addInternalPBProtocol(confCopy, InterQJournalProtocolPB.class,",
        "patch": "@@ -125,7 +125,7 @@ public class JournalNodeRpcServer implements QJournalProtocol,\n     BlockingService interQJournalProtocolService = InterQJournalProtocolService\n         .newReflectiveBlockingService(qJournalProtocolServerSideTranslatorPB);\n \n-    DFSUtil.addPBProtocol(confCopy, InterQJournalProtocolPB.class,\n+    DFSUtil.addInternalPBProtocol(confCopy, InterQJournalProtocolPB.class,\n         interQJournalProtocolService, server);\n \n "
    },
    {
        "commit_id": "9bc159f4ac3cbc3d3aa253c2e591dec17cd81648",
        "commit_message": "HADOOP-18487. Make protobuf 2.5 an optional runtime dependency. (#4996)\n\n\r\nProtobuf 2.5 JAR is no longer needed at runtime. \r\n\r\nThe option common.protobuf.scope defines whether the protobuf 2.5.0\r\ndependency is marked as provided or not.\r\n\r\n* New package org.apache.hadoop.ipc.internal for internal only protobuf classes\r\n  ...with a ShadedProtobufHelper in there which has shaded protobuf refs\r\n  only, so guaranteed not to need protobuf-2.5 on the CP\r\n* All uses of org.apache.hadoop.ipc.ProtobufHelper have\r\n  been replaced by uses of org.apache.hadoop.ipc.internal.ShadedProtobufHelper\r\n* The scope of protobuf-2.5 is set by the option common.protobuf2.scope\r\n  In this patch is it is still \"compile\"\r\n* There is explicit reference to it in modules where it may be needed.\r\n*  The maven scope of the dependency can be set with the common.protobuf2.scope\r\n   option. It can be set to \"provided\" in a build:\r\n       -Dcommon.protobuf2.scope=provided\r\n* Add new ipc(callable) method to catch and convert shaded protobuf\r\n  exceptions raised during invocation of the supplied lambda expression\r\n* This is adopted in the code where the migration is not traumatically\r\n  over-complex. RouterAdminProtocolTranslatorPB is left alone for this\r\n  reason.\r\n\r\nContributed by Steve Loughran",
        "commit_url": "https://github.com/apache/hadoop/commit/9bc159f4ac3cbc3d3aa253c2e591dec17cd81648",
        "buggy_code": "DFSUtil.addPBProtocol(conf, JournalProtocolPB.class, service,",
        "fixed_code": "DFSUtil.addInternalPBProtocol(conf, JournalProtocolPB.class, service,",
        "patch": "@@ -246,7 +246,7 @@ private BackupNodeRpcServer(Configuration conf, BackupNode nn)\n           new JournalProtocolServerSideTranslatorPB(this);\n       BlockingService service = JournalProtocolService\n           .newReflectiveBlockingService(journalProtocolTranslator);\n-      DFSUtil.addPBProtocol(conf, JournalProtocolPB.class, service,\n+      DFSUtil.addInternalPBProtocol(conf, JournalProtocolPB.class, service,\n           this.clientRpcServer);\n     }\n     "
    },
    {
        "commit_id": "81edbebdd80dc590d2c4fa8e8fb95a0e91c8115d",
        "commit_message": "HADOOP-18889. S3A v2 SDK third party support (#6141)\n\n\r\nTune AWS v2 SDK changes based on testing with third party stores\r\nincluding GCS. \r\n\r\nContains HADOOP-18889. S3A v2 SDK error translations and troubleshooting docs\r\n\r\n* Changes needed to work with multiple third party stores\r\n* New third_party_stores document on how to bind to and test\r\n  third party stores, including google gcs (which works!)\r\n* Troubleshooting docs mostly updated for v2 SDK\r\n\r\nException translation/resilience\r\n\r\n* New AWSUnsupportedFeatureException for unsupported/unavailable errors\r\n* Handle 501 method unimplemented as one of these\r\n* Error codes > 500 mapped to the AWSStatus500Exception if no explicit\r\n  handler.\r\n* Precondition errors handled a bit better\r\n* GCS throttle exception also recognized.\r\n* GCS raises 404 on a delete of a file which doesn't exist: swallow it.\r\n* Error translation uses reflection to create IOE of the right type.\r\n  All IOEs at the bottom of an AWS stack chain are regenerated.\r\n  then a new exception of that specific type is created, with the top level ex\r\n  its cause. This is done to retain the whole stack chain.\r\n* Reduce the number of retries within the AWS SDK\r\n* And those of s3a code.\r\n* S3ARetryPolicy explicitly declare SocketException as connectivity failure\r\n  but subclasses BindException\r\n* SocketTimeoutException also considered connectivity  \r\n* Log at debug whenever retry policies looked up\r\n* Reorder exceptions to alphabetical order, with commentary\r\n* Review use of the Invoke.retry() method \r\n\r\n The reduction in retries is because its clear when you try to create a bucket\r\n which doesn't resolve that the time for even an UnknownHostException to\r\n eventually fail over 90s, which then hit the s3a retry code.\r\n - Reducing the SDK retries means these escalate to our code better.\r\n - Cutting back on our own retries makes it a bit more responsive for most real\r\n deployments.\r\n - maybeTranslateNetworkException() and s3a retry policy means that\r\n   unknown host exception is recognised and fails fast.\r\n\r\nContributed by Steve Loughran",
        "commit_url": "https://github.com/apache/hadoop/commit/81edbebdd80dc590d2c4fa8e8fb95a0e91c8115d",
        "buggy_code": "\", secretKey.empty'\" + secretKey.isEmpty() +",
        "fixed_code": "\", secretKey.empty=\" + secretKey.isEmpty() +",
        "patch": "@@ -90,7 +90,7 @@ public AwsCredentials resolveCredentials() {\n   public String toString() {\n     return \"SimpleAWSCredentialsProvider{\" +\n         \"accessKey.empty=\" + accessKey.isEmpty() +\n-        \", secretKey.empty'\" + secretKey.isEmpty() +\n+        \", secretKey.empty=\" + secretKey.isEmpty() +\n         '}';\n   }\n "
    },
    {
        "commit_id": "81edbebdd80dc590d2c4fa8e8fb95a0e91c8115d",
        "commit_message": "HADOOP-18889. S3A v2 SDK third party support (#6141)\n\n\r\nTune AWS v2 SDK changes based on testing with third party stores\r\nincluding GCS. \r\n\r\nContains HADOOP-18889. S3A v2 SDK error translations and troubleshooting docs\r\n\r\n* Changes needed to work with multiple third party stores\r\n* New third_party_stores document on how to bind to and test\r\n  third party stores, including google gcs (which works!)\r\n* Troubleshooting docs mostly updated for v2 SDK\r\n\r\nException translation/resilience\r\n\r\n* New AWSUnsupportedFeatureException for unsupported/unavailable errors\r\n* Handle 501 method unimplemented as one of these\r\n* Error codes > 500 mapped to the AWSStatus500Exception if no explicit\r\n  handler.\r\n* Precondition errors handled a bit better\r\n* GCS throttle exception also recognized.\r\n* GCS raises 404 on a delete of a file which doesn't exist: swallow it.\r\n* Error translation uses reflection to create IOE of the right type.\r\n  All IOEs at the bottom of an AWS stack chain are regenerated.\r\n  then a new exception of that specific type is created, with the top level ex\r\n  its cause. This is done to retain the whole stack chain.\r\n* Reduce the number of retries within the AWS SDK\r\n* And those of s3a code.\r\n* S3ARetryPolicy explicitly declare SocketException as connectivity failure\r\n  but subclasses BindException\r\n* SocketTimeoutException also considered connectivity  \r\n* Log at debug whenever retry policies looked up\r\n* Reorder exceptions to alphabetical order, with commentary\r\n* Review use of the Invoke.retry() method \r\n\r\n The reduction in retries is because its clear when you try to create a bucket\r\n which doesn't resolve that the time for even an UnknownHostException to\r\n eventually fail over 90s, which then hit the s3a retry code.\r\n - Reducing the SDK retries means these escalate to our code better.\r\n - Cutting back on our own retries makes it a bit more responsive for most real\r\n deployments.\r\n - maybeTranslateNetworkException() and s3a retry policy means that\r\n   unknown host exception is recognised and fails fast.\r\n\r\nContributed by Steve Loughran",
        "commit_url": "https://github.com/apache/hadoop/commit/81edbebdd80dc590d2c4fa8e8fb95a0e91c8115d",
        "buggy_code": "\"Multipart Upload Abort Unner Path Invoked\",",
        "fixed_code": "\"Multipart Upload Abort Under Path Invoked\",",
        "patch": "@@ -582,7 +582,7 @@ public enum Statistic {\n       TYPE_COUNTER),\n   MULTIPART_UPLOAD_ABORT_UNDER_PATH_INVOKED(\n       StoreStatisticNames.MULTIPART_UPLOAD_ABORT_UNDER_PATH_INVOKED,\n-      \"Multipart Upload Abort Unner Path Invoked\",\n+      \"Multipart Upload Abort Under Path Invoked\",\n       TYPE_COUNTER),\n   MULTIPART_UPLOAD_COMPLETED(\n       StoreStatisticNames.MULTIPART_UPLOAD_COMPLETED,"
    },
    {
        "commit_id": "594e9f29f5d89563bd9afc2e45553d624a6accc7",
        "commit_message": "HADOOP-18869: [ABFS] Fix behavior of a File System APIs on root path (#6003)\n\nContributed by Anuj Modi",
        "commit_url": "https://github.com/apache/hadoop/commit/594e9f29f5d89563bd9afc2e45553d624a6accc7",
        "buggy_code": "LOG.debug(\"setFilesystemProperties for filesystem: {} path: {} with properties: {}\",",
        "fixed_code": "LOG.debug(\"setPathProperties for filesystem: {} path: {} with properties: {}\",",
        "patch": "@@ -494,7 +494,7 @@ public void setPathProperties(final Path path,\n       final Hashtable<String, String> properties, TracingContext tracingContext)\n       throws AzureBlobFileSystemException {\n     try (AbfsPerfInfo perfInfo = startTracking(\"setPathProperties\", \"setPathProperties\")){\n-      LOG.debug(\"setFilesystemProperties for filesystem: {} path: {} with properties: {}\",\n+      LOG.debug(\"setPathProperties for filesystem: {} path: {} with properties: {}\",\n               client.getFileSystem(),\n               path,\n               properties);"
    },
    {
        "commit_id": "882378c3e9d6bd16884655927a290586350782bb",
        "commit_message": "Revert \"HADOOP-18869: [ABFS] Fix behavior of a File System APIs on root path (#6003)\"\n\nThis reverts commit 6c6df40d35e69f0340ab7a9afae3f96be1f497ca.\n\n...so as to give the correct credit",
        "commit_url": "https://github.com/apache/hadoop/commit/882378c3e9d6bd16884655927a290586350782bb",
        "buggy_code": "LOG.debug(\"setPathProperties for filesystem: {} path: {} with properties: {}\",",
        "fixed_code": "LOG.debug(\"setFilesystemProperties for filesystem: {} path: {} with properties: {}\",",
        "patch": "@@ -494,7 +494,7 @@ public void setPathProperties(final Path path,\n       final Hashtable<String, String> properties, TracingContext tracingContext)\n       throws AzureBlobFileSystemException {\n     try (AbfsPerfInfo perfInfo = startTracking(\"setPathProperties\", \"setPathProperties\")){\n-      LOG.debug(\"setPathProperties for filesystem: {} path: {} with properties: {}\",\n+      LOG.debug(\"setFilesystemProperties for filesystem: {} path: {} with properties: {}\",\n               client.getFileSystem(),\n               path,\n               properties);"
    },
    {
        "commit_id": "6c6df40d35e69f0340ab7a9afae3f96be1f497ca",
        "commit_message": "HADOOP-18869: [ABFS] Fix behavior of a File System APIs on root path (#6003)\n\n\r\nContributed by  Anmol Asrani",
        "commit_url": "https://github.com/apache/hadoop/commit/6c6df40d35e69f0340ab7a9afae3f96be1f497ca",
        "buggy_code": "LOG.debug(\"setFilesystemProperties for filesystem: {} path: {} with properties: {}\",",
        "fixed_code": "LOG.debug(\"setPathProperties for filesystem: {} path: {} with properties: {}\",",
        "patch": "@@ -494,7 +494,7 @@ public void setPathProperties(final Path path,\n       final Hashtable<String, String> properties, TracingContext tracingContext)\n       throws AzureBlobFileSystemException {\n     try (AbfsPerfInfo perfInfo = startTracking(\"setPathProperties\", \"setPathProperties\")){\n-      LOG.debug(\"setFilesystemProperties for filesystem: {} path: {} with properties: {}\",\n+      LOG.debug(\"setPathProperties for filesystem: {} path: {} with properties: {}\",\n               client.getFileSystem(),\n               path,\n               properties);"
    },
    {
        "commit_id": "f3a27f2b228972aacb9f233597143a6d9f359d60",
        "commit_message": "YARN-11579. Fix 'Physical Mem Used' and 'Physical VCores Used' are not displaying data. (#6123) Contributed by Shilun Fan.\n\nReviewed-by: Inigo Goiri <inigoiri@apache.org>\r\nSigned-off-by: Shilun Fan <slfan1989@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/f3a27f2b228972aacb9f233597143a6d9f359d60",
        "buggy_code": "assertEquals(\"incorrect number of elements\", 35, clusterinfo.length());",
        "fixed_code": "assertEquals(\"incorrect number of elements\", 37, clusterinfo.length());",
        "patch": "@@ -481,7 +481,7 @@ public void verifyClusterMetricsJSON(JSONObject json) throws JSONException,\n       Exception {\n     assertEquals(\"incorrect number of elements\", 1, json.length());\n     JSONObject clusterinfo = json.getJSONObject(\"clusterMetrics\");\n-    assertEquals(\"incorrect number of elements\", 35, clusterinfo.length());\n+    assertEquals(\"incorrect number of elements\", 37, clusterinfo.length());\n     verifyClusterMetrics(\n         clusterinfo.getInt(\"appsSubmitted\"), clusterinfo.getInt(\"appsCompleted\"),\n         clusterinfo.getInt(\"reservedMB\"), clusterinfo.getInt(\"availableMB\"),"
    },
    {
        "commit_id": "9e489b9ab5367b431849f957d0fe1ffadc43cd47",
        "commit_message": "HDFS-17190. EC: Fix bug of OIV processing XAttr. (#6067). Contributed by Shuyan Zhang.\n\nSigned-off-by: He Xiaoqiao <hexiaoqiao@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/9e489b9ab5367b431849f957d0fe1ffadc43cd47",
        "buggy_code": "static int toInt(XAttr a) {",
        "fixed_code": "public static int toInt(XAttr a) {",
        "patch": "@@ -72,7 +72,7 @@ public static String getName(int record) {\n     return SerialNumberManager.XATTR.getString(nid);\n   }\n \n-  static int toInt(XAttr a) {\n+  public static int toInt(XAttr a) {\n     int nid = SerialNumberManager.XATTR.getSerialNumber(a.getName());\n     int nsOrd = a.getNameSpace().ordinal();\n     long value = NS.BITS.combine(nsOrd & NS_MASK, 0L);"
    },
    {
        "commit_id": "81d90fd65b3c0c7ac66ebae78e22dcb240a0547b",
        "commit_message": "HADOOP-18073. S3A: Upgrade AWS SDK to V2 (#5995)\n\n\r\nThis patch migrates the S3A connector to use the V2 AWS SDK.\r\n\r\nThis is a significant change at the source code level.\r\nAny applications using the internal extension/override points in\r\nthe filesystem connector are likely to break.\r\n\r\nThis includes but is not limited to:\r\n- Code invoking methods on the S3AFileSystem class\r\n  which used classes from the V1 SDK.\r\n- The ability to define the factory for the `AmazonS3` client, and\r\n  to retrieve it from the S3AFileSystem. There is a new factory\r\n  API and a special interface S3AInternals to access a limited\r\n  set of internal classes and operations.\r\n- Delegation token and auditing extensions.\r\n- Classes trying to integrate with the AWS SDK.\r\n\r\nAll standard V1 credential providers listed in the option \r\nfs.s3a.aws.credentials.provider will be automatically remapped to their\r\nV2 equivalent.\r\n\r\nOther V1 Credential Providers are supported, but only if the V1 SDK is\r\nadded back to the classpath.  \r\n\r\nThe SDK Signing plugin has changed; all v1 signers are incompatible.\r\nThere is no support for the S3 \"v2\" signing algorithm.\r\n\r\nFinally, the aws-sdk-bundle JAR has been replaced by the shaded V2\r\nequivalent, \"bundle.jar\", which is now exported by the hadoop-aws module.\r\n\r\nConsult the document aws_sdk_upgrade for the full details.\r\n\r\nContributed by Ahmar Suhail + some bits by Steve Loughran",
        "commit_url": "https://github.com/apache/hadoop/commit/81d90fd65b3c0c7ac66ebae78e22dcb240a0547b",
        "buggy_code": "LoggerFactory.getLogger(InconsistentAmazonS3Client.class);",
        "fixed_code": "LoggerFactory.getLogger(FailureInjectionPolicy.class);",
        "patch": "@@ -36,7 +36,7 @@ public class FailureInjectionPolicy {\n   public static final String DEFAULT_DELAY_KEY_SUBSTRING = \"DELAY_LISTING_ME\";\n \n   private static final Logger LOG =\n-      LoggerFactory.getLogger(InconsistentAmazonS3Client.class);\n+      LoggerFactory.getLogger(FailureInjectionPolicy.class);\n \n   /**\n    * Probability of throttling a request."
    },
    {
        "commit_id": "81d90fd65b3c0c7ac66ebae78e22dcb240a0547b",
        "commit_message": "HADOOP-18073. S3A: Upgrade AWS SDK to V2 (#5995)\n\n\r\nThis patch migrates the S3A connector to use the V2 AWS SDK.\r\n\r\nThis is a significant change at the source code level.\r\nAny applications using the internal extension/override points in\r\nthe filesystem connector are likely to break.\r\n\r\nThis includes but is not limited to:\r\n- Code invoking methods on the S3AFileSystem class\r\n  which used classes from the V1 SDK.\r\n- The ability to define the factory for the `AmazonS3` client, and\r\n  to retrieve it from the S3AFileSystem. There is a new factory\r\n  API and a special interface S3AInternals to access a limited\r\n  set of internal classes and operations.\r\n- Delegation token and auditing extensions.\r\n- Classes trying to integrate with the AWS SDK.\r\n\r\nAll standard V1 credential providers listed in the option \r\nfs.s3a.aws.credentials.provider will be automatically remapped to their\r\nV2 equivalent.\r\n\r\nOther V1 Credential Providers are supported, but only if the V1 SDK is\r\nadded back to the classpath.  \r\n\r\nThe SDK Signing plugin has changed; all v1 signers are incompatible.\r\nThere is no support for the S3 \"v2\" signing algorithm.\r\n\r\nFinally, the aws-sdk-bundle JAR has been replaced by the shaded V2\r\nequivalent, \"bundle.jar\", which is now exported by the hadoop-aws module.\r\n\r\nConsult the document aws_sdk_upgrade for the full details.\r\n\r\nContributed by Ahmar Suhail + some bits by Steve Loughran",
        "commit_url": "https://github.com/apache/hadoop/commit/81d90fd65b3c0c7ac66ebae78e22dcb240a0547b",
        "buggy_code": "import com.amazonaws.services.securitytoken.model.Credentials;",
        "fixed_code": "import software.amazon.awssdk.services.sts.model.Credentials;",
        "patch": "@@ -23,7 +23,7 @@\n import java.util.UUID;\n import java.util.concurrent.TimeUnit;\n \n-import com.amazonaws.services.securitytoken.model.Credentials;\n+import software.amazon.awssdk.services.sts.model.Credentials;\n import org.apache.hadoop.classification.VisibleForTesting;\n import org.apache.hadoop.util.Preconditions;\n import org.slf4j.Logger;"
    },
    {
        "commit_id": "81d90fd65b3c0c7ac66ebae78e22dcb240a0547b",
        "commit_message": "HADOOP-18073. S3A: Upgrade AWS SDK to V2 (#5995)\n\n\r\nThis patch migrates the S3A connector to use the V2 AWS SDK.\r\n\r\nThis is a significant change at the source code level.\r\nAny applications using the internal extension/override points in\r\nthe filesystem connector are likely to break.\r\n\r\nThis includes but is not limited to:\r\n- Code invoking methods on the S3AFileSystem class\r\n  which used classes from the V1 SDK.\r\n- The ability to define the factory for the `AmazonS3` client, and\r\n  to retrieve it from the S3AFileSystem. There is a new factory\r\n  API and a special interface S3AInternals to access a limited\r\n  set of internal classes and operations.\r\n- Delegation token and auditing extensions.\r\n- Classes trying to integrate with the AWS SDK.\r\n\r\nAll standard V1 credential providers listed in the option \r\nfs.s3a.aws.credentials.provider will be automatically remapped to their\r\nV2 equivalent.\r\n\r\nOther V1 Credential Providers are supported, but only if the V1 SDK is\r\nadded back to the classpath.  \r\n\r\nThe SDK Signing plugin has changed; all v1 signers are incompatible.\r\nThere is no support for the S3 \"v2\" signing algorithm.\r\n\r\nFinally, the aws-sdk-bundle JAR has been replaced by the shaded V2\r\nequivalent, \"bundle.jar\", which is now exported by the hadoop-aws module.\r\n\r\nConsult the document aws_sdk_upgrade for the full details.\r\n\r\nContributed by Ahmar Suhail + some bits by Steve Loughran",
        "commit_url": "https://github.com/apache/hadoop/commit/81d90fd65b3c0c7ac66ebae78e22dcb240a0547b",
        "buggy_code": "= \"Service: Amazon S3; Status Code: 403;\";",
        "fixed_code": "= \"Service: S3, Status Code: 403\";",
        "patch": "@@ -63,7 +63,7 @@\n public class ITestS3AEncryptionSSEC extends AbstractTestS3AEncryption {\n \n   private static final String SERVICE_AMAZON_S3_STATUS_CODE_403\n-      = \"Service: Amazon S3; Status Code: 403;\";\n+      = \"Service: S3, Status Code: 403\";\n   private static final String KEY_1\n       = \"4niV/jPK5VFRHY+KNb6wtqYd4xXyMgdJ9XQJpcQUVbs=\";\n   private static final String KEY_2"
    },
    {
        "commit_id": "81d90fd65b3c0c7ac66ebae78e22dcb240a0547b",
        "commit_message": "HADOOP-18073. S3A: Upgrade AWS SDK to V2 (#5995)\n\n\r\nThis patch migrates the S3A connector to use the V2 AWS SDK.\r\n\r\nThis is a significant change at the source code level.\r\nAny applications using the internal extension/override points in\r\nthe filesystem connector are likely to break.\r\n\r\nThis includes but is not limited to:\r\n- Code invoking methods on the S3AFileSystem class\r\n  which used classes from the V1 SDK.\r\n- The ability to define the factory for the `AmazonS3` client, and\r\n  to retrieve it from the S3AFileSystem. There is a new factory\r\n  API and a special interface S3AInternals to access a limited\r\n  set of internal classes and operations.\r\n- Delegation token and auditing extensions.\r\n- Classes trying to integrate with the AWS SDK.\r\n\r\nAll standard V1 credential providers listed in the option \r\nfs.s3a.aws.credentials.provider will be automatically remapped to their\r\nV2 equivalent.\r\n\r\nOther V1 Credential Providers are supported, but only if the V1 SDK is\r\nadded back to the classpath.  \r\n\r\nThe SDK Signing plugin has changed; all v1 signers are incompatible.\r\nThere is no support for the S3 \"v2\" signing algorithm.\r\n\r\nFinally, the aws-sdk-bundle JAR has been replaced by the shaded V2\r\nequivalent, \"bundle.jar\", which is now exported by the hadoop-aws module.\r\n\r\nConsult the document aws_sdk_upgrade for the full details.\r\n\r\nContributed by Ahmar Suhail + some bits by Steve Loughran",
        "commit_url": "https://github.com/apache/hadoop/commit/81d90fd65b3c0c7ac66ebae78e22dcb240a0547b",
        "buggy_code": "\"403 Forbidden\",",
        "fixed_code": "\"403\",",
        "patch": "@@ -107,7 +107,7 @@ public void testRequesterPaysDisabledFails() throws Throwable {\n     try (FileSystem fs = requesterPaysPath.getFileSystem(conf)) {\n       intercept(\n           AccessDeniedException.class,\n-          \"403 Forbidden\",\n+          \"403\",\n           \"Expected requester pays bucket to fail without header set\",\n           () -> fs.open(requesterPaysPath).close()\n       );"
    },
    {
        "commit_id": "1046f9cf9888155c27923f3f56efa107d908ad5b",
        "commit_message": "HADOOP-18860. Upgrade mockito version to 4.11.0 (#5977)\n\n\r\nAs well as the POM update, this patch moves to the (renamed) verify methods. \r\nBackporting mockito test changes may now require cherrypicking this patch, otherwise\r\nuse the old method names.\r\n\r\nContributed by Anmol Asrani",
        "commit_url": "https://github.com/apache/hadoop/commit/1046f9cf9888155c27923f3f56efa107d908ad5b",
        "buggy_code": "verifyZeroInteractions(logger);",
        "fixed_code": "verifyNoInteractions(logger);",
        "patch": "@@ -151,7 +151,7 @@ public Writable call(\n \n     // Nothing should be logged for a suppressed exception.\n     server.logException(logger, new TestException1(), dummyCall);\n-    verifyZeroInteractions(logger);\n+    verifyNoInteractions(logger);\n \n     // No stack trace should be logged for a terse exception.\n     server.logException(logger, new TestException2(), dummyCall);"
    },
    {
        "commit_id": "1046f9cf9888155c27923f3f56efa107d908ad5b",
        "commit_message": "HADOOP-18860. Upgrade mockito version to 4.11.0 (#5977)\n\n\r\nAs well as the POM update, this patch moves to the (renamed) verify methods. \r\nBackporting mockito test changes may now require cherrypicking this patch, otherwise\r\nuse the old method names.\r\n\r\nContributed by Anmol Asrani",
        "commit_url": "https://github.com/apache/hadoop/commit/1046f9cf9888155c27923f3f56efa107d908ad5b",
        "buggy_code": "import static org.mockito.Matchers.any;",
        "fixed_code": "import static org.mockito.ArgumentMatchers.any;",
        "patch": "@@ -24,7 +24,7 @@\n import static org.junit.Assert.assertNotNull;\n import static org.junit.Assert.assertTrue;\n import static org.junit.Assert.fail;\n-import static org.mockito.Matchers.any;\n+import static org.mockito.ArgumentMatchers.any;\n import static org.mockito.Mockito.doThrow;\n import static org.mockito.Mockito.mock;\n import static org.apache.hadoop.test.Whitebox.getInternalState;"
    },
    {
        "commit_id": "1046f9cf9888155c27923f3f56efa107d908ad5b",
        "commit_message": "HADOOP-18860. Upgrade mockito version to 4.11.0 (#5977)\n\n\r\nAs well as the POM update, this patch moves to the (renamed) verify methods. \r\nBackporting mockito test changes may now require cherrypicking this patch, otherwise\r\nuse the old method names.\r\n\r\nContributed by Anmol Asrani",
        "commit_url": "https://github.com/apache/hadoop/commit/1046f9cf9888155c27923f3f56efa107d908ad5b",
        "buggy_code": "Mockito.verifyZeroInteractions(locations);",
        "fixed_code": "Mockito.verifyNoInteractions(locations);",
        "patch": "@@ -1575,7 +1575,7 @@ public void testNoLookupsWhenNotUsed() throws Exception {\n     CacheManager cm = cluster.getNamesystem().getCacheManager();\n     LocatedBlocks locations = Mockito.mock(LocatedBlocks.class);\n     cm.setCachedLocations(locations);\n-    Mockito.verifyZeroInteractions(locations);\n+    Mockito.verifyNoInteractions(locations);\n   }\n \n   @Test(timeout=120000)"
    },
    {
        "commit_id": "1046f9cf9888155c27923f3f56efa107d908ad5b",
        "commit_message": "HADOOP-18860. Upgrade mockito version to 4.11.0 (#5977)\n\n\r\nAs well as the POM update, this patch moves to the (renamed) verify methods. \r\nBackporting mockito test changes may now require cherrypicking this patch, otherwise\r\nuse the old method names.\r\n\r\nContributed by Anmol Asrani",
        "commit_url": "https://github.com/apache/hadoop/commit/1046f9cf9888155c27923f3f56efa107d908ad5b",
        "buggy_code": "Mockito.verifyZeroInteractions(iip);",
        "fixed_code": "Mockito.verifyNoInteractions(iip);",
        "patch": "@@ -447,6 +447,6 @@ public void testShortCircuitSnapshotSearch() throws SnapshotException {\n     INodesInPath iip = Mockito.mock(INodesInPath.class);\n     List<INodeDirectory> snapDirs = new ArrayList<>();\n     FSDirSnapshotOp.checkSnapshot(fsn.getFSDirectory(), iip, snapDirs);\n-    Mockito.verifyZeroInteractions(iip);\n+    Mockito.verifyNoInteractions(iip);\n   }\n }"
    },
    {
        "commit_id": "9cfe9ccd2627571452ac2ac55a40d1f7ded07eb2",
        "commit_message": "HDFS-17119. RBF: Logger fix for StateStoreMySQLImpl. (#5882). Contributed by Zhaohui Wang.\n\nReviewed-by: Simbarashe Dzinamarira <sdzinamarira@linkedin.com>\r\nSigned-off-by: Ayush Saxena <ayushsaxena@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/9cfe9ccd2627571452ac2ac55a40d1f7ded07eb2",
        "buggy_code": "LoggerFactory.getLogger(StateStoreSerializableImpl.class);",
        "fixed_code": "LoggerFactory.getLogger(StateStoreMySQLImpl.class);",
        "patch": "@@ -67,7 +67,7 @@ public class StateStoreMySQLImpl extends StateStoreSerializableImpl {\n       SQL_STATE_STORE_CONF_PREFIX + \"connection.driver\";\n \n   private static final Logger LOG =\n-      LoggerFactory.getLogger(StateStoreSerializableImpl.class);\n+      LoggerFactory.getLogger(StateStoreMySQLImpl.class);\n   private SQLConnectionFactory connectionFactory;\n   /** If the driver has been initialized. */\n   private boolean initialized = false;"
    },
    {
        "commit_id": "fbe9a2924695ad1af63096b818270a81801441c9",
        "commit_message": "YARN-11540. Fix typo: form -> from (#5861). Contributed by Seokchan Yoon.\n\nReviewed-by: Tao Li <tomscut@apache.org>\r\nSigned-off-by: Ayush Saxena <ayushsaxena@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/fbe9a2924695ad1af63096b818270a81801441c9",
        "buggy_code": "LOG.debug(\"Retrieved credentials form RM for {}: {}\",",
        "fixed_code": "LOG.debug(\"Retrieved credentials from RM for {}: {}\",",
        "patch": "@@ -818,7 +818,7 @@ private static Map<ApplicationId, Credentials> parseCredentials(\n     }\n     if (LOG.isDebugEnabled()) {\n       for (Map.Entry<ApplicationId, Credentials> entry : map.entrySet()) {\n-        LOG.debug(\"Retrieved credentials form RM for {}: {}\",\n+        LOG.debug(\"Retrieved credentials from RM for {}: {}\",\n             entry.getKey(), entry.getValue().getAllTokens());\n       }\n     }"
    },
    {
        "commit_id": "8dd9c874e1478b653f9610a923b5bcaf9a422b4a",
        "commit_message": "HDFS-17086. Fix the parameter settings in TestDiskspaceQuotaUpdate#updateCountForQuota (#5842). Contributed by Haiyang Hu.\n\nReviewed-by: Shilun Fan <slfan1989@apache.org>\r\nSigned-off-by: Ayush Saxena <ayushsaxena@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/8dd9c874e1478b653f9610a923b5bcaf9a422b4a",
        "buggy_code": "getFSDirectory().updateCountForQuota(1);",
        "fixed_code": "getFSDirectory().updateCountForQuota(i);",
        "patch": "@@ -394,7 +394,7 @@ private void updateCountForQuota(int i) {\n     FSNamesystem fsn = cluster.getNamesystem();\n     fsn.writeLock();\n     try {\n-      getFSDirectory().updateCountForQuota(1);\n+      getFSDirectory().updateCountForQuota(i);\n     } finally {\n       fsn.writeUnlock();\n     }"
    },
    {
        "commit_id": "e6937d707603d1d69e80a8641bca096e41db091a",
        "commit_message": "YARN-11425. [Hotfix] YARN-11425. Modify Expiration Time Unit error. (#5712)",
        "commit_url": "https://github.com/apache/hadoop/commit/e6937d707603d1d69e80a8641bca096e41db091a",
        "buggy_code": "YarnConfiguration.DEFAULT_ROUTER_SUBCLUSTER_EXPIRATION_TIME, TimeUnit.MINUTES);",
        "fixed_code": "YarnConfiguration.DEFAULT_ROUTER_SUBCLUSTER_EXPIRATION_TIME, TimeUnit.MILLISECONDS);",
        "patch": "@@ -47,7 +47,7 @@ public SubClusterCleaner(Configuration conf) {\n     federationFacade = FederationStateStoreFacade.getInstance();\n     this.heartbeatExpirationMillis =\n         conf.getTimeDuration(YarnConfiguration.ROUTER_SUBCLUSTER_EXPIRATION_TIME,\n-        YarnConfiguration.DEFAULT_ROUTER_SUBCLUSTER_EXPIRATION_TIME, TimeUnit.MINUTES);\n+        YarnConfiguration.DEFAULT_ROUTER_SUBCLUSTER_EXPIRATION_TIME, TimeUnit.MILLISECONDS);\n   }\n \n   @Override"
    },
    {
        "commit_id": "5d6ca13c5cfb661caebb78978f4d58e723f031c6",
        "commit_message": "HDFS-16983. Fix concat operation doesn't honor dfs.permissions.enabled (#5561). Contributed by caozhiqiang.\n\nReviewed-by: zhangshuyan <zqingchai@gmail.com>\r\nReviewed-by: He Xiaoqiao <hexiaoqiao@apache.org>\r\nSigned-off-by: Ayush Saxena <ayushsaxena@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/5d6ca13c5cfb661caebb78978f4d58e723f031c6",
        "buggy_code": "if (pc != null) {",
        "fixed_code": "if (pc != null && fsd.isPermissionEnabled()) {",
        "patch": "@@ -121,7 +121,7 @@ private static INodeFile[] verifySrcFiles(FSDirectory fsd, String[] srcs,\n     for(String src : srcs) {\n       final INodesInPath iip = fsd.resolvePath(pc, src, DirOp.WRITE);\n       // permission check for srcs\n-      if (pc != null) {\n+      if (pc != null && fsd.isPermissionEnabled()) {\n         fsd.checkPathAccess(pc, iip, FsAction.READ); // read the file\n         fsd.checkParentAccess(pc, iip, FsAction.WRITE); // for delete\n       }"
    },
    {
        "commit_id": "9acf462d268a7dee8d031978add0034fcf77f861",
        "commit_message": "HDFS-17000. Fix faulty loop condition in TestDFSStripedOutputStreamUpdatePipeline (#5699). Contributed by Marcono1234.\n\nSigned-off-by: Ayush Saxena <ayushsaxena@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/9acf462d268a7dee8d031978add0034fcf77f861",
        "buggy_code": "for (int i = 0; i < Long.MAX_VALUE; i++) {",
        "fixed_code": "for (int i = 0; i < Integer.MAX_VALUE; i++) {",
        "patch": "@@ -45,7 +45,7 @@ public void testDFSStripedOutputStreamUpdatePipeline() throws Exception {\n       Path filePath = new Path(\"/test/file\");\n       FSDataOutputStream out = dfs.create(filePath);\n       try {\n-        for (int i = 0; i < Long.MAX_VALUE; i++) {\n+        for (int i = 0; i < Integer.MAX_VALUE; i++) {\n           out.write(i);\n           if (i == 1024 * 1024 * 5) {\n             cluster.stopDataNode(0);"
    },
    {
        "commit_id": "3b65b5d68f80dc15374067324d9d5413f0efd5ef",
        "commit_message": "HDFS-17020. RBF: mount table addAll should print failed records in std error (#5674)",
        "commit_url": "https://github.com/apache/hadoop/commit/3b65b5d68f80dc15374067324d9d5413f0efd5ef",
        "buggy_code": "failedRecordsKeys.add(primaryKey);",
        "fixed_code": "failedRecordsKeys.add(getOriginalPrimaryKey(primaryKey));",
        "patch": "@@ -255,7 +255,7 @@ public <T extends BaseRecord> StateStoreOperationResult putAll(\n               String recordZNode = getNodePath(znode, primaryKey);\n               byte[] data = serialize(record);\n               if (!writeNode(recordZNode, data, update, error)) {\n-                failedRecordsKeys.add(primaryKey);\n+                failedRecordsKeys.add(getOriginalPrimaryKey(primaryKey));\n                 status.set(false);\n               }\n               return null;"
    },
    {
        "commit_id": "3b65b5d68f80dc15374067324d9d5413f0efd5ef",
        "commit_message": "HDFS-17020. RBF: mount table addAll should print failed records in std error (#5674)",
        "commit_url": "https://github.com/apache/hadoop/commit/3b65b5d68f80dc15374067324d9d5413f0efd5ef",
        "buggy_code": "System.err.println(\"Cannot add some or all mount points\");",
        "fixed_code": "System.err.println(\"Cannot add mount points: \" + addResponse.getFailedRecordsKeys());",
        "patch": "@@ -515,7 +515,7 @@ private boolean addAllMount(String[] parameters, int i) throws IOException {\n         mountTable.addMountTableEntries(request);\n     boolean added = addResponse.getStatus();\n     if (!added) {\n-      System.err.println(\"Cannot add some or all mount points\");\n+      System.err.println(\"Cannot add mount points: \" + addResponse.getFailedRecordsKeys());\n     }\n     return added;\n   }"
    },
    {
        "commit_id": "9a524ede8721448b418654446a446a1bfbd62471",
        "commit_message": "HDFS-17022. Fix the exception message to print the Identifier pattern (#5678). Contributed by Nishtha Shah.\n\nSigned-off-by: Ayush Saxena <ayushsaxena@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/9a524ede8721448b418654446a446a1bfbd62471",
        "buggy_code": "MessageFormat.format(\"[{0}] = [{1}] must be '{2}'\", name, value, IDENTIFIER_PATTERN_STR));",
        "fixed_code": "MessageFormat.format(\"[{0}] = [{1}] must be \\\"{2}\\\"\", name, value, IDENTIFIER_PATTERN_STR));",
        "patch": "@@ -130,7 +130,7 @@ public static String validIdentifier(String value, int maxLen, String name) {\n     }\n     if (!IDENTIFIER_PATTERN.matcher(value).find()) {\n       throw new IllegalArgumentException(\n-        MessageFormat.format(\"[{0}] = [{1}] must be '{2}'\", name, value, IDENTIFIER_PATTERN_STR));\n+        MessageFormat.format(\"[{0}] = [{1}] must be \\\"{2}\\\"\", name, value, IDENTIFIER_PATTERN_STR));\n     }\n     return value;\n   }"
    },
    {
        "commit_id": "03163f9de260a45e4975d1718ac5592fc32764c2",
        "commit_message": "HDFS-17011. Fix the metric of  \"HttpPort\" at DataNodeInfo (#5657). Contributed by Zhaohui Wang.\n\nReviewed-by: Inigo Goiri <inigoiri@apache.org>\r\nSigned-off-by: Ayush Saxena <ayushsaxena@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/03163f9de260a45e4975d1718ac5592fc32764c2",
        "buggy_code": "return this.getConf().get(\"dfs.datanode.info.port\");",
        "fixed_code": "return String.valueOf(infoPort);",
        "patch": "@@ -3558,7 +3558,7 @@ public String getDataPort(){\n \n   @Override // DataNodeMXBean\n   public String getHttpPort(){\n-    return this.getConf().get(\"dfs.datanode.info.port\");\n+    return String.valueOf(infoPort);\n   }\n \n   @Override // DataNodeMXBean"
    },
    {
        "commit_id": "e0938b4c2aa5120280023fb5985ec7508d2a2ba3",
        "commit_message": "YARN-11495. Fix typos in hadoop-yarn-server-web-proxy. (#5652). Contributed by Shilun Fan.\n\nSigned-off-by: Ayush Saxena <ayushsaxena@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/e0938b4c2aa5120280023fb5985ec7508d2a2ba3",
        "buggy_code": "private enum HTTP { GET, POST, HEAD, PUT, DELETE };",
        "fixed_code": "private enum HTTP { GET, POST, HEAD, PUT, DELETE }",
        "patch": "@@ -103,7 +103,7 @@ public class WebAppProxyServlet extends HttpServlet {\n   /**\n    * HTTP methods.\n    */\n-  private enum HTTP { GET, POST, HEAD, PUT, DELETE };\n+  private enum HTTP { GET, POST, HEAD, PUT, DELETE }\n \n   /**\n    * Empty Hamlet class."
    },
    {
        "commit_id": "03bf8f982a7e33073f8a46e5253adccbbf73931a",
        "commit_message": "HDFS-16999. Fix wrong use of processFirstBlockReport(). (#5622). Contributed by Shuyan Zhang.\n\nSigned-off-by: Ayush Saxena <ayushsaxena@apache.org>\r\nSigned-off-by: He Xiaoqiao <hexiaoqiao@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/03bf8f982a7e33073f8a46e5253adccbbf73931a",
        "buggy_code": "if (storageInfo.getBlockReportCount() == 0) {",
        "fixed_code": "if (!storageInfo.hasReceivedBlockReport()) {",
        "patch": "@@ -2913,7 +2913,7 @@ public boolean processReport(final DatanodeID nodeID,\n         return !node.hasStaleStorages();\n       }\n \n-      if (storageInfo.getBlockReportCount() == 0) {\n+      if (!storageInfo.hasReceivedBlockReport()) {\n         // The first block report can be processed a lot more efficiently than\n         // ordinary block reports.  This shortens restart times.\n         blockLog.info(\"BLOCK* processReport 0x{} with lease ID 0x{}: Processing first \""
    },
    {
        "commit_id": "03bf8f982a7e33073f8a46e5253adccbbf73931a",
        "commit_message": "HDFS-16999. Fix wrong use of processFirstBlockReport(). (#5622). Contributed by Shuyan Zhang.\n\nSigned-off-by: Ayush Saxena <ayushsaxena@apache.org>\r\nSigned-off-by: He Xiaoqiao <hexiaoqiao@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/03bf8f982a7e33073f8a46e5253adccbbf73931a",
        "buggy_code": "File getCurrentDir() {",
        "fixed_code": "public File getCurrentDir() {",
        "patch": "@@ -350,7 +350,7 @@ boolean checkClosed() {\n   }\n \n   @VisibleForTesting\n-  File getCurrentDir() {\n+  public File getCurrentDir() {\n     return currentDir;\n   }\n "
    },
    {
        "commit_id": "70c0aa342e6a6a12b647bbfe30bb73313958e450",
        "commit_message": "YARN-11482. Fix bug of DRF comparision DominantResourceFairnessComparator2 in fair scheduler. (#5607). Contributed by Xiaoqiao He.\n\nReviewed-by: Shilun Fan <slfan1989@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/70c0aa342e6a6a12b647bbfe30bb73313958e450",
        "buggy_code": "boolean s2Needy = resourceInfo1[dominant2].getValue() <",
        "fixed_code": "boolean s2Needy = resourceInfo2[dominant2].getValue() <",
        "patch": "@@ -390,7 +390,7 @@ public int compare(Schedulable s1, Schedulable s2) {\n       // share for that resource\n       boolean s1Needy = resourceInfo1[dominant1].getValue() <\n           minShareInfo1[dominant1].getValue();\n-      boolean s2Needy = resourceInfo1[dominant2].getValue() <\n+      boolean s2Needy = resourceInfo2[dominant2].getValue() <\n           minShareInfo2[dominant2].getValue();\n \n       int res;"
    },
    {
        "commit_id": "5af0845076fc3fb7cd00aefc049b4708e4c3b0c0",
        "commit_message": "HDFS-16672. Fix lease interval comparison in BlockReportLeaseManager (#4598). Contributed by dzcxzl.\n\nReviewed-by: He Xiaoqiao <hexiaoqiao@apache.org>\r\nSigned-off-by: Ayush Saxena <ayushsaxena@apache.org",
        "commit_url": "https://github.com/apache/hadoop/commit/5af0845076fc3fb7cd00aefc049b4708e4c3b0c0",
        "buggy_code": "if (monotonicNowMs < node.leaseTimeMs + leaseExpiryMs) {",
        "fixed_code": "if (monotonicNowMs - node.leaseTimeMs < leaseExpiryMs) {",
        "patch": "@@ -267,7 +267,7 @@ public synchronized long requestLease(DatanodeDescriptor dn) {\n \n   private synchronized boolean pruneIfExpired(long monotonicNowMs,\n                                               NodeData node) {\n-    if (monotonicNowMs < node.leaseTimeMs + leaseExpiryMs) {\n+    if (monotonicNowMs - node.leaseTimeMs < leaseExpiryMs) {\n       return false;\n     }\n     LOG.info(\"Removing expired block report lease 0x{} for DN {}.\","
    },
    {
        "commit_id": "dd6d0ac5108ffa616241886d9e8d8f07dbc034cf",
        "commit_message": "YARN-11462. Fix Typo of hadoop-yarn-common. (#5539)\n\nCo-authored-by: Shilun Fan <slfan1989@apache.org>\r\nReviewed-by: He Xiaoqiao <hexiaoqiao@apache.org>\r\nSigned-off-by: Shilun Fan <slfan1989@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/dd6d0ac5108ffa616241886d9e8d8f07dbc034cf",
        "buggy_code": "LOG.info(\"WebAppp /{} exiting...\", webApp.name());",
        "fixed_code": "LOG.info(\"WebApp /{} exiting...\", webApp.name());",
        "patch": "@@ -252,7 +252,7 @@ private void prepareToExit() {\n     checkState(devMode, \"only in dev mode\");\n     new Timer(\"webapp exit\", true).schedule(new TimerTask() {\n       @Override public void run() {\n-        LOG.info(\"WebAppp /{} exiting...\", webApp.name());\n+        LOG.info(\"WebApp /{} exiting...\", webApp.name());\n         webApp.stop();\n         System.exit(0); // FINDBUG: this is intended in dev mode\n       }"
    },
    {
        "commit_id": "dd6d0ac5108ffa616241886d9e8d8f07dbc034cf",
        "commit_message": "YARN-11462. Fix Typo of hadoop-yarn-common. (#5539)\n\nCo-authored-by: Shilun Fan <slfan1989@apache.org>\r\nReviewed-by: He Xiaoqiao <hexiaoqiao@apache.org>\r\nSigned-off-by: Shilun Fan <slfan1989@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/dd6d0ac5108ffa616241886d9e8d8f07dbc034cf",
        "buggy_code": "LOG.trace(\"GOT EXCEPITION\", e);",
        "fixed_code": "LOG.trace(\"GOT EXCEPTION\", e);",
        "patch": "@@ -53,7 +53,7 @@ public class GenericExceptionHandler implements ExceptionMapper<Exception> {\n   @Override\n   public Response toResponse(Exception e) {\n     if (LOG.isTraceEnabled()) {\n-      LOG.trace(\"GOT EXCEPITION\", e);\n+      LOG.trace(\"GOT EXCEPTION\", e);\n     }\n     // Don't catch this as filter forward on 404\n     // (ServletContainer.FEATURE_FILTER_FORWARD_ON_404)"
    },
    {
        "commit_id": "dd6d0ac5108ffa616241886d9e8d8f07dbc034cf",
        "commit_message": "YARN-11462. Fix Typo of hadoop-yarn-common. (#5539)\n\nCo-authored-by: Shilun Fan <slfan1989@apache.org>\r\nReviewed-by: He Xiaoqiao <hexiaoqiao@apache.org>\r\nSigned-off-by: Shilun Fan <slfan1989@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/dd6d0ac5108ffa616241886d9e8d8f07dbc034cf",
        "buggy_code": "fail(\"deSerialze should throw YarnRuntimeException\");",
        "fixed_code": "fail(\"deSerialize should throw YarnRuntimeException\");",
        "patch": "@@ -49,7 +49,7 @@ void testDeserialize() throws Exception {\n \n     try {\n       pb.deSerialize();\n-      fail(\"deSerialze should throw YarnRuntimeException\");\n+      fail(\"deSerialize should throw YarnRuntimeException\");\n     } catch (YarnRuntimeException e) {\n       assertEquals(ClassNotFoundException.class,\n           e.getCause().getClass());"
    },
    {
        "commit_id": "dd6d0ac5108ffa616241886d9e8d8f07dbc034cf",
        "commit_message": "YARN-11462. Fix Typo of hadoop-yarn-common. (#5539)\n\nCo-authored-by: Shilun Fan <slfan1989@apache.org>\r\nReviewed-by: He Xiaoqiao <hexiaoqiao@apache.org>\r\nSigned-off-by: Shilun Fan <slfan1989@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/dd6d0ac5108ffa616241886d9e8d8f07dbc034cf",
        "buggy_code": "domain.setId(\"namesapce id\");",
        "fixed_code": "domain.setId(\"namespace id\");",
        "patch": "@@ -439,7 +439,7 @@ private static TimelineEntity generateEntity() {\n \n   public static TimelineDomain generateDomain() {\n     TimelineDomain domain = new TimelineDomain();\n-    domain.setId(\"namesapce id\");\n+    domain.setId(\"namespace id\");\n     domain.setDescription(\"domain description\");\n     domain.setOwner(\"domain owner\");\n     domain.setReaders(\"domain_reader\");"
    },
    {
        "commit_id": "dd6d0ac5108ffa616241886d9e8d8f07dbc034cf",
        "commit_message": "YARN-11462. Fix Typo of hadoop-yarn-common. (#5539)\n\nCo-authored-by: Shilun Fan <slfan1989@apache.org>\r\nReviewed-by: He Xiaoqiao <hexiaoqiao@apache.org>\r\nSigned-off-by: Shilun Fan <slfan1989@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/dd6d0ac5108ffa616241886d9e8d8f07dbc034cf",
        "buggy_code": "domain.setId(\"namesapce id\");",
        "fixed_code": "domain.setId(\"namespace id\");",
        "patch": "@@ -238,7 +238,7 @@ private static TimelineEntity generateEntity(String type) {\n \n   private static TimelineDomain generateDomain() {\n     TimelineDomain domain = new TimelineDomain();\n-    domain.setId(\"namesapce id\");\n+    domain.setId(\"namespace id\");\n     domain.setDescription(\"domain description\");\n     domain.setOwner(\"domain owner\");\n     domain.setReaders(\"domain_reader\");"
    },
    {
        "commit_id": "dd6d0ac5108ffa616241886d9e8d8f07dbc034cf",
        "commit_message": "YARN-11462. Fix Typo of hadoop-yarn-common. (#5539)\n\nCo-authored-by: Shilun Fan <slfan1989@apache.org>\r\nReviewed-by: He Xiaoqiao <hexiaoqiao@apache.org>\r\nSigned-off-by: Shilun Fan <slfan1989@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/dd6d0ac5108ffa616241886d9e8d8f07dbc034cf",
        "buggy_code": "void testFetchApplictionLogsHar() throws Exception {",
        "fixed_code": "void testFetchApplicationLogsHar() throws Exception {",
        "patch": "@@ -385,7 +385,7 @@ public boolean isRollover(final FileContext fc, final Path candidate) throws IOE\n \n   @Test\n   @Timeout(15000)\n-  void testFetchApplictionLogsHar() throws Exception {\n+  void testFetchApplicationLogsHar() throws Exception {\n     List<String> newLogTypes = new ArrayList<>();\n     newLogTypes.add(\"syslog\");\n     newLogTypes.add(\"stdout\");"
    },
    {
        "commit_id": "8f6be3678d1113e3e7f5477c357fc81f62d460b8",
        "commit_message": "MAPREDUCE-7434. Fix ShuffleHandler tests. Contributed by Tamas Domok",
        "commit_url": "https://github.com/apache/hadoop/commit/8f6be3678d1113e3e7f5477c357fc81f62d460b8",
        "buggy_code": "String dataFile = getDataFile(tempDir.toAbsolutePath().toString(), TEST_ATTEMPT_2);",
        "fixed_code": "String dataFile = getDataFile(TEST_USER, tempDir.toAbsolutePath().toString(), TEST_ATTEMPT_2);",
        "patch": "@@ -225,7 +225,7 @@ public void testInvalidMapNoDataFile() {\n     final ShuffleTest t = createShuffleTest();\n     final EmbeddedChannel shuffle = t.createShuffleHandlerChannelFileRegion();\n \n-    String dataFile = getDataFile(tempDir.toAbsolutePath().toString(), TEST_ATTEMPT_2);\n+    String dataFile = getDataFile(TEST_USER, tempDir.toAbsolutePath().toString(), TEST_ATTEMPT_2);\n     assertTrue(\"should delete\", new File(dataFile).delete());\n \n     FullHttpRequest req = t.createRequest(getUri(TEST_JOB_ID, 0,"
    },
    {
        "commit_id": "8025a60ae79382b8885aa9619fed9fa2cd1b62b8",
        "commit_message": "HDFS-16901: Minor fix for unit test.\n\nSigned-off-by: Owen O'Malley <oomalley@linkedin.com>",
        "commit_url": "https://github.com/apache/hadoop/commit/8025a60ae79382b8885aa9619fed9fa2cd1b62b8",
        "buggy_code": "GenericTestUtils.LogCapturer.captureLogs(FSNamesystem.auditLog);",
        "fixed_code": "GenericTestUtils.LogCapturer.captureLogs(FSNamesystem.AUDIT_LOG);",
        "patch": "@@ -2090,7 +2090,7 @@ public void testMkdirsWithCallerContext() throws IOException {\n   public void testRealUserPropagationInCallerContext()\n       throws IOException, InterruptedException {\n     GenericTestUtils.LogCapturer auditlog =\n-        GenericTestUtils.LogCapturer.captureLogs(FSNamesystem.auditLog);\n+        GenericTestUtils.LogCapturer.captureLogs(FSNamesystem.AUDIT_LOG);\n \n     // Current callerContext is null\n     assertNull(CallerContext.getCurrent());"
    },
    {
        "commit_id": "fe5bb49ad9b17975eaf185c4061f3e88f6f16024",
        "commit_message": "Revert \"YARN-11404. Add junit5 dependency to hadoop-mapreduce-client-app to fix few unit test failure. Contributed by Susheel Gupta\"\n\nThis reverts commit 8eda456d379bcd84fd00af0a74718ff2c668a45e.",
        "commit_url": "https://github.com/apache/hadoop/commit/fe5bb49ad9b17975eaf185c4061f3e88f6f16024",
        "buggy_code": "import org.junit.jupiter.api.Test;",
        "fixed_code": "import org.junit.Test;",
        "patch": "@@ -47,7 +47,7 @@\n import org.apache.hadoop.yarn.event.EventHandler;\n import org.apache.hadoop.yarn.factories.RecordFactory;\n import org.apache.hadoop.yarn.factory.providers.RecordFactoryProvider;\n-import org.junit.jupiter.api.Test;\n+import org.junit.Test;\n \n public class TestKillAMPreemptionPolicy {\n   private final RecordFactory recordFactory = RecordFactoryProvider"
    },
    {
        "commit_id": "fe0541b58d8a1aead015bb440528ec84e25ec1c9",
        "commit_message": "HDFS-16913. Fix flaky some unit tests since they offen timeout  (#5377)\n\nCo-authored-by: gf13871 <gf13871@ly.com>\r\nReviewed-by: Tao Li <tomscut@apache.org>\r\nReviewed-by: Shilun Fan <slfan1989@apache.org>\r\nSigned-off-by: Shilun Fan <slfan1989@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/fe0541b58d8a1aead015bb440528ec84e25ec1c9",
        "buggy_code": "@Test(timeout = 60000)",
        "fixed_code": "@Test(timeout = 120000)",
        "patch": "@@ -32,7 +32,7 @@ public class TestFileLengthOnClusterRestart {\n    * Tests the fileLength when we sync the file and restart the cluster and\n    * Datanodes not report to Namenode yet.\n    */\n-  @Test(timeout = 60000)\n+  @Test(timeout = 120000)\n   public void testFileLengthWithHSyncAndClusterRestartWithOutDNsRegister()\n       throws Exception {\n     final Configuration conf = new HdfsConfiguration();"
    },
    {
        "commit_id": "fe0541b58d8a1aead015bb440528ec84e25ec1c9",
        "commit_message": "HDFS-16913. Fix flaky some unit tests since they offen timeout  (#5377)\n\nCo-authored-by: gf13871 <gf13871@ly.com>\r\nReviewed-by: Tao Li <tomscut@apache.org>\r\nReviewed-by: Shilun Fan <slfan1989@apache.org>\r\nSigned-off-by: Shilun Fan <slfan1989@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/fe0541b58d8a1aead015bb440528ec84e25ec1c9",
        "buggy_code": "@Test(timeout = 60000)",
        "fixed_code": "@Test(timeout = 120000)",
        "patch": "@@ -168,7 +168,7 @@ public void testBalancerServiceBalanceTwice() throws Exception {\n     }\n   }\n \n-  @Test(timeout = 60000)\n+  @Test(timeout = 120000)\n   public void testBalancerServiceOnError() throws Exception {\n     Configuration conf = new HdfsConfiguration();\n     // retry for every 5 seconds"
    },
    {
        "commit_id": "fe0541b58d8a1aead015bb440528ec84e25ec1c9",
        "commit_message": "HDFS-16913. Fix flaky some unit tests since they offen timeout  (#5377)\n\nCo-authored-by: gf13871 <gf13871@ly.com>\r\nReviewed-by: Tao Li <tomscut@apache.org>\r\nReviewed-by: Shilun Fan <slfan1989@apache.org>\r\nSigned-off-by: Shilun Fan <slfan1989@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/fe0541b58d8a1aead015bb440528ec84e25ec1c9",
        "buggy_code": "@Test(timeout=30000)",
        "fixed_code": "@Test(timeout=60000)",
        "patch": "@@ -210,7 +210,7 @@ public void testWriteOverGracefulFailoverWithDnFail() throws Exception {\n     doTestWriteOverFailoverWithDnFail(TestScenario.GRACEFUL_FAILOVER);\n   }\n   \n-  @Test(timeout=30000)\n+  @Test(timeout=60000)\n   public void testWriteOverCrashFailoverWithDnFail() throws Exception {\n     doTestWriteOverFailoverWithDnFail(TestScenario.ORIGINAL_ACTIVE_CRASHED);\n   }"
    },
    {
        "commit_id": "fe0541b58d8a1aead015bb440528ec84e25ec1c9",
        "commit_message": "HDFS-16913. Fix flaky some unit tests since they offen timeout  (#5377)\n\nCo-authored-by: gf13871 <gf13871@ly.com>\r\nReviewed-by: Tao Li <tomscut@apache.org>\r\nReviewed-by: Shilun Fan <slfan1989@apache.org>\r\nSigned-off-by: Shilun Fan <slfan1989@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/fe0541b58d8a1aead015bb440528ec84e25ec1c9",
        "buggy_code": "@Test(timeout = 60000)",
        "fixed_code": "@Test(timeout = 120000)",
        "patch": "@@ -548,7 +548,7 @@ public void testSnapshotOpsOnReservedPath() throws Exception {\n    * paths work and the NN can load the resulting edits. This test if for\n    * snapshots at the root level.\n    */\n-  @Test(timeout = 60000)\n+  @Test(timeout = 120000)\n   public void testSnapshotOpsOnRootReservedPath() throws Exception {\n     Path dir = new Path(\"/\");\n     Path sub = new Path(dir, \"sub\");"
    },
    {
        "commit_id": "26fba8701c97928bb2ed2e6b456ab5ba9513e0fe",
        "commit_message": "HDFS-18324. Fix race condition in closing IPC connections. (#5371)",
        "commit_url": "https://github.com/apache/hadoop/commit/26fba8701c97928bb2ed2e6b456ab5ba9513e0fe",
        "buggy_code": "@Test(timeout=60000)",
        "fixed_code": "@Test(timeout=100000)",
        "patch": "@@ -1336,7 +1336,7 @@ interface DummyProtocol {\n   /**\n    * Test the retry count while used in a retry proxy.\n    */\n-  @Test(timeout=60000)\n+  @Test(timeout=100000)\n   public void testRetryProxy() throws IOException {\n     final Client client = new Client(LongWritable.class, conf);\n     "
    },
    {
        "commit_id": "113a9e40cbf2b8303910547d8d4476af60846996",
        "commit_message": "HADOOP-18625. Fix method name of RPC.Builder#setnumReaders (#5301)\n\n\r\nChanges method name of RPC.Builder#setnumReaders to setNumReaders()\r\n\r\nThe original method is still there, just marked deprecated.\r\nIt is the one which should be used when working with older branches.\r\n\r\nContributed by Haiyang Hu",
        "commit_url": "https://github.com/apache/hadoop/commit/113a9e40cbf2b8303910547d8d4476af60846996",
        "buggy_code": ".setNumHandlers(1).setnumReaders(3).setQueueSizePerHandler(200)",
        "fixed_code": ".setNumHandlers(1).setNumReaders(3).setQueueSizePerHandler(200)",
        "patch": "@@ -378,7 +378,7 @@ public void testConfRpc() throws IOException {\n     assertEquals(confReaders, server.getNumReaders());\n \n     server = newServerBuilder(conf)\n-        .setNumHandlers(1).setnumReaders(3).setQueueSizePerHandler(200)\n+        .setNumHandlers(1).setNumReaders(3).setQueueSizePerHandler(200)\n         .setVerbose(false).build();\n \n     assertEquals(3, server.getNumReaders());"
    },
    {
        "commit_id": "113a9e40cbf2b8303910547d8d4476af60846996",
        "commit_message": "HADOOP-18625. Fix method name of RPC.Builder#setnumReaders (#5301)\n\n\r\nChanges method name of RPC.Builder#setnumReaders to setNumReaders()\r\n\r\nThe original method is still there, just marked deprecated.\r\nIt is the one which should be used when working with older branches.\r\n\r\nContributed by Haiyang Hu",
        "commit_url": "https://github.com/apache/hadoop/commit/113a9e40cbf2b8303910547d8d4476af60846996",
        "buggy_code": ".setnumReaders(readerCount)",
        "fixed_code": ".setNumReaders(readerCount)",
        "patch": "@@ -333,7 +333,7 @@ public RouterRpcServer(Configuration conf, Router router,\n         .setBindAddress(confRpcAddress.getHostName())\n         .setPort(confRpcAddress.getPort())\n         .setNumHandlers(handlerCount)\n-        .setnumReaders(readerCount)\n+        .setNumReaders(readerCount)\n         .setQueueSizePerHandler(handlerQueueSize)\n         .setVerbose(false)\n         .setAlignmentContext(routerStateIdContext)"
    },
    {
        "commit_id": "8eda456d379bcd84fd00af0a74718ff2c668a45e",
        "commit_message": "YARN-11404. Add junit5 dependency to hadoop-mapreduce-client-app to fix few unit test failure. Contributed by Susheel Gupta",
        "commit_url": "https://github.com/apache/hadoop/commit/8eda456d379bcd84fd00af0a74718ff2c668a45e",
        "buggy_code": "import org.junit.Test;",
        "fixed_code": "import org.junit.jupiter.api.Test;",
        "patch": "@@ -47,7 +47,7 @@\n import org.apache.hadoop.yarn.event.EventHandler;\n import org.apache.hadoop.yarn.factories.RecordFactory;\n import org.apache.hadoop.yarn.factory.providers.RecordFactoryProvider;\n-import org.junit.Test;\n+import org.junit.jupiter.api.Test;\n \n public class TestKillAMPreemptionPolicy {\n   private final RecordFactory recordFactory = RecordFactoryProvider"
    },
    {
        "commit_id": "168fa078013debd88a7d97d59ab107c3a65be247",
        "commit_message": "YARN-11409. Fix Typo of ResourceManager#webapp moudle. (#5285)",
        "commit_url": "https://github.com/apache/hadoop/commit/168fa078013debd88a7d97d59ab107c3a65be247",
        "buggy_code": "};",
        "fixed_code": "}",
        "patch": "@@ -61,7 +61,7 @@ public class CapacitySchedulerLeafQueueInfo extends CapacitySchedulerQueueInfo {\n   protected String orderingPolicyDisplayName;\n \n   CapacitySchedulerLeafQueueInfo() {\n-  };\n+  }\n \n   CapacitySchedulerLeafQueueInfo(CapacityScheduler cs, AbstractLeafQueue q) {\n     super(cs, q);"
    },
    {
        "commit_id": "168fa078013debd88a7d97d59ab107c3a65be247",
        "commit_message": "YARN-11409. Fix Typo of ResourceManager#webapp moudle. (#5285)",
        "commit_url": "https://github.com/apache/hadoop/commit/168fa078013debd88a7d97d59ab107c3a65be247",
        "buggy_code": "};",
        "fixed_code": "}",
        "patch": "@@ -96,7 +96,7 @@ public class CapacitySchedulerQueueInfo {\n       new AutoQueueTemplatePropertiesInfo();\n \n   CapacitySchedulerQueueInfo() {\n-  };\n+  }\n \n   CapacitySchedulerQueueInfo(CapacityScheduler cs, CSQueue q) {\n "
    },
    {
        "commit_id": "3d21cff263c68a45b45b927e3ddb411e925d8eb6",
        "commit_message": "YARN-11413. Fix Junit Test ERROR Introduced By YARN-6412. (#5289)\n\n* YARN-11413. Fix Junit Test ERROR Introduced By YARN-6412.\r\n\r\n* YARN-11413. Fix CheckStyle.\r\n\r\n* YARN-11413. Fix CheckStyle.\r\n\r\nCo-authored-by: slfan1989 <louj1988@@>",
        "commit_url": "https://github.com/apache/hadoop/commit/3d21cff263c68a45b45b927e3ddb411e925d8eb6",
        "buggy_code": "String propRegex = \"^[A-Za-z][A-Za-z0-9_-]+(\\\\.[A-Za-z0-9_-]+)+$\";",
        "fixed_code": "String propRegex = \"^[A-Za-z][A-Za-z0-9_-]+(\\\\.[A-Za-z%s0-9_-]+)+$\";",
        "patch": "@@ -194,7 +194,7 @@ public abstract class TestConfigurationFieldsBase {\n     HashMap<String,String> retVal = new HashMap<>();\n \n     // Setup regexp for valid properties\n-    String propRegex = \"^[A-Za-z][A-Za-z0-9_-]+(\\\\.[A-Za-z0-9_-]+)+$\";\n+    String propRegex = \"^[A-Za-z][A-Za-z0-9_-]+(\\\\.[A-Za-z%s0-9_-]+)+$\";\n     Pattern p = Pattern.compile(propRegex);\n \n     // Iterate through class member variables"
    },
    {
        "commit_id": "c67c2b756907af2d7167a32bdb2756ca18fd3960",
        "commit_message": "HADOOP-18546. ABFS. disable purging list of in progress reads in abfs stream close() (#5176)\n\n\r\nThis addresses HADOOP-18521, \"ABFS ReadBufferManager buffer sharing\r\nacross concurrent HTTP requests\" by not trying to cancel\r\nin progress reads.\r\n\r\nIt supercedes HADOOP-18528, which disables the prefetching.\r\nIf that patch is applied *after* this one, prefetching\r\nwill be disabled.\r\n\r\nAs well as changing the default value in the code,\r\ncore-default.xml is updated to set\r\nfs.azure.enable.readahead = true\r\n\r\nAs a result, if Configuration.get(\"fs.azure.enable.readahead\")\r\nreturns a non-null value, then it can be inferred that\r\nit was set in or core-default.xml (the fix is present)\r\nor in core-site.xml (someone asked for it).\r\n\r\nContributed by Pranav Saxena.",
        "commit_url": "https://github.com/apache/hadoop/commit/c67c2b756907af2d7167a32bdb2756ca18fd3960",
        "buggy_code": "public static final boolean DEFAULT_ENABLE_READAHEAD = false;",
        "fixed_code": "public static final boolean DEFAULT_ENABLE_READAHEAD = true;",
        "patch": "@@ -109,7 +109,7 @@ public final class FileSystemConfigurations {\n   public static final boolean DEFAULT_ABFS_LATENCY_TRACK = false;\n   public static final long DEFAULT_SAS_TOKEN_RENEW_PERIOD_FOR_STREAMS_IN_SECONDS = 120;\n \n-  public static final boolean DEFAULT_ENABLE_READAHEAD = false;\n+  public static final boolean DEFAULT_ENABLE_READAHEAD = true;\n   public static final String DEFAULT_FS_AZURE_USER_AGENT_PREFIX = EMPTY_STRING;\n   public static final String DEFAULT_VALUE_UNKNOWN = \"UNKNOWN\";\n "
    },
    {
        "commit_id": "c67c2b756907af2d7167a32bdb2756ca18fd3960",
        "commit_message": "HADOOP-18546. ABFS. disable purging list of in progress reads in abfs stream close() (#5176)\n\n\r\nThis addresses HADOOP-18521, \"ABFS ReadBufferManager buffer sharing\r\nacross concurrent HTTP requests\" by not trying to cancel\r\nin progress reads.\r\n\r\nIt supercedes HADOOP-18528, which disables the prefetching.\r\nIf that patch is applied *after* this one, prefetching\r\nwill be disabled.\r\n\r\nAs well as changing the default value in the code,\r\ncore-default.xml is updated to set\r\nfs.azure.enable.readahead = true\r\n\r\nAs a result, if Configuration.get(\"fs.azure.enable.readahead\")\r\nreturns a non-null value, then it can be inferred that\r\nit was set in or core-default.xml (the fix is present)\r\nor in core-site.xml (someone asked for it).\r\n\r\nContributed by Pranav Saxena.",
        "commit_url": "https://github.com/apache/hadoop/commit/c67c2b756907af2d7167a32bdb2756ca18fd3960",
        "buggy_code": "private boolean isReadAheadEnabled = false;",
        "fixed_code": "private boolean isReadAheadEnabled = true;",
        "patch": "@@ -35,7 +35,7 @@ public class AbfsInputStreamContext extends AbfsStreamContext {\n \n   private boolean tolerateOobAppends;\n \n-  private boolean isReadAheadEnabled = false;\n+  private boolean isReadAheadEnabled = true;\n \n   private boolean alwaysReadBufferSize;\n "
    },
    {
        "commit_id": "86ac1ad9e573f8f42737a9260f4bc9878e20448d",
        "commit_message": "YARN-10978. Fix ApplicationClassLoader to Correctly Expand Glob for Windows Path (#3558)",
        "commit_url": "https://github.com/apache/hadoop/commit/86ac1ad9e573f8f42737a9260f4bc9878e20448d",
        "buggy_code": "if (element.endsWith(\"/*\")) {",
        "fixed_code": "if (element.endsWith(File.separator + \"*\")) {",
        "patch": "@@ -108,7 +108,7 @@ static URL[] constructUrlsFromClasspath(String classpath)\n       throws MalformedURLException {\n     List<URL> urls = new ArrayList<URL>();\n     for (String element : classpath.split(File.pathSeparator)) {\n-      if (element.endsWith(\"/*\")) {\n+      if (element.endsWith(File.separator + \"*\")) {\n         List<Path> jars = FileUtil.getJarsInDirectory(element);\n         if (!jars.isEmpty()) {\n           for (Path jar: jars) {"
    },
    {
        "commit_id": "7d39abd799a5f801a9fd07868a193205ab500bfa",
        "commit_message": "HADOOP-18429. fix infinite loop in MutableGaugeFloat#incr(float) (#4823)",
        "commit_url": "https://github.com/apache/hadoop/commit/7d39abd799a5f801a9fd07868a193205ab500bfa",
        "buggy_code": "float current = value.get();",
        "fixed_code": "float current = Float.intBitsToFloat(value.get());",
        "patch": "@@ -69,7 +69,7 @@ private final boolean compareAndSet(float expect, float update) {\n \n   private void incr(float delta) {\n     while (true) {\n-      float current = value.get();\n+      float current = Float.intBitsToFloat(value.get());\n       float next = current + delta;\n       if (compareAndSet(current, next)) {\n         setChanged();"
    },
    {
        "commit_id": "69e50c7b4499bffc1eb372799ccba3f26c5fe54e",
        "commit_message": "HADOOP-18528. Disable abfs prefetching by default (#5134)\n\n\r\nDisables block prefetching on ABFS InputStreams, by setting\r\nfs.azure.enable.readahead to false in core-default.xml and\r\nthe matching java constant.\r\n\r\nThis prevents\r\nHADOOP-18521. ABFS ReadBufferManager buffer sharing across concurrent HTTP requests.\r\n\r\nOnce a fix for that is committed, this change can be reverted.\r\n\r\nContributed by Mehakmeet Singh.",
        "commit_url": "https://github.com/apache/hadoop/commit/69e50c7b4499bffc1eb372799ccba3f26c5fe54e",
        "buggy_code": "public static final boolean DEFAULT_ENABLE_READAHEAD = true;",
        "fixed_code": "public static final boolean DEFAULT_ENABLE_READAHEAD = false;",
        "patch": "@@ -106,7 +106,7 @@ public final class FileSystemConfigurations {\n   public static final boolean DEFAULT_ABFS_LATENCY_TRACK = false;\n   public static final long DEFAULT_SAS_TOKEN_RENEW_PERIOD_FOR_STREAMS_IN_SECONDS = 120;\n \n-  public static final boolean DEFAULT_ENABLE_READAHEAD = true;\n+  public static final boolean DEFAULT_ENABLE_READAHEAD = false;\n   public static final String DEFAULT_FS_AZURE_USER_AGENT_PREFIX = EMPTY_STRING;\n   public static final String DEFAULT_VALUE_UNKNOWN = \"UNKNOWN\";\n "
    },
    {
        "commit_id": "69e50c7b4499bffc1eb372799ccba3f26c5fe54e",
        "commit_message": "HADOOP-18528. Disable abfs prefetching by default (#5134)\n\n\r\nDisables block prefetching on ABFS InputStreams, by setting\r\nfs.azure.enable.readahead to false in core-default.xml and\r\nthe matching java constant.\r\n\r\nThis prevents\r\nHADOOP-18521. ABFS ReadBufferManager buffer sharing across concurrent HTTP requests.\r\n\r\nOnce a fix for that is committed, this change can be reverted.\r\n\r\nContributed by Mehakmeet Singh.",
        "commit_url": "https://github.com/apache/hadoop/commit/69e50c7b4499bffc1eb372799ccba3f26c5fe54e",
        "buggy_code": "private boolean isReadAheadEnabled = true;",
        "fixed_code": "private boolean isReadAheadEnabled = false;",
        "patch": "@@ -35,7 +35,7 @@ public class AbfsInputStreamContext extends AbfsStreamContext {\n \n   private boolean tolerateOobAppends;\n \n-  private boolean isReadAheadEnabled = true;\n+  private boolean isReadAheadEnabled = false;\n \n   private boolean alwaysReadBufferSize;\n "
    },
    {
        "commit_id": "4891bf50491373306b89cb5cc310b9d5ebf35156",
        "commit_message": "HDFS-13369. Fix for FSCK Report broken with RequestHedgingProxyProvider (#4917)\n\nContributed-by: navinko <nakumr@cloudera.com>",
        "commit_url": "https://github.com/apache/hadoop/commit/4891bf50491373306b89cb5cc310b9d5ebf35156",
        "buggy_code": "HATestUtil.setFailoverConfigurations(cluster, conf, logicalName, 0);",
        "fixed_code": "HATestUtil.setFailoverConfigurations(cluster, conf, logicalName, null, 0);",
        "patch": "@@ -95,7 +95,7 @@ public void setupCluster() throws Exception {\n     cluster.waitActive();\n     \n     String logicalName = HATestUtil.getLogicalHostname(cluster);\n-    HATestUtil.setFailoverConfigurations(cluster, conf, logicalName, 0);\n+    HATestUtil.setFailoverConfigurations(cluster, conf, logicalName, null, 0);\n \n     nn0 = cluster.getNameNode(0);\n     nn1 = cluster.getNameNode(1);"
    },
    {
        "commit_id": "ce54b7e55d912b2f8c34801209c86518cd4642e5",
        "commit_message": "HADOOP-18118. Fix KMS Accept Queue Size default value to 500 (#3972)",
        "commit_url": "https://github.com/apache/hadoop/commit/ce54b7e55d912b2f8c34801209c86518cd4642e5",
        "buggy_code": "public static final int HTTP_SOCKET_BACKLOG_SIZE_DEFAULT = 128;",
        "fixed_code": "public static final int HTTP_SOCKET_BACKLOG_SIZE_DEFAULT = 500;",
        "patch": "@@ -144,7 +144,7 @@ public final class HttpServer2 implements FilterContainer {\n \n   public static final String HTTP_SOCKET_BACKLOG_SIZE_KEY =\n       \"hadoop.http.socket.backlog.size\";\n-  public static final int HTTP_SOCKET_BACKLOG_SIZE_DEFAULT = 128;\n+  public static final int HTTP_SOCKET_BACKLOG_SIZE_DEFAULT = 500;\n   public static final String HTTP_MAX_THREADS_KEY = \"hadoop.http.max.threads\";\n   public static final String HTTP_ACCEPTOR_COUNT_KEY =\n       \"hadoop.http.acceptor.count\";"
    },
    {
        "commit_id": "2dd8b1342ee13cba41fae3bef67d10a04c335889",
        "commit_message": "HDFS-16755. TestQJMWithFaults.testUnresolvableHostName() can fail due to unexpected host resolution (#4833)\n\n\r\nUse \".invalid\" domain from IETF RFC 2606 to ensure that the host doesn't resolve.\r\n\r\nContributed by Steve Vaughan Jr",
        "commit_url": "https://github.com/apache/hadoop/commit/2dd8b1342ee13cba41fae3bef67d10a04c335889",
        "buggy_code": "new URI(\"qjournal://\" + \"bogus:12345\" + \"/\" + JID), FAKE_NSINFO);",
        "fixed_code": "new URI(\"qjournal://\" + \"bogus.invalid:12345\" + \"/\" + JID), FAKE_NSINFO);",
        "patch": "@@ -198,7 +198,7 @@ public void testRecoverAfterDoubleFailures() throws Exception {\n   public void testUnresolvableHostName() throws Exception {\n     expectedException.expect(UnknownHostException.class);\n     new QuorumJournalManager(conf,\n-        new URI(\"qjournal://\" + \"bogus:12345\" + \"/\" + JID), FAKE_NSINFO);\n+        new URI(\"qjournal://\" + \"bogus.invalid:12345\" + \"/\" + JID), FAKE_NSINFO);\n   }\n \n   /**"
    },
    {
        "commit_id": "7fb9c306e22eee334667d4b761e3fd330f45cb01",
        "commit_message": "HADOOP-18382. AWS SDK v2 upgrade prerequisites (#4698)\n\n\r\nThis patch prepares the hadoop-aws module for a future\r\nmigration to using the v2 AWS SDK (HADOOP-18073)\r\n\r\nThat upgrade will be incompatible; this patch prepares\r\nfor it:\r\n-marks some credential providers and other \r\n classes and methods as @deprecated.\r\n-updates site documentation\r\n-reduces the visibility of the s3 client;\r\n other than for testing, it is kept private to\r\n the S3AFileSystem class.\r\n-logs some warnings when deprecated APIs are used.\r\n\r\nThe warning messages are printed only once\r\nper JVM's life. To disable them, set the\r\nlog level of org.apache.hadoop.fs.s3a.SDKV2Upgrade\r\nto ERROR\r\n \r\nContributed by Ahmar Suhail",
        "commit_url": "https://github.com/apache/hadoop/commit/7fb9c306e22eee334667d4b761e3fd330f45cb01",
        "buggy_code": "public class AWSCredentialProviderList implements AWSCredentialsProvider,",
        "fixed_code": "public final class AWSCredentialProviderList implements AWSCredentialsProvider,",
        "patch": "@@ -61,7 +61,7 @@\n  */\n @InterfaceAudience.Private\n @InterfaceStability.Evolving\n-public class AWSCredentialProviderList implements AWSCredentialsProvider,\n+public final class AWSCredentialProviderList implements AWSCredentialsProvider,\n     AutoCloseable {\n \n   private static final Logger LOG = LoggerFactory.getLogger("
    },
    {
        "commit_id": "75aff247ae47002e27b8d6014e912591e6dd2161",
        "commit_message": "YARN-11240. Fix incorrect placeholder in yarn-module. (#4678). Contributed by fanshilun\n\nSigned-off-by: Ayush Saxena <ayushsaxena@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/75aff247ae47002e27b8d6014e912591e6dd2161",
        "buggy_code": "LOG.error(\"Fail to sync service spec: {}\", e);",
        "fixed_code": "LOG.error(\"Fail to sync service spec.\", e);",
        "patch": "@@ -1139,7 +1139,7 @@ public void syncSysFs(Service yarnApp) {\n         LOG.info(\"YARN sysfs synchronized.\");\n       }\n     } catch (IOException | URISyntaxException | InterruptedException e) {\n-      LOG.error(\"Fail to sync service spec: {}\", e);\n+      LOG.error(\"Fail to sync service spec.\", e);\n     }\n   }\n }"
    },
    {
        "commit_id": "75aff247ae47002e27b8d6014e912591e6dd2161",
        "commit_message": "YARN-11240. Fix incorrect placeholder in yarn-module. (#4678). Contributed by fanshilun\n\nSigned-off-by: Ayush Saxena <ayushsaxena@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/75aff247ae47002e27b8d6014e912591e6dd2161",
        "buggy_code": "LOG.error(\"Fail to sync sysfs: {}\", e);",
        "fixed_code": "LOG.error(\"Fail to sync sysfs.\", e);",
        "patch": "@@ -553,7 +553,7 @@ public void testSyncSysFS() {\n       am.stop();\n       am.close();\n     } catch (Exception e) {\n-      LOG.error(\"Fail to sync sysfs: {}\", e);\n+      LOG.error(\"Fail to sync sysfs.\", e);\n       Assert.fail(\"Fail to sync sysfs.\");\n     }\n   }"
    },
    {
        "commit_id": "b7d4dc61bf43338767d6913b9e5882e044b35b9d",
        "commit_message": "HADOOP-18365. Update the remote address when a change is detected (#4692)\n\nAvoid reconnecting to the old address after detecting that the address has been updated.\r\n\r\n* Fix Checkstyle line length violation\r\n* Keep ConnectionId as Immutable for map key\r\n\r\nThe ConnectionId is used as a key in the connections map, and updating the remoteId caused problems with the cleanup of connections when the removeMethod was used.\r\n\r\nInstead of updating the address within the remoteId, use the removeMethod to cleanup references to the current identifier and then replace it with a new identifier using the updated address.\r\n\r\n* Use final to protect immutable ConnectionId\r\n\r\nMark non-test fields as private and final, and add a missing accessor.\r\n\r\n* Use a stable hashCode to allow safe IP addr changes\r\n* Add test that updated address is used\r\n\r\nOnce the address has been updated, it should be used in future calls.  Check to ensure that a second request succeeds and that it uses the existing updated address instead of having to re-resolve.\r\n\r\nSigned-off-by: Nick Dimiduk <ndimiduk@apache.org>\r\nSigned-off-by: sokui\r\nSigned-off-by: XanderZu\r\nSigned-off-by: stack <stack@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/b7d4dc61bf43338767d6913b9e5882e044b35b9d",
        "buggy_code": "connId.ticket, conf, factory, connId.getRpcTimeout(),",
        "fixed_code": "connId.getTicket(), conf, factory, connId.getRpcTimeout(),",
        "patch": "@@ -323,7 +323,7 @@ public <T> ProtocolProxy<T> getProxy(Class<T> protocol, long clientVersion,\n       Client.ConnectionId connId, Configuration conf, SocketFactory factory)\n       throws IOException {\n     return getProxy(protocol, clientVersion, connId.getAddress(),\n-        connId.ticket, conf, factory, connId.getRpcTimeout(),\n+        connId.getTicket(), conf, factory, connId.getRpcTimeout(),\n         connId.getRetryPolicy(), null, null);\n   }\n "
    },
    {
        "commit_id": "d0fdb1d6e013748b2dd01ee81a4906ae7c0a0767",
        "commit_message": "HADOOP-18404. Fix broken link to wiki help page in org.apache.hadoop.util.Shell (#4718). Contributed by Paul King.\n\nSigned-off-by: Ayush Saxena <ayushsaxena@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/d0fdb1d6e013748b2dd01ee81a4906ae7c0a0767",
        "buggy_code": "\"https://wiki.apache.org/hadoop/WindowsProblems\";",
        "fixed_code": "\"https://cwiki.apache.org/confluence/display/HADOOP2/WindowsProblems\";",
        "patch": "@@ -60,7 +60,7 @@ public abstract class Shell {\n    * {@value}\n    */\n   private static final String WINDOWS_PROBLEMS =\n-      \"https://wiki.apache.org/hadoop/WindowsProblems\";\n+      \"https://cwiki.apache.org/confluence/display/HADOOP2/WindowsProblems\";\n \n   /**\n    * Name of the windows utils binary: {@value}."
    },
    {
        "commit_id": "d8d3325d2f6b337c6befc8d6711886d52e756ece",
        "commit_message": "HADOOP-18387. Fix incorrect placeholder in hadoop-common (#4679). Contributed by fanshilun.",
        "commit_url": "https://github.com/apache/hadoop/commit/d8d3325d2f6b337c6befc8d6711886d52e756ece",
        "buggy_code": "LOG.error(\"Error initialize YARN Service Client: {}\", e);",
        "fixed_code": "LOG.error(\"Error initialize YARN Service Client.\", e);",
        "patch": "@@ -61,7 +61,7 @@ public YarnServiceClient() {\n     try {\n       asc = new ApiServiceClient(conf);\n     } catch (Exception e) {\n-      LOG.error(\"Error initialize YARN Service Client: {}\", e);\n+      LOG.error(\"Error initialize YARN Service Client.\", e);\n     }\n   }\n "
    },
    {
        "commit_id": "0f36539d600aaff67ec244d042bcb66125c0347b",
        "commit_message": "HDFS-16712. Fix incorrect placeholder in DataNode.java (#4672). Contributed by ZanderXu.\n\nSigned-off-by: Ayush Saxena <ayushsaxena@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/0f36539d600aaff67ec244d042bcb66125c0347b",
        "buggy_code": "LOG.debug(\"Reading diskbalancer Status failed. ex:{}\", ex);",
        "fixed_code": "LOG.debug(\"Reading diskbalancer Status failed.\", ex);",
        "patch": "@@ -3642,7 +3642,7 @@ public String getDiskBalancerStatus() {\n     try {\n       return getDiskBalancer().queryWorkStatus().toJsonString();\n     } catch (IOException ex) {\n-      LOG.debug(\"Reading diskbalancer Status failed. ex:{}\", ex);\n+      LOG.debug(\"Reading diskbalancer Status failed.\", ex);\n       return \"\";\n     }\n   }"
    },
    {
        "commit_id": "213ea037589d8a69c34e1f98413ddbc06dfd42bf",
        "commit_message": "YARN-11210. Fix YARN RMAdminCLI retry logic for non-retryable kerbero\u2026 (#4563)\n\nCo-authored-by: Kevin Wikant <wikak@amazon.com>",
        "commit_url": "https://github.com/apache/hadoop/commit/213ea037589d8a69c34e1f98413ddbc06dfd42bf",
        "buggy_code": "retryOtherThanRemoteException(TRY_ONCE_THEN_FAIL,",
        "fixed_code": "retryOtherThanRemoteAndSaslException(TRY_ONCE_THEN_FAIL,",
        "patch": "@@ -291,7 +291,7 @@ public void testRetryOtherThanRemoteException() throws Throwable {\n \n     UnreliableInterface unreliable = (UnreliableInterface)\n         RetryProxy.create(UnreliableInterface.class, unreliableImpl,\n-            retryOtherThanRemoteException(TRY_ONCE_THEN_FAIL,\n+            retryOtherThanRemoteAndSaslException(TRY_ONCE_THEN_FAIL,\n                 exceptionToPolicyMap));\n     // should retry with local IOException.\n     unreliable.failsOnceWithIOException();"
    },
    {
        "commit_id": "213ea037589d8a69c34e1f98413ddbc06dfd42bf",
        "commit_message": "YARN-11210. Fix YARN RMAdminCLI retry logic for non-retryable kerbero\u2026 (#4563)\n\nCo-authored-by: Kevin Wikant <wikak@amazon.com>",
        "commit_url": "https://github.com/apache/hadoop/commit/213ea037589d8a69c34e1f98413ddbc06dfd42bf",
        "buggy_code": "return RetryPolicies.retryOtherThanRemoteException(",
        "fixed_code": "return RetryPolicies.retryOtherThanRemoteAndSaslException(",
        "patch": "@@ -300,7 +300,7 @@ protected static RetryPolicy createRetryPolicy(Configuration conf,\n     // YARN-4288: local IOException is also possible.\n     exceptionToPolicyMap.put(IOException.class, retryPolicy);\n     // Not retry on remote IO exception.\n-    return RetryPolicies.retryOtherThanRemoteException(\n+    return RetryPolicies.retryOtherThanRemoteAndSaslException(\n         RetryPolicies.TRY_ONCE_THEN_FAIL, exceptionToPolicyMap);\n   }\n }"
    },
    {
        "commit_id": "823f5ee0d4cc508a709baf836a31b1400dd1f20c",
        "commit_message": "HADOOP-18242. ABFS Rename Failure when tracking metadata is in an incomplete state (#4331)\n\n\r\nABFS rename fails intermittently when the Storage-blob tracking\r\nmetadata is in an incomplete state. This surfaces as the error code\r\n404 and an error message of \"RenameDestinationParentPathNotFound\"\r\n\r\nTo mitigate this issue, when a request fails with this response.\r\nthe ABFS client issues a HEAD call on the source file\r\nand then retries the rename operation again\r\n\r\nABFS filesystem statistics track when this occurs with new counters\r\n  rename_recovery\r\n  metadata_incomplete_rename_failures\r\n  rename_path_attempts\r\n\r\nThis is very rare occurrence and appears to be triggered under certain\r\nheavy load conditions, just as with HADOOP-18163.\r\n\r\nContributed by Mehakmeet Singh.",
        "commit_url": "https://github.com/apache/hadoop/commit/823f5ee0d4cc508a709baf836a31b1400dd1f20c",
        "buggy_code": "AzureBlobFileSystemStore getAbfsStore() {",
        "fixed_code": "public AzureBlobFileSystemStore getAbfsStore() {",
        "patch": "@@ -1576,7 +1576,7 @@ public boolean failed() {\n   }\n \n   @VisibleForTesting\n-  AzureBlobFileSystemStore getAbfsStore() {\n+  public AzureBlobFileSystemStore getAbfsStore() {\n     return abfsStore;\n   }\n "
    },
    {
        "commit_id": "170668b994d4b9bc74b814b7b7f8096daabba12a",
        "commit_message": "HDFS-16609. Fix Flakes Junit Tests that often report timeouts. (#4382). Contributed by fanshilun.",
        "commit_url": "https://github.com/apache/hadoop/commit/170668b994d4b9bc74b814b7b7f8096daabba12a",
        "buggy_code": "@Test (timeout = 30000)",
        "fixed_code": "@Test (timeout = 60000)",
        "patch": "@@ -1410,7 +1410,7 @@ private void confirmOwner(String owner, String group,\n     }\n   }\n \n-  @Test (timeout = 30000)\n+  @Test (timeout = 60000)\n   public void testFilePermissions() throws IOException {\n     Configuration conf = new HdfsConfiguration();\n "
    },
    {
        "commit_id": "170668b994d4b9bc74b814b7b7f8096daabba12a",
        "commit_message": "HDFS-16609. Fix Flakes Junit Tests that often report timeouts. (#4382). Contributed by fanshilun.",
        "commit_url": "https://github.com/apache/hadoop/commit/170668b994d4b9bc74b814b7b7f8096daabba12a",
        "buggy_code": "@Test(timeout = 300000)",
        "fixed_code": "@Test(timeout = 600000)",
        "patch": "@@ -1351,7 +1351,7 @@ public void testSPSWhenFileHasLowRedundancyBlocks() throws Exception {\n    * 4. Set policy and call satisfyStoragePolicy for file.\n    * 5. Block should be moved successfully.\n    */\n-  @Test(timeout = 300000)\n+  @Test(timeout = 600000)\n   public void testSPSWhenFileHasExcessRedundancyBlocks() throws Exception {\n     try {\n       config.set(DFSConfigKeys"
    },
    {
        "commit_id": "f469b0e143c352b0d56fdeff002c2258dd486812",
        "commit_message": "HADOOP-18249. Fix getUri() in HttpRequest has been deprecated. (#4335)\n\n* HADOOP-18249. Fix getUri() in HttpRequest has been deprecated.\r\nWebHdfsHandler.java req.getUri() replace uri(), req.getMethod() replace method()\r\nHostRestrictingAuthorizationFilterHandler.java req.getUri() replace uri()\r\nTestHostRestrictingAuthorizationFilterHandler.java remove throws Exception, channelResponse.getStatus() replace status().\r\n\r\n* HADOOP-18249. Fix getUri() in HttpRequest has been deprecated.\r\n\r\n* HADOOP-18249. Fix Some CheckStyle.\r\n\r\nCo-authored-by: slfan1989 <louj1988@@>",
        "commit_url": "https://github.com/apache/hadoop/commit/f469b0e143c352b0d56fdeff002c2258dd486812",
        "buggy_code": "String uri = req.getUri();",
        "fixed_code": "String uri = req.uri();",
        "patch": "@@ -43,7 +43,7 @@ class URLDispatcher extends SimpleChannelInboundHandler<HttpRequest> {\n   @Override\n   protected void channelRead0(ChannelHandlerContext ctx, HttpRequest req)\n       throws Exception {\n-    String uri = req.getUri();\n+    String uri = req.uri();\n     ChannelPipeline p = ctx.pipeline();\n     if (uri.startsWith(WEBHDFS_PREFIX)) {\n       WebHdfsHandler h = new WebHdfsHandler(conf, confForCreate);"
    },
    {
        "commit_id": "e0cd0a82e032b926774dcea69edd4fa20aae2603",
        "commit_message": "HADOOP-16202. Enhanced openFile(): hadoop-aws changes. (#2584/3)\n\nS3A input stream support for the few fs.option.openfile settings.\nAs well as supporting the read policy option and values,\nif the file length is declared in fs.option.openfile.length\nthen no HEAD request will be issued when opening a file.\nThis can cut a few tens of milliseconds off the operation.\n\nThe patch adds a new openfile parameter/FS configuration option\nfs.s3a.input.async.drain.threshold (default: 16000).\nIt declares the number of bytes remaining in the http input stream\nabove which any operation to read and discard the rest of the stream,\n\"draining\", is executed asynchronously.\nThis asynchronous draining offers some performance benefit on seek-heavy\nfile IO.\n\nContributed by Steve Loughran.\n\nChange-Id: I9b0626bbe635e9fd97ac0f463f5e7167e0111e39",
        "commit_url": "https://github.com/apache/hadoop/commit/e0cd0a82e032b926774dcea69edd4fa20aae2603",
        "buggy_code": "import static org.apache.hadoop.fs.impl.FutureIOSupport.raiseInnerCause;",
        "fixed_code": "import static org.apache.hadoop.util.functional.FutureIO.raiseInnerCause;",
        "patch": "@@ -35,7 +35,7 @@\n import org.apache.hadoop.fs.store.audit.AuditSpan;\n import org.apache.hadoop.util.DurationInfo;\n \n-import static org.apache.hadoop.fs.impl.FutureIOSupport.raiseInnerCause;\n+import static org.apache.hadoop.util.functional.FutureIO.raiseInnerCause;\n \n /**\n  * A bridge from Callable to Supplier; catching exceptions"
    },
    {
        "commit_id": "e0cd0a82e032b926774dcea69edd4fa20aae2603",
        "commit_message": "HADOOP-16202. Enhanced openFile(): hadoop-aws changes. (#2584/3)\n\nS3A input stream support for the few fs.option.openfile settings.\nAs well as supporting the read policy option and values,\nif the file length is declared in fs.option.openfile.length\nthen no HEAD request will be issued when opening a file.\nThis can cut a few tens of milliseconds off the operation.\n\nThe patch adds a new openfile parameter/FS configuration option\nfs.s3a.input.async.drain.threshold (default: 16000).\nIt declares the number of bytes remaining in the http input stream\nabove which any operation to read and discard the rest of the stream,\n\"draining\", is executed asynchronously.\nThis asynchronous draining offers some performance benefit on seek-heavy\nfile IO.\n\nContributed by Steve Loughran.\n\nChange-Id: I9b0626bbe635e9fd97ac0f463f5e7167e0111e39",
        "commit_url": "https://github.com/apache/hadoop/commit/e0cd0a82e032b926774dcea69edd4fa20aae2603",
        "buggy_code": "options.addAll(InternalConstants.STANDARD_OPENFILE_KEYS);",
        "fixed_code": "options.addAll(InternalConstants.S3A_OPENFILE_KEYS);",
        "patch": "@@ -71,7 +71,7 @@ private InternalSelectConstants() {\n         CSV_OUTPUT_QUOTE_FIELDS,\n         CSV_OUTPUT_RECORD_DELIMITER\n     ));\n-    options.addAll(InternalConstants.STANDARD_OPENFILE_KEYS);\n+    options.addAll(InternalConstants.S3A_OPENFILE_KEYS);\n     SELECT_OPTIONS = Collections.unmodifiableSet(options);\n   }\n }"
    },
    {
        "commit_id": "e0cd0a82e032b926774dcea69edd4fa20aae2603",
        "commit_message": "HADOOP-16202. Enhanced openFile(): hadoop-aws changes. (#2584/3)\n\nS3A input stream support for the few fs.option.openfile settings.\nAs well as supporting the read policy option and values,\nif the file length is declared in fs.option.openfile.length\nthen no HEAD request will be issued when opening a file.\nThis can cut a few tens of milliseconds off the operation.\n\nThe patch adds a new openfile parameter/FS configuration option\nfs.s3a.input.async.drain.threshold (default: 16000).\nIt declares the number of bytes remaining in the http input stream\nabove which any operation to read and discard the rest of the stream,\n\"draining\", is executed asynchronously.\nThis asynchronous draining offers some performance benefit on seek-heavy\nfile IO.\n\nContributed by Steve Loughran.\n\nChange-Id: I9b0626bbe635e9fd97ac0f463f5e7167e0111e39",
        "commit_url": "https://github.com/apache/hadoop/commit/e0cd0a82e032b926774dcea69edd4fa20aae2603",
        "buggy_code": "+ \"require them\");",
        "fixed_code": "+ \" require them\");",
        "patch": "@@ -79,7 +79,7 @@ public void testVersionCheckingHandlingNoVersions() throws Throwable {\n   public void testVersionCheckingHandlingNoVersionsVersionRequired()\n       throws Throwable {\n     LOG.info(\"If an endpoint doesn't return versions but we are configured to\"\n-        + \"require them\");\n+        + \" require them\");\n     ChangeTracker tracker = newTracker(\n         ChangeDetectionPolicy.Mode.Client,\n         ChangeDetectionPolicy.Source.VersionId,"
    },
    {
        "commit_id": "e0cd0a82e032b926774dcea69edd4fa20aae2603",
        "commit_message": "HADOOP-16202. Enhanced openFile(): hadoop-aws changes. (#2584/3)\n\nS3A input stream support for the few fs.option.openfile settings.\nAs well as supporting the read policy option and values,\nif the file length is declared in fs.option.openfile.length\nthen no HEAD request will be issued when opening a file.\nThis can cut a few tens of milliseconds off the operation.\n\nThe patch adds a new openfile parameter/FS configuration option\nfs.s3a.input.async.drain.threshold (default: 16000).\nIt declares the number of bytes remaining in the http input stream\nabove which any operation to read and discard the rest of the stream,\n\"draining\", is executed asynchronously.\nThis asynchronous draining offers some performance benefit on seek-heavy\nfile IO.\n\nContributed by Steve Loughran.\n\nChange-Id: I9b0626bbe635e9fd97ac0f463f5e7167e0111e39",
        "commit_url": "https://github.com/apache/hadoop/commit/e0cd0a82e032b926774dcea69edd4fa20aae2603",
        "buggy_code": "import static org.apache.hadoop.fs.impl.FutureIOSupport.awaitFuture;",
        "fixed_code": "import static org.apache.hadoop.util.functional.FutureIO.awaitFuture;",
        "patch": "@@ -60,11 +60,11 @@\n import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl;\n import org.apache.hadoop.util.DurationInfo;\n \n-import static org.apache.hadoop.fs.impl.FutureIOSupport.awaitFuture;\n import static org.apache.hadoop.fs.s3a.S3ATestUtils.getLandsatCSVPath;\n import static org.apache.hadoop.fs.s3a.select.CsvFile.ALL_QUOTES;\n import static org.apache.hadoop.fs.s3a.select.SelectConstants.*;\n import static org.apache.hadoop.test.LambdaTestUtils.intercept;\n+import static org.apache.hadoop.util.functional.FutureIO.awaitFuture;\n \n /**\n  * Superclass for S3 Select tests."
    },
    {
        "commit_id": "4b1a6bfb10cacf1b5e070edf09b74ff8a3c680a7",
        "commit_message": "YARN-11102. Fix spotbugs error in hadoop-sls module. Contributed by Szilard Nemeth, Andras Gyori.",
        "commit_url": "https://github.com/apache/hadoop/commit/4b1a6bfb10cacf1b5e070edf09b74ff8a3c680a7",
        "buggy_code": "this.inputTraces = inputTraces;",
        "fixed_code": "this.inputTraces = inputTraces.clone();",
        "patch": "@@ -205,7 +205,7 @@ public void setInputType(TraceType inputType) {\n   }\n \n   public void setInputTraces(String[] inputTraces) {\n-    this.inputTraces = inputTraces;\n+    this.inputTraces = inputTraces.clone();\n   }\n \n   public int getNumNMs() {"
    },
    {
        "commit_id": "6eea28c3f3813594279b81c5be9cc3087bf3d99f",
        "commit_message": "HDFS-16498. Fix NPE for checkBlockReportLease #4057. Contributed by tomscut.\n\nReviewed-by: Ayush Saxena <ayushsaxena@apache.org>\nSigned-off-by: He Xiaoqiao <hexiaoqiao@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/6eea28c3f3813594279b81c5be9cc3087bf3d99f",
        "buggy_code": "LOG.debug(\"Datanode {} is attempting to report but not register yet.\",",
        "fixed_code": "LOG.warn(\"Datanode {} is attempting to report but not register yet.\",",
        "patch": "@@ -1641,7 +1641,7 @@ public DatanodeCommand blockReport(final DatanodeRegistration nodeReg,\n         }\n       }\n     } catch (UnregisteredNodeException une) {\n-      LOG.debug(\"Datanode {} is attempting to report but not register yet.\",\n+      LOG.warn(\"Datanode {} is attempting to report but not register yet.\",\n           nodeReg);\n       return RegisterCommand.REGISTER;\n     }"
    },
    {
        "commit_id": "4537b34e1c088f2b6d61c9bad8b82438cd94b944",
        "commit_message": "YARN-11089. Fix typo in RM audit log. Contributed by Junfan Zhang.",
        "commit_url": "https://github.com/apache/hadoop/commit/4537b34e1c088f2b6d61c9bad8b82438cd94b944",
        "buggy_code": "String operation = \"UNKONWN\";",
        "fixed_code": "String operation = \"UNKNOWN\";",
        "patch": "@@ -294,7 +294,7 @@ protected synchronized void finishApplication(ApplicationId applicationId) {\n \n   protected void writeAuditLog(ApplicationId appId) {\n     RMApp app = rmContext.getRMApps().get(appId);\n-    String operation = \"UNKONWN\";\n+    String operation = \"UNKNOWN\";\n     boolean success = false;\n     switch (app.getState()) {\n       case FAILED:"
    },
    {
        "commit_id": "8294bd5a37c0de15af576700c6cba46791eddd07",
        "commit_message": "HADOOP-18163. hadoop-azure support for the Manifest Committer of MAPREDUCE-7341\n\nFollow-on patch to MAPREDUCE-7341, adding ABFS support and tests\n\n* resilient rename\n* tests for job commit through the manifest committer.\n\ncontains\n- HADOOP-17976. ABFS etag extraction inconsistent between LIST and HEAD calls\n- HADOOP-16204. ABFS tests to include terasort\n\nContributed by Steve Loughran.\n\nChange-Id: I0a7d4043bdf19bcb00c033fc389730109b93b77f",
        "commit_url": "https://github.com/apache/hadoop/commit/8294bd5a37c0de15af576700c6cba46791eddd07",
        "buggy_code": "protected AbfsFileSystemContract(final Configuration conf, boolean secure) {",
        "fixed_code": "public AbfsFileSystemContract(final Configuration conf, boolean secure) {",
        "patch": "@@ -34,7 +34,7 @@ public class AbfsFileSystemContract extends AbstractBondedFSContract {\n   public static final String CONTRACT_XML = \"abfs.xml\";\n   private final boolean isSecure;\n \n-  protected AbfsFileSystemContract(final Configuration conf, boolean secure) {\n+  public AbfsFileSystemContract(final Configuration conf, boolean secure) {\n     super(conf);\n     //insert the base features\n     addConfResource(CONTRACT_XML);"
    },
    {
        "commit_id": "672e380c4f6ffcb0a6fee6d8263166e16b4323c2",
        "commit_message": "HADOOP-18112: Implement paging during multi object delete. (#4045)\n\n\r\nMulti object delete of size more than 1000 is not supported by S3 and \r\nfails with MalformedXML error. So implementing paging of requests to \r\nreduce the number of keys in a single request. Page size can be configured\r\nusing \"fs.s3a.bulk.delete.page.size\" \r\n\r\n Contributed By: Mukund Thakur",
        "commit_url": "https://github.com/apache/hadoop/commit/672e380c4f6ffcb0a6fee6d8263166e16b4323c2",
        "buggy_code": "operations.removeKeys(page, true, false));",
        "fixed_code": "operations.removeKeys(page, true));",
        "patch": "@@ -817,7 +817,7 @@ pages, suffix(pages),\n           end);\n       once(\"Remove S3 Keys\",\n           tracker.getBasePath().toString(), () ->\n-              operations.removeKeys(page, true, false));\n+              operations.removeKeys(page, true));\n       summary.deleteRequests++;\n       // and move to the start of the next page\n       start = end;"
    },
    {
        "commit_id": "672e380c4f6ffcb0a6fee6d8263166e16b4323c2",
        "commit_message": "HADOOP-18112: Implement paging during multi object delete. (#4045)\n\n\r\nMulti object delete of size more than 1000 is not supported by S3 and \r\nfails with MalformedXML error. So implementing paging of requests to \r\nreduce the number of keys in a single request. Page size can be configured\r\nusing \"fs.s3a.bulk.delete.page.size\" \r\n\r\n Contributed By: Mukund Thakur",
        "commit_url": "https://github.com/apache/hadoop/commit/672e380c4f6ffcb0a6fee6d8263166e16b4323c2",
        "buggy_code": "a(factory.newBulkDeleteRequest(new ArrayList<>(), true));",
        "fixed_code": "a(factory.newBulkDeleteRequest(new ArrayList<>()));",
        "patch": "@@ -164,7 +164,7 @@ private void createFactoryObjects(RequestFactory factory) {\n         new ArrayList<>()));\n     a(factory.newCopyObjectRequest(path, path2, md));\n     a(factory.newDeleteObjectRequest(path));\n-    a(factory.newBulkDeleteRequest(new ArrayList<>(), true));\n+    a(factory.newBulkDeleteRequest(new ArrayList<>()));\n     a(factory.newDirectoryMarkerRequest(path));\n     a(factory.newGetObjectRequest(path));\n     a(factory.newGetObjectMetadataRequest(path));"
    },
    {
        "commit_id": "672e380c4f6ffcb0a6fee6d8263166e16b4323c2",
        "commit_message": "HADOOP-18112: Implement paging during multi object delete. (#4045)\n\n\r\nMulti object delete of size more than 1000 is not supported by S3 and \r\nfails with MalformedXML error. So implementing paging of requests to \r\nreduce the number of keys in a single request. Page size can be configured\r\nusing \"fs.s3a.bulk.delete.page.size\" \r\n\r\n Contributed By: Mukund Thakur",
        "commit_url": "https://github.com/apache/hadoop/commit/672e380c4f6ffcb0a6fee6d8263166e16b4323c2",
        "buggy_code": "import static org.apache.hadoop.fs.s3a.impl.ITestPartialRenamesDeletes.createFiles;",
        "fixed_code": "import static org.apache.hadoop.fs.s3a.S3ATestUtils.createFiles;",
        "patch": "@@ -38,7 +38,7 @@\n import static org.apache.hadoop.fs.s3a.Constants.EXPERIMENTAL_AWS_INTERNAL_THROTTLING;\n import static org.apache.hadoop.fs.s3a.Constants.USER_AGENT_PREFIX;\n import static org.apache.hadoop.fs.s3a.S3ATestUtils.lsR;\n-import static org.apache.hadoop.fs.s3a.impl.ITestPartialRenamesDeletes.createFiles;\n+import static org.apache.hadoop.fs.s3a.S3ATestUtils.createFiles;\n import static org.apache.hadoop.test.GenericTestUtils.filenameOfIndex;\n \n /**"
    },
    {
        "commit_id": "356d337d1e5a97dc72ae209df919cfb99e0dd841",
        "commit_message": "YARN-11042. Fix testQueueSubmitWithACLsEnabledWithQueueMapping in TestAppManager. Contributed by Tamas Domok",
        "commit_url": "https://github.com/apache/hadoop/commit/356d337d1e5a97dc72ae209df919cfb99e0dd841",
        "buggy_code": "asContext.setQueue(\"test\");",
        "fixed_code": "asContext.setQueue(\"oldQueue\");",
        "patch": "@@ -331,7 +331,7 @@ public void testQueueSubmitWithACLsEnabledWithQueueMapping()\n     csConf.set(PREFIX + \"root.test.acl_submit_applications\", \"test\");\n     csConf.set(PREFIX + \"root.test.acl_administer_queue\", \"test\");\n \n-    asContext.setQueue(\"test\");\n+    asContext.setQueue(\"oldQueue\");\n \n     MockRM newMockRM = new MockRM(csConf);\n     RMContext newMockRMContext = newMockRM.getRMContext();"
    },
    {
        "commit_id": "bb1135c77c34ca5dcd8d43fca31c635b65bf3638",
        "commit_message": "YARN-10894. Follow up YARN-10237: fix the new test case in TestRMWebServicesCapacitySched. Contributed by Tamas Domok",
        "commit_url": "https://github.com/apache/hadoop/commit/bb1135c77c34ca5dcd8d43fca31c635b65bf3638",
        "buggy_code": "new String[] {\"a\", \"b\"});",
        "fixed_code": "new String[] {\"a\", \"b\", \"c\"});",
        "patch": "@@ -120,7 +120,7 @@ public static void setupQueueConfiguration(\n \n     // Define top-level queues\n     config.setQueues(CapacitySchedulerConfiguration.ROOT,\n-        new String[] {\"a\", \"b\"});\n+        new String[] {\"a\", \"b\", \"c\"});\n \n     final String a = CapacitySchedulerConfiguration.ROOT + \".a\";\n     config.setCapacity(a, 10.5f);"
    },
    {
        "commit_id": "14ba19af06e22b6c35e7fd72f6ad5610d796c9cb",
        "commit_message": "HADOOP-17409. Remove s3guard from S3A module (#3534)\n\n\r\nCompletely removes S3Guard support from the S3A codebase.\r\n\r\nIf the connector is configured to use any metastore other than\r\nthe null and local stores (i.e. DynamoDB is selected) the s3a client\r\nwill raise an exception and refuse to initialize.\r\n\r\nThis is to ensure that there is no mix of S3Guard enabled and disabled\r\ndeployments with the same configuration but different hadoop releases\r\n-it must be turned off completely.\r\n\r\nThe \"hadoop s3guard\" command has been retained -but the supported\r\nsubcommands have been reduced to those which are not purely S3Guard\r\nrelated: \"bucket-info\" and \"uploads\".\r\n\r\nThis is major change in terms of the number of files\r\nchanged; before cherry picking subsequent s3a patches into\r\nolder releases, this patch will probably need backporting\r\nfirst.\r\n\r\nGoodbye S3Guard, your work is done. Time to die.\r\n\r\nContributed by Steve Loughran.",
        "commit_url": "https://github.com/apache/hadoop/commit/14ba19af06e22b6c35e7fd72f6ad5610d796c9cb",
        "buggy_code": "+ \" stream writing to {}. This is unsupported\",",
        "fixed_code": "+ \" stream writing to {}. This is Unsupported\",",
        "patch": "@@ -674,7 +674,7 @@ private void handleSyncableInvocation() {\n     }\n     // downgrading.\n     WARN_ON_SYNCABLE.warn(\"Application invoked the Syncable API against\"\n-        + \" stream writing to {}. This is unsupported\",\n+        + \" stream writing to {}. This is Unsupported\",\n         key);\n     // and log at debug\n     LOG.debug(\"Downgrading Syncable call\", ex);"
    },
    {
        "commit_id": "9eea0e28f2d79c3691bee29cb3fc4123062e5f7a",
        "commit_message": "HDFS-16409. Fix typo: testHasExeceptionsReturnsCorrectValue -> testHasExceptionsReturnsCorrectValue (#3835)\n\nReviewed-by: Fei Hui <feihui.ustc@gmail.com>\r\nSigned-off-by: Akira Ajisaka <aajisaka@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/9eea0e28f2d79c3691bee29cb3fc4123062e5f7a",
        "buggy_code": "public void testHasExeceptionsReturnsCorrectValue() {",
        "fixed_code": "public void testHasExceptionsReturnsCorrectValue() {",
        "patch": "@@ -34,7 +34,7 @@\n public class TestAddBlockPoolException {\n \n   @Test\n-  public void testHasExeceptionsReturnsCorrectValue() {\n+  public void testHasExceptionsReturnsCorrectValue() {\n     AddBlockPoolException e = new AddBlockPoolException();\n     assertFalse(e.hasExceptions());\n "
    },
    {
        "commit_id": "61d424f385539c14172386c19ecda5ea57af2632",
        "commit_message": "HDFS-16393. RBF: Fix TestRouterRPCMultipleDestinationMountTableResolver. (#3849). Contributed by Ayush Saxena.\n\nReviewed-by: Inigo Goiri <inigoiri@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/61d424f385539c14172386c19ecda5ea57af2632",
        "buggy_code": "dfsCluster.restartNameNode(0, false);",
        "fixed_code": "dfsCluster.restartNameNode(0);",
        "patch": "@@ -679,7 +679,7 @@ public void testInvokeAtAvailableNs() throws IOException {\n           rpcServer.invokeAtAvailableNs(method, FsServerDefaults.class);\n       assertNotNull(serverDefaults);\n     } finally {\n-      dfsCluster.restartNameNode(0, false);\n+      dfsCluster.restartNameNode(0);\n       dfsCluster.restartNameNode(1);\n     }\n   }"
    },
    {
        "commit_id": "ebdbe7eb82464ffe1bc7fdc568f33c9dd8d7260b",
        "commit_message": "HADOOP-18057. Fix typo: validateEncrytionSecrets -> validateEncryptionSecrets (#3826)",
        "commit_url": "https://github.com/apache/hadoop/commit/ebdbe7eb82464ffe1bc7fdc568f33c9dd8d7260b",
        "buggy_code": "validateEncrytionSecrets(secrets);",
        "fixed_code": "validateEncryptionSecrets(secrets);",
        "patch": "@@ -135,7 +135,7 @@ public void testEncryptionOverRename2() throws Throwable {\n     Path src = path(createFilename(1024));\n     byte[] data = dataset(1024, 'a', 'z');\n     EncryptionSecrets secrets = fs.getEncryptionSecrets();\n-    validateEncrytionSecrets(secrets);\n+    validateEncryptionSecrets(secrets);\n     writeDataset(fs, src, data, data.length, 1024 * 1024, true);\n     ContractTestUtils.verifyFileContents(fs, src, data);\n "
    },
    {
        "commit_id": "1ebdac2cd6a02c4de76802a0fbf6bd1c42636067",
        "commit_message": "HDFS-16361. Fix log format for QueryCommand (#3732). Contributed by tomscut.",
        "commit_url": "https://github.com/apache/hadoop/commit/1ebdac2cd6a02c4de76802a0fbf6bd1c42636067",
        "buggy_code": "LOG.error(\"Query plan failed. ex: {}\", ex);",
        "fixed_code": "LOG.error(\"Query plan failed.\", ex);",
        "patch": "@@ -84,7 +84,7 @@ public void execute(CommandLine cmd) throws Exception {\n         System.out.printf(\"%s\", workStatus.currentStateString());\n       }\n     } catch (DiskBalancerException ex) {\n-      LOG.error(\"Query plan failed. ex: {}\", ex);\n+      LOG.error(\"Query plan failed.\", ex);\n       throw ex;\n     }\n   }"
    },
    {
        "commit_id": "08f3df3ea2afcce3973749199269a032df3732e0",
        "commit_message": "YARN-10991. Fix to ignore the grouping \"[]\" for resourcesStr in parseResourcesString method (#3592)\n\nSigned-off-by: Akira Ajisaka <aajisaka@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/08f3df3ea2afcce3973749199269a032df3732e0",
        "buggy_code": "resourcesStr = resourcesStr.substring(0, resourcesStr.length());",
        "fixed_code": "resourcesStr = resourcesStr.substring(0, resourcesStr.length() - 1);",
        "patch": "@@ -1373,7 +1373,7 @@ static Map<String, Long> parseResourcesString(String resourcesStr) {\n       resourcesStr = resourcesStr.substring(1);\n     }\n     if (resourcesStr.endsWith(\"]\")) {\n-      resourcesStr = resourcesStr.substring(0, resourcesStr.length());\n+      resourcesStr = resourcesStr.substring(0, resourcesStr.length() - 1);\n     }\n \n     for (String resource : resourcesStr.trim().split(\",\")) {"
    },
    {
        "commit_id": "dc751df63b4ab2c9c26a1efe7479c31fd1de80d5",
        "commit_message": "HDFS-16329. Fix log format for BlockManager (#3670)\n\nSigned-off-by: Akira Ajisaka <aajisaka@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/dc751df63b4ab2c9c26a1efe7479c31fd1de80d5",
        "buggy_code": "\" blocks or blocks pending reconstruction. Safe to decommission or\",",
        "fixed_code": "\" blocks or blocks pending reconstruction. Safe to decommission or\" +",
        "patch": "@@ -4541,7 +4541,7 @@ boolean isNodeHealthyForDecommissionOrMaintenance(DatanodeDescriptor node) {\n     if (pendingReconstructionBlocksCount == 0 &&\n         lowRedundancyBlocksCount == 0) {\n       LOG.info(\"Node {} is dead and there are no low redundancy\" +\n-          \" blocks or blocks pending reconstruction. Safe to decommission or\",\n+          \" blocks or blocks pending reconstruction. Safe to decommission or\" +\n           \" put in maintenance.\", node);\n       return true;\n     }"
    },
    {
        "commit_id": "6e6f2e4baad24a9b6972c540b7f5233e89f9fa76",
        "commit_message": "HDFS-16321. Fix invalid config in TestAvailableSpaceRackFaultTolerantBPP  (#3655). Contributed by guo.\n\nSigned-off-by: Ayush Saxena <ayushsaxena@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/6e6f2e4baad24a9b6972c540b7f5233e89f9fa76",
        "buggy_code": "DFSConfigKeys.DFS_NAMENODE_AVAILABLE_SPACE_BLOCK_PLACEMENT_POLICY_BALANCED_SPACE_PREFERENCE_FRACTION_KEY,",
        "fixed_code": "DFSConfigKeys.DFS_NAMENODE_AVAILABLE_SPACE_RACK_FAULT_TOLERANT_BLOCK_PLACEMENT_POLICY_BALANCED_SPACE_PREFERENCE_FRACTION_KEY,",
        "patch": "@@ -65,7 +65,7 @@ public class TestAvailableSpaceRackFaultTolerantBPP {\n   public static void setupCluster() throws Exception {\n     conf = new HdfsConfiguration();\n     conf.setFloat(\n-        DFSConfigKeys.DFS_NAMENODE_AVAILABLE_SPACE_BLOCK_PLACEMENT_POLICY_BALANCED_SPACE_PREFERENCE_FRACTION_KEY,\n+        DFSConfigKeys.DFS_NAMENODE_AVAILABLE_SPACE_RACK_FAULT_TOLERANT_BLOCK_PLACEMENT_POLICY_BALANCED_SPACE_PREFERENCE_FRACTION_KEY,\n         0.6f);\n     String[] racks = new String[NUM_RACKS];\n     for (int i = 0; i < NUM_RACKS; i++) {"
    },
    {
        "commit_id": "1c1cf64616f34d039cf9246da9613914aa870515",
        "commit_message": "HDFS-16311. Metric metadataOperationRate calculation error in DataNodeVolumeMetrics (#3636)\n\nReviewed-by: Ayush Saxena <ayushsaxena@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/1c1cf64616f34d039cf9246da9613914aa870515",
        "buggy_code": "metadataOperationRate.add(latency);",
        "fixed_code": "fileIoErrorRate.add(latency);",
        "patch": "@@ -284,6 +284,6 @@ public void addWriteIoLatency(final long latency) {\n \n   public void addFileIoError(final long latency) {\n     totalFileIoErrors.incr();\n-    metadataOperationRate.add(latency);\n+    fileIoErrorRate.add(latency);\n   }\n }\n\\ No newline at end of file"
    },
    {
        "commit_id": "2a1a11c039518bab7e5b581ef23bc5a2e8c81544",
        "commit_message": "HDFS-16312. Fix typo for DataNodeVolumeMetrics and ProfilingFileIoEvents (#3637)\n\nReviewed-by: Hui Fei <ferhui@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/2a1a11c039518bab7e5b581ef23bc5a2e8c81544",
        "buggy_code": "metrics.addMetadastaOperationLatency(Time.monotonicNow() - begin);",
        "fixed_code": "metrics.addMetadataOperationLatency(Time.monotonicNow() - begin);",
        "patch": "@@ -80,7 +80,7 @@ public void afterMetadataOp(@Nullable FsVolumeSpi volume,\n     if (isEnabled) {\n       DataNodeVolumeMetrics metrics = getVolumeMetrics(volume);\n       if (metrics != null) {\n-        metrics.addMetadastaOperationLatency(Time.monotonicNow() - begin);\n+        metrics.addMetadataOperationLatency(Time.monotonicNow() - begin);\n       }\n     }\n   }"
    },
    {
        "commit_id": "ea65fc26d80db478f7e49749065c69da7e241bf0",
        "commit_message": "HDFS-16298. Improve error msg for BlockMissingException (#3615)\n\nReviewed-by: Hui Fei <ferhui@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/ea65fc26d80db478f7e49749065c69da7e241bf0",
        "buggy_code": "throw new BlockMissingException(src, description,",
        "fixed_code": "throw new BlockMissingException(src, description + errMsg,",
        "patch": "@@ -1003,7 +1003,7 @@ private LocatedBlock refetchLocations(LocatedBlock block,\n       String description = \"Could not obtain block: \" + blockInfo;\n       DFSClient.LOG.warn(description + errMsg\n           + \". Throwing a BlockMissingException\");\n-      throw new BlockMissingException(src, description,\n+      throw new BlockMissingException(src, description + errMsg,\n           block.getStartOffset());\n     }\n "
    },
    {
        "commit_id": "cbc7fb6bcad8c959830a84521ea642cbf973ea83",
        "commit_message": "HDFS-16299. Fix bug for TestDataNodeVolumeMetrics#verifyDataNodeVolumeMetrics (#3616)\n\nReviewed-by: Inigo Goiri <inigoiri@apache.org>\r\nReviewed-by: Ayush Saxena <ayushsaxena@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/cbc7fb6bcad8c959830a84521ea642cbf973ea83",
        "buggy_code": "LOG.info(\"readIoSampleCount : \" + metrics.getReadIoMean());",
        "fixed_code": "LOG.info(\"readIoSampleCount : \" + metrics.getReadIoSampleCount());",
        "patch": "@@ -166,7 +166,7 @@ private void verifyDataNodeVolumeMetrics(final FileSystem fs,\n     LOG.info(\"syncIoMean : \" + metrics.getSyncIoMean());\n     LOG.info(\"syncIoStdDev : \" + metrics.getSyncIoStdDev());\n \n-    LOG.info(\"readIoSampleCount : \" + metrics.getReadIoMean());\n+    LOG.info(\"readIoSampleCount : \" + metrics.getReadIoSampleCount());\n     LOG.info(\"readIoMean : \" + metrics.getReadIoMean());\n     LOG.info(\"readIoStdDev : \" + metrics.getReadIoStdDev());\n "
    },
    {
        "commit_id": "6c6d1b64d4a7cd5288fcded78043acaf23228f96",
        "commit_message": "HADOOP-17928. Syncable: S3A to warn and downgrade (#3585)\n\n\r\nThis switches the default behavior of S3A output streams\r\nto warning that Syncable.hsync() or hflush() have been\r\ncalled; it's not considered an error unless the defaults\r\nare overridden.\r\n\r\nThis avoids breaking applications which call the APIs,\r\nat the risk of people trying to use S3 as a safe store\r\nof streamed data (HBase WALs, audit logs etc).\r\n\r\nContributed by Steve Loughran.",
        "commit_url": "https://github.com/apache/hadoop/commit/6c6d1b64d4a7cd5288fcded78043acaf23228f96",
        "buggy_code": "false;",
        "fixed_code": "true;",
        "patch": "@@ -387,7 +387,7 @@ private Constants() {\n    * Value: {@value}.\n    */\n   public static final boolean DOWNGRADE_SYNCABLE_EXCEPTIONS_DEFAULT =\n-      false;\n+      true;\n \n   /**\n    * The capacity of executor queues for operations other than block"
    },
    {
        "commit_id": "c1a82853635d384a83a03e0a43ee76ae423bfc77",
        "commit_message": "HDFS-16281. Fix flaky unit tests failed due to timeout (#3574)",
        "commit_url": "https://github.com/apache/hadoop/commit/c1a82853635d384a83a03e0a43ee76ae423bfc77",
        "buggy_code": "@Test",
        "fixed_code": "@Test(timeout = 60000)",
        "patch": "@@ -103,7 +103,7 @@ protected String getDefaultWorkingDirectory() {\n   }\n \n   @Override\n-  @Test\n+  @Test(timeout = 60000)\n   public void testAppend() throws IOException {\n     AppendTestUtil.testAppend(fs, new Path(\"/append/f\"));\n   }"
    },
    {
        "commit_id": "c1a82853635d384a83a03e0a43ee76ae423bfc77",
        "commit_message": "HDFS-16281. Fix flaky unit tests failed due to timeout (#3574)",
        "commit_url": "https://github.com/apache/hadoop/commit/c1a82853635d384a83a03e0a43ee76ae423bfc77",
        "buggy_code": "@Test",
        "fixed_code": "@Test(timeout = 60000)",
        "patch": "@@ -63,7 +63,7 @@ protected String getDefaultWorkingDirectory() {\n     return defaultWorkingDirectory;\n   }\n \n-  @Test\n+  @Test(timeout = 60000)\n   public void testAppend() throws IOException {\n     AppendTestUtil.testAppend(fs, new Path(\"/testAppend/f\"));\n   }"
    },
    {
        "commit_id": "c1a82853635d384a83a03e0a43ee76ae423bfc77",
        "commit_message": "HDFS-16281. Fix flaky unit tests failed due to timeout (#3574)",
        "commit_url": "https://github.com/apache/hadoop/commit/c1a82853635d384a83a03e0a43ee76ae423bfc77",
        "buggy_code": "@Test (timeout=60000)",
        "fixed_code": "@Test (timeout=120000)",
        "patch": "@@ -224,7 +224,7 @@ public void testSnapshotCommandsWithURI()throws Exception {\n     fs.delete(new Path(\"/Fully/QPath\"), true);\n   }\n \n-  @Test (timeout=60000)\n+  @Test (timeout=120000)\n   public void testSnapshotDiff()throws Exception {\n     Configuration config = new HdfsConfiguration();\n     Path snapDirPath = new Path(fs.getUri().toString() + \"/snap_dir\");"
    },
    {
        "commit_id": "c1a82853635d384a83a03e0a43ee76ae423bfc77",
        "commit_message": "HDFS-16281. Fix flaky unit tests failed due to timeout (#3574)",
        "commit_url": "https://github.com/apache/hadoop/commit/c1a82853635d384a83a03e0a43ee76ae423bfc77",
        "buggy_code": "@Test(timeout = 60000)",
        "fixed_code": "@Test(timeout = 120000)",
        "patch": "@@ -158,7 +158,7 @@ public void testSubmitPlanInNonRegularStatus() throws Exception {\n    * Tests running multiple commands under on setup. This mainly covers\n    * {@link org.apache.hadoop.hdfs.server.diskbalancer.command.Command#close}\n    */\n-  @Test(timeout = 60000)\n+  @Test(timeout = 120000)\n   public void testRunMultipleCommandsUnderOneSetup() throws Exception {\n \n     final int numDatanodes = 1;"
    },
    {
        "commit_id": "c1a82853635d384a83a03e0a43ee76ae423bfc77",
        "commit_message": "HDFS-16281. Fix flaky unit tests failed due to timeout (#3574)",
        "commit_url": "https://github.com/apache/hadoop/commit/c1a82853635d384a83a03e0a43ee76ae423bfc77",
        "buggy_code": "@Test(timeout=60000)",
        "fixed_code": "@Test(timeout=120000)",
        "patch": "@@ -192,7 +192,7 @@ private void testStorageTypes(StorageType[][] storageTypes,\n    * Types.\n    * @throws IOException\n    */\n-  @Test(timeout=60000)\n+  @Test(timeout=120000)\n   public void testTargetStorageTypes() throws ReconfigurationException,\n       InterruptedException, TimeoutException, IOException {\n     // DISK and not anything else."
    },
    {
        "commit_id": "c1a82853635d384a83a03e0a43ee76ae423bfc77",
        "commit_message": "HDFS-16281. Fix flaky unit tests failed due to timeout (#3574)",
        "commit_url": "https://github.com/apache/hadoop/commit/c1a82853635d384a83a03e0a43ee76ae423bfc77",
        "buggy_code": "@Test",
        "fixed_code": "@Test(timeout = 60000)",
        "patch": "@@ -396,7 +396,7 @@ public void testOffsetPlusLengthParamsLongerThanFile() throws IOException {\n     }\n   }\n \n-  @Test\n+  @Test(timeout = 60000)\n   public void testResponseCode() throws IOException {\n     final WebHdfsFileSystem webhdfs = (WebHdfsFileSystem)fs;\n     final Path root = new Path(\"/\");"
    },
    {
        "commit_id": "2c37bebac447c61b3d2c2bb492ee7e6243b5ce37",
        "commit_message": "HDFS-16280. Fix typo for ShortCircuitReplica#isStale (#3568). Contributed by tomscut.\n\nReviewed-by: Hui Fei <ferhui@apache.org>\r\nSigned-off-by: Ayush Saxena <ayushsaxena@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/2c37bebac447c61b3d2c2bb492ee7e6243b5ce37",
        "buggy_code": "LOG.trace(\"{} is stale because it's {} ms old and staleThreadholdMS={}\",",
        "fixed_code": "LOG.trace(\"{} is stale because it's {} ms old and staleThresholdMs={}\",",
        "patch": "@@ -160,7 +160,7 @@ boolean isStale() {\n       long deltaMs = Time.monotonicNow() - creationTimeMs;\n       long staleThresholdMs = cache.getStaleThresholdMs();\n       if (deltaMs > staleThresholdMs) {\n-        LOG.trace(\"{} is stale because it's {} ms old and staleThreadholdMS={}\",\n+        LOG.trace(\"{} is stale because it's {} ms old and staleThresholdMs={}\",\n             this, deltaMs, staleThresholdMs);\n         return true;\n       } else {"
    },
    {
        "commit_id": "a73ff6915ae3e0ced1b4c814a94845f51e655a0c",
        "commit_message": "HDFS-7612: Fix default cache directory in TestOfflineEditsViewer.testStored. Contributed by Michael Kuchenbecker (#3571)",
        "commit_url": "https://github.com/apache/hadoop/commit/a73ff6915ae3e0ced1b4c814a94845f51e655a0c",
        "buggy_code": "\"build/test/cache\");",
        "fixed_code": "\"target/test-classes\");",
        "patch": "@@ -164,7 +164,7 @@ public void testRecoveryMode() throws IOException {\n   public void testStored() throws IOException {\n     // reference edits stored with source code (see build.xml)\n     final String cacheDir = System.getProperty(\"test.cache.data\",\n-        \"build/test/cache\");\n+        \"target/test-classes\");\n     // binary, XML, reparsed binary\n     String editsStored = cacheDir + \"/editsStored\";\n     String editsStoredParsedXml = cacheDir + \"/editsStoredParsed.xml\";"
    },
    {
        "commit_id": "35eff545560d9275a03d3d2dcb019d7cdfc39e3e",
        "commit_message": "YARN-10934. Fix LeafQueue#activateApplication NPE when the user of the pending application is missing from usersManager. Contributed by Benjamin Teke \n\nCo-authored-by: Benjamin Teke <bteke@cloudera.com>",
        "commit_url": "https://github.com/apache/hadoop/commit/35eff545560d9275a03d3d2dcb019d7cdfc39e3e",
        "buggy_code": "User user = getUser(application.getUser());",
        "fixed_code": "User user = usersManager.getUserAndAddIfAbsent(application.getUser());",
        "patch": "@@ -878,7 +878,7 @@ protected void activateApplications() {\n         }\n \n         // Check user am resource limit\n-        User user = getUser(application.getUser());\n+        User user = usersManager.getUserAndAddIfAbsent(application.getUser());\n         Resource userAMLimit = userAmPartitionLimit.get(partitionName);\n \n         // Verify whether we already calculated user-am-limit for this label."
    },
    {
        "commit_id": "acffe203b8128989a1cde872dc5576c810e5a0f0",
        "commit_message": "HADOOP-17195. ABFS: OutOfMemory error while uploading huge files (#3446)\n\n\r\nAddresses the problem of processes running out of memory when\r\nthere are many ABFS output streams queuing data to upload,\r\nespecially when the network upload bandwidth is less than the rate\r\ndata is generated.\r\n\r\nABFS Output streams now buffer their blocks of data to\r\n\"disk\", \"bytebuffer\" or \"array\", as set in\r\n\"fs.azure.data.blocks.buffer\"\r\n\r\nWhen buffering via disk, the location for temporary storage\r\nis set in \"fs.azure.buffer.dir\"\r\n\r\nFor safe scaling: use \"disk\" (default); for performance, when\r\nconfident that upload bandwidth will never be a bottleneck,\r\nexperiment with the memory options.\r\n\r\nThe number of blocks a single stream can have queued for uploading\r\nis set in \"fs.azure.block.upload.active.blocks\".\r\nThe default value is 20.\r\n\r\nContributed by Mehakmeet Singh.",
        "commit_url": "https://github.com/apache/hadoop/commit/acffe203b8128989a1cde872dc5576c810e5a0f0",
        "buggy_code": "Path path) throws AzureBlobFileSystemException {",
        "fixed_code": "Path path) throws IOException {",
        "patch": "@@ -488,7 +488,7 @@ protected AbfsDelegationTokenManager getDelegationTokenManager()\n    */\n   protected AbfsOutputStream createAbfsOutputStreamWithFlushEnabled(\n       AzureBlobFileSystem fs,\n-      Path path) throws AzureBlobFileSystemException {\n+      Path path) throws IOException {\n     AzureBlobFileSystemStore abfss = fs.getAbfsStore();\n     abfss.getAbfsConfiguration().setDisableOutputStreamFlush(false);\n "
    },
    {
        "commit_id": "10f3abeae76863222f59ce6b80c508100d07fcfd",
        "commit_message": "Revert \"HADOOP-17195. OutOfMemory error while performing hdfs CopyFromLocal to ABFS (#3406)\" (#3443)\n\nThis reverts commit 52c024cc3aac2571e60e69c7f8b620299aad8e27.",
        "commit_url": "https://github.com/apache/hadoop/commit/10f3abeae76863222f59ce6b80c508100d07fcfd",
        "buggy_code": "Path path) throws IOException {",
        "fixed_code": "Path path) throws AzureBlobFileSystemException {",
        "patch": "@@ -488,7 +488,7 @@ protected AbfsDelegationTokenManager getDelegationTokenManager()\n    */\n   protected AbfsOutputStream createAbfsOutputStreamWithFlushEnabled(\n       AzureBlobFileSystem fs,\n-      Path path) throws IOException {\n+      Path path) throws AzureBlobFileSystemException {\n     AzureBlobFileSystemStore abfss = fs.getAbfsStore();\n     abfss.getAbfsConfiguration().setDisableOutputStreamFlush(false);\n "
    },
    {
        "commit_id": "52c024cc3aac2571e60e69c7f8b620299aad8e27",
        "commit_message": "HADOOP-17195. OutOfMemory error while performing hdfs CopyFromLocal to ABFS (#3406)\n\n\r\nThis migrates the fs.s3a-server-side encryption configuration options\r\nto a name which covers client-side encryption too.\r\n\r\nfs.s3a.server-side-encryption-algorithm becomes fs.s3a.encryption.algorithm\r\nfs.s3a.server-side-encryption.key becomes fs.s3a.encryption.key\r\n\r\nThe existing keys remain valid, simply deprecated and remapped\r\nto the new values. If you want server-side encryption options\r\nto be picked up regardless of hadoop versions, use\r\nthe old keys.\r\n\r\n(the old key also works for CSE, though as no version of Hadoop\r\nwith CSE support has shipped without this remapping, it's less\r\nrelevant)\r\n\r\n\r\nContributed by: Mehakmeet Singh",
        "commit_url": "https://github.com/apache/hadoop/commit/52c024cc3aac2571e60e69c7f8b620299aad8e27",
        "buggy_code": "Path path) throws AzureBlobFileSystemException {",
        "fixed_code": "Path path) throws IOException {",
        "patch": "@@ -488,7 +488,7 @@ protected AbfsDelegationTokenManager getDelegationTokenManager()\n    */\n   protected AbfsOutputStream createAbfsOutputStreamWithFlushEnabled(\n       AzureBlobFileSystem fs,\n-      Path path) throws AzureBlobFileSystemException {\n+      Path path) throws IOException {\n     AzureBlobFileSystemStore abfss = fs.getAbfsStore();\n     abfss.getAbfsConfiguration().setDisableOutputStreamFlush(false);\n "
    },
    {
        "commit_id": "3ecaa39668b396a62c495ce7f4b837d795b61e93",
        "commit_message": "HDFS-16181. [SBN Read] Fix display of JournalNode metric RpcRequestCacheMissAmount (#3317)\n\nCo-authored-by: wangzhaohui8 <wangzhaohui8@jd.com>",
        "commit_url": "https://github.com/apache/hadoop/commit/3ecaa39668b396a62c495ce7f4b837d795b61e93",
        "buggy_code": "metrics.rpcRequestCacheMissAmount.add(cme.getCacheMissAmount());",
        "fixed_code": "metrics.addRpcRequestCacheMissAmount(cme.getCacheMissAmount());",
        "patch": "@@ -773,7 +773,7 @@ public GetJournaledEditsResponseProto getJournaledEdits(long sinceTxId,\n           .setEditLog(output.toByteString())\n           .build();\n     } catch (JournaledEditsCache.CacheMissException cme) {\n-      metrics.rpcRequestCacheMissAmount.add(cme.getCacheMissAmount());\n+      metrics.addRpcRequestCacheMissAmount(cme.getCacheMissAmount());\n       throw cme;\n     }\n   }"
    },
    {
        "commit_id": "b53cae0ffb1b383824a86185ef23f698719d7a6d",
        "commit_message": "HDFS-16157. Support configuring DNS record to get list of journal nodes contributed by Leon Gao. (#3284)\n\n* Add DNS resolution for QJM\r\n\r\n* Add log\r\n\r\n* Resolve comments\r\n\r\n* checkstyle\r\n\r\n* typo",
        "commit_url": "https://github.com/apache/hadoop/commit/b53cae0ffb1b383824a86185ef23f698719d7a6d",
        "buggy_code": "List<InetSocketAddress> addrs = Util.getAddressesList(uri);",
        "fixed_code": "List<InetSocketAddress> addrs = Util.getAddressesList(uri, conf);",
        "patch": "@@ -414,7 +414,7 @@ static List<AsyncLogger> createLoggers(Configuration conf,\n                                          String nameServiceId)\n       throws IOException {\n     List<AsyncLogger> ret = Lists.newArrayList();\n-    List<InetSocketAddress> addrs = Util.getAddressesList(uri);\n+    List<InetSocketAddress> addrs = Util.getAddressesList(uri, conf);\n     if (addrs.size() % 2 == 0) {\n       LOG.warn(\"Quorum journal URI '\" + uri + \"' has an even number \" +\n           \"of Journal Nodes specified. This is not recommended!\");"
    },
    {
        "commit_id": "b53cae0ffb1b383824a86185ef23f698719d7a6d",
        "commit_message": "HDFS-16157. Support configuring DNS record to get list of journal nodes contributed by Leon Gao. (#3284)\n\n* Add DNS resolution for QJM\r\n\r\n* Add log\r\n\r\n* Resolve comments\r\n\r\n* checkstyle\r\n\r\n* typo",
        "commit_url": "https://github.com/apache/hadoop/commit/b53cae0ffb1b383824a86185ef23f698719d7a6d",
        "buggy_code": "Sets.newHashSet(jn.getBoundIpcAddress()));",
        "fixed_code": "Sets.newHashSet(jn.getBoundIpcAddress()), conf);",
        "patch": "@@ -315,7 +315,7 @@ private List<InetSocketAddress> getJournalAddrList(String uriStr) throws\n       IOException {\n     URI uri = new URI(uriStr);\n     return Util.getLoggerAddresses(uri,\n-        Sets.newHashSet(jn.getBoundIpcAddress()));\n+        Sets.newHashSet(jn.getBoundIpcAddress()), conf);\n   }\n \n   private void getMissingLogSegments(List<RemoteEditLog> thisJournalEditLogs,"
    },
    {
        "commit_id": "07627ef19e2bf4c87f12b53e508edf8fee05856a",
        "commit_message": "HDFS-16177. Bug fix for Util#receiveFile (#3310)\n\nReviewed-by: Hui Fei <ferhui@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/07627ef19e2bf4c87f12b53e508edf8fee05856a",
        "buggy_code": "(flushStartTime - Time.monotonicNow())) / 1000.0, 0.001);",
        "fixed_code": "(Time.monotonicNow() - flushStartTime)) / 1000.0, 0.001);",
        "patch": "@@ -287,7 +287,7 @@ public static MD5Hash receiveFile(String url, List<File> localPaths,\n         fos.getChannel().force(true);\n         fos.close();\n         double writeSec = Math.max(((float)\n-            (flushStartTime - Time.monotonicNow())) / 1000.0, 0.001);\n+            (Time.monotonicNow() - flushStartTime)) / 1000.0, 0.001);\n         xferCombined += writeSec;\n         xferStats.append(String\n             .format(\" Synchronous (fsync) write to disk of \" +"
    },
    {
        "commit_id": "f813554769606d59c23bcdc184d52249793d0f12",
        "commit_message": "HADOOP-13887. Support S3 client side encryption (S3-CSE) using AWS-SDK (#2706)\n\n\r\nThis (big!) patch adds support for client side encryption in AWS S3,\r\nwith keys managed by AWS-KMS.\r\n\r\nRead the documentation in encryption.md very, very carefully before\r\nuse and consider it unstable.\r\n\r\nS3-CSE is enabled in the existing configuration option\r\n\"fs.s3a.server-side-encryption-algorithm\":\r\n\r\nfs.s3a.server-side-encryption-algorithm=CSE-KMS\r\nfs.s3a.server-side-encryption.key=<KMS_KEY_ID>\r\n\r\nYou cannot enable CSE and SSE in the same client, although\r\nyou can still enable a default SSE option in the S3 console. \r\n  \r\n* Filesystem list/get status operations subtract 16 bytes from the length\r\n  of all files >= 16 bytes long to compensate for the padding which CSE\r\n  adds.\r\n* The SDK always warns about the specific algorithm chosen being\r\n  deprecated. It is critical to use this algorithm for ranged\r\n  GET requests to work (i.e. random IO). Ignore.\r\n* Unencrypted files CANNOT BE READ.\r\n  The entire bucket SHOULD be encrypted with S3-CSE.\r\n* Uploading files may be a bit slower as blocks are now\r\n  written sequentially.\r\n* The Multipart Upload API is disabled when S3-CSE is active.\r\n\r\nContributed by Mehakmeet Singh",
        "commit_url": "https://github.com/apache/hadoop/commit/f813554769606d59c23bcdc184d52249793d0f12",
        "buggy_code": "if (objectRepresentsDirectory(srcKey, len)) {",
        "fixed_code": "if (objectRepresentsDirectory(srcKey)) {",
        "patch": "@@ -638,7 +638,7 @@ private Path copySourceAndUpdateTracker(\n       copyResult = callbacks.copyFile(srcKey, destinationKey,\n           srcAttributes, readContext);\n     }\n-    if (objectRepresentsDirectory(srcKey, len)) {\n+    if (objectRepresentsDirectory(srcKey)) {\n       renameTracker.directoryMarkerCopied(\n           sourceFile,\n           destination,"
    },
    {
        "commit_id": "f813554769606d59c23bcdc184d52249793d0f12",
        "commit_message": "HADOOP-13887. Support S3 client side encryption (S3-CSE) using AWS-SDK (#2706)\n\n\r\nThis (big!) patch adds support for client side encryption in AWS S3,\r\nwith keys managed by AWS-KMS.\r\n\r\nRead the documentation in encryption.md very, very carefully before\r\nuse and consider it unstable.\r\n\r\nS3-CSE is enabled in the existing configuration option\r\n\"fs.s3a.server-side-encryption-algorithm\":\r\n\r\nfs.s3a.server-side-encryption-algorithm=CSE-KMS\r\nfs.s3a.server-side-encryption.key=<KMS_KEY_ID>\r\n\r\nYou cannot enable CSE and SSE in the same client, although\r\nyou can still enable a default SSE option in the S3 console. \r\n  \r\n* Filesystem list/get status operations subtract 16 bytes from the length\r\n  of all files >= 16 bytes long to compensate for the padding which CSE\r\n  adds.\r\n* The SDK always warns about the specific algorithm chosen being\r\n  deprecated. It is critical to use this algorithm for ranged\r\n  GET requests to work (i.e. random IO). Ignore.\r\n* Unencrypted files CANNOT BE READ.\r\n  The entire bucket SHOULD be encrypted with S3-CSE.\r\n* Uploading files may be a bit slower as blocks are now\r\n  written sequentially.\r\n* The Multipart Upload API is disabled when S3-CSE is active.\r\n\r\nContributed by Mehakmeet Singh",
        "commit_url": "https://github.com/apache/hadoop/commit/f813554769606d59c23bcdc184d52249793d0f12",
        "buggy_code": "fs.getServerSideEncryptionAlgorithm(),",
        "fixed_code": "fs.getS3EncryptionAlgorithm(),",
        "patch": "@@ -95,7 +95,7 @@ private S3AInputStream getMockedS3AInputStream() {\n         fs.getBucket(),\n         path,\n         fs.pathToKey(path),\n-        fs.getServerSideEncryptionAlgorithm(),\n+        fs.getS3EncryptionAlgorithm(),\n         new EncryptionSecrets().getEncryptionKey(),\n         eTag,\n         versionId,"
    },
    {
        "commit_id": "997d749f8a78a0d86eef26ed722ad80b4e8515ea",
        "commit_message": "HADOOP-17801. No error message reported when bucket doesn't exist in S3AFS (#3202)\n\n\r\n\r\nContributed by: Mehakmeet Singh.",
        "commit_url": "https://github.com/apache/hadoop/commit/997d749f8a78a0d86eef26ed722ad80b4e8515ea",
        "buggy_code": "ioe = new UnknownStoreException(path, ase);",
        "fixed_code": "ioe = new UnknownStoreException(path, message, ase);",
        "patch": "@@ -254,7 +254,7 @@ public static IOException translateException(@Nullable String operation,\n       case 404:\n         if (isUnknownBucket(ase)) {\n           // this is a missing bucket\n-          ioe = new UnknownStoreException(path, ase);\n+          ioe = new UnknownStoreException(path, message, ase);\n         } else {\n           // a normal unknown object\n           ioe = new FileNotFoundException(message);"
    },
    {
        "commit_id": "fef53aacc9b5144b1c43762c853b33dd473f75c3",
        "commit_message": "HDFS-16122. Fix DistCpContext#toString() (#3191). Contributed by  tomscut.\n\nSigned-off-by: Ayush Saxena <ayushsaxena@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/fef53aacc9b5144b1c43762c853b33dd473f75c3",
        "buggy_code": "\", preserveRawXattrs\" + preserveRawXattrs;",
        "fixed_code": "\", preserveRawXattrs=\" + preserveRawXattrs;",
        "patch": "@@ -204,7 +204,7 @@ public String toString() {\n     return options.toString() +\n         \", sourcePaths=\" + sourcePaths +\n         \", targetPathExists=\" + targetPathExists +\n-        \", preserveRawXattrs\" + preserveRawXattrs;\n+        \", preserveRawXattrs=\" + preserveRawXattrs;\n   }\n \n }"
    },
    {
        "commit_id": "b6c06c4b76ca5c206639e2ddea4f3bef393dbb11",
        "commit_message": "HDFS-15796. ConcurrentModificationException error happens on NameNode occasionally. Contributed by Daniel Ma",
        "commit_url": "https://github.com/apache/hadoop/commit/b6c06c4b76ca5c206639e2ddea4f3bef393dbb11",
        "buggy_code": "return found.targets;",
        "fixed_code": "return new ArrayList<>(found.targets);",
        "patch": "@@ -333,7 +333,7 @@ List<DatanodeStorageInfo> getTargets(BlockInfo block) {\n     synchronized (pendingReconstructions) {\n       PendingBlockInfo found = pendingReconstructions.get(block);\n       if (found != null) {\n-        return found.targets;\n+        return new ArrayList<>(found.targets);\n       }\n     }\n     return null;"
    },
    {
        "commit_id": "b87bac13e43c7a0b4f16002f5aff9d8c596413a9",
        "commit_message": "HDFS-16109. Fix flaky some unit tests since they offen timeout (#3172)\n\nReviewed-by: Ayush Saxena <ayushsaxena@apache.org>\r\nSigned-off-by: Akira Ajisaka <aajisaka@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/b87bac13e43c7a0b4f16002f5aff9d8c596413a9",
        "buggy_code": "@Test(timeout=180000)",
        "fixed_code": "@Test(timeout=360000)",
        "patch": "@@ -877,7 +877,7 @@ public void run() {\n    * 2. close file with decommissioning\n    * @throws Exception\n    */\n-  @Test(timeout=180000)\n+  @Test(timeout=360000)\n   public void testDecommissionWithCloseFileAndListOpenFiles()\n       throws Exception {\n     LOG.info(\"Starting test testDecommissionWithCloseFileAndListOpenFiles\");"
    },
    {
        "commit_id": "b87bac13e43c7a0b4f16002f5aff9d8c596413a9",
        "commit_message": "HDFS-16109. Fix flaky some unit tests since they offen timeout (#3172)\n\nReviewed-by: Ayush Saxena <ayushsaxena@apache.org>\r\nSigned-off-by: Akira Ajisaka <aajisaka@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/b87bac13e43c7a0b4f16002f5aff9d8c596413a9",
        "buggy_code": "@Test(timeout = 60000)",
        "fixed_code": "@Test(timeout = 300000)",
        "patch": "@@ -358,7 +358,7 @@ public void testDfsReservedPercentageForDifferentStorageTypes()\n     assertEquals(200, volume5.getAvailable());\n   }\n \n-  @Test(timeout = 60000)\n+  @Test(timeout = 300000)\n   public void testAddRplicaProcessorForAddingReplicaInMap() throws Exception {\n     BlockPoolSlice.reInitializeAddReplicaThreadPool();\n     Configuration cnf = new Configuration();"
    },
    {
        "commit_id": "b87bac13e43c7a0b4f16002f5aff9d8c596413a9",
        "commit_message": "HDFS-16109. Fix flaky some unit tests since they offen timeout (#3172)\n\nReviewed-by: Ayush Saxena <ayushsaxena@apache.org>\r\nSigned-off-by: Akira Ajisaka <aajisaka@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/b87bac13e43c7a0b4f16002f5aff9d8c596413a9",
        "buggy_code": "@Test(timeout=30000)",
        "fixed_code": "@Test(timeout=180000)",
        "patch": "@@ -241,7 +241,7 @@ public void testOtherNodeNotActive() throws Exception {\n    * {@link DFSConfigKeys#DFS_IMAGE_TRANSFER_BOOTSTRAP_STANDBY_RATE_KEY}\n    * created by HDFS-8808.\n    */\n-  @Test(timeout=30000)\n+  @Test(timeout=180000)\n   public void testRateThrottling() throws Exception {\n     cluster.getConfiguration(0).setLong(\n         DFSConfigKeys.DFS_IMAGE_TRANSFER_RATE_KEY, 1);"
    },
    {
        "commit_id": "390f8603d3a54ffae743fe240b9b0195bd01de06",
        "commit_message": "HDFS-16106. Fix flaky unit test TestDFSShell (#3168)",
        "commit_url": "https://github.com/apache/hadoop/commit/390f8603d3a54ffae743fe240b9b0195bd01de06",
        "buggy_code": "conf.setLong(DFSConfigKeys.DFS_NAMENODE_ACCESSTIME_PRECISION_KEY, 1000);",
        "fixed_code": "conf.setLong(DFSConfigKeys.DFS_NAMENODE_ACCESSTIME_PRECISION_KEY, 120000);",
        "patch": "@@ -117,7 +117,7 @@ public static void setup() throws IOException {\n         GenericTestUtils.getTestDir(\"TestDFSShell\").getAbsolutePath());\n     conf.setBoolean(DFSConfigKeys.DFS_NAMENODE_XATTRS_ENABLED_KEY, true);\n     conf.setBoolean(DFSConfigKeys.DFS_NAMENODE_ACLS_ENABLED_KEY, true);\n-    conf.setLong(DFSConfigKeys.DFS_NAMENODE_ACCESSTIME_PRECISION_KEY, 1000);\n+    conf.setLong(DFSConfigKeys.DFS_NAMENODE_ACCESSTIME_PRECISION_KEY, 120000);\n \n     miniCluster = new MiniDFSCluster.Builder(conf).numDataNodes(2).build();\n     miniCluster.waitActive();"
    },
    {
        "commit_id": "748570b73c86ff02f1c056b988717ff0e1f2aee5",
        "commit_message": "Revert \"HDFS-16044. Fix getListing call getLocatedBlocks even source is a directory. Contributed by ludun.\"\n\nThis reverts commit 0d078377120da9ea886bd95b19c8a618dc4d7ab5.",
        "commit_url": "https://github.com/apache/hadoop/commit/748570b73c86ff02f1c056b988717ff0e1f2aee5",
        "buggy_code": "if (null == locations && isdir && null == symlink) {",
        "fixed_code": "if (null == locations && !isdir && null == symlink) {",
        "patch": "@@ -274,7 +274,7 @@ public Builder locations(LocatedBlocks locations) {\n      * @return An {@link HdfsFileStatus} instance from these parameters.\n      */\n     public HdfsFileStatus build() {\n-      if (null == locations && isdir && null == symlink) {\n+      if (null == locations && !isdir && null == symlink) {\n         return new HdfsNamedFileStatus(length, isdir, replication, blocksize,\n             mtime, atime, permission, flags, owner, group, symlink, path,\n             fileId, childrenNum, feInfo, storagePolicy, ecPolicy);"
    },
    {
        "commit_id": "0d078377120da9ea886bd95b19c8a618dc4d7ab5",
        "commit_message": "HDFS-16044. Fix getListing call getLocatedBlocks even source is a directory. Contributed by ludun.",
        "commit_url": "https://github.com/apache/hadoop/commit/0d078377120da9ea886bd95b19c8a618dc4d7ab5",
        "buggy_code": "if (null == locations && !isdir && null == symlink) {",
        "fixed_code": "if (null == locations && isdir && null == symlink) {",
        "patch": "@@ -274,7 +274,7 @@ public Builder locations(LocatedBlocks locations) {\n      * @return An {@link HdfsFileStatus} instance from these parameters.\n      */\n     public HdfsFileStatus build() {\n-      if (null == locations && !isdir && null == symlink) {\n+      if (null == locations && isdir && null == symlink) {\n         return new HdfsNamedFileStatus(length, isdir, replication, blocksize,\n             mtime, atime, permission, flags, owner, group, symlink, path,\n             fileId, childrenNum, feInfo, storagePolicy, ecPolicy);"
    },
    {
        "commit_id": "35ca1dcb9d9b14e31ad5d0e327a556cc0529f4ce",
        "commit_message": "HADOOP-17685. Fix junit deprecation warnings in hadoop-common module. (#2983)\n\nSigned-off-by: Takanobu Asanuma <tasanuma@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/35ca1dcb9d9b14e31ad5d0e327a556cc0529f4ce",
        "buggy_code": "import org.junit.internal.AssumptionViolatedException;",
        "fixed_code": "import org.junit.AssumptionViolatedException;",
        "patch": "@@ -17,7 +17,7 @@\n  */\n package org.apache.hadoop.test;\n \n-import org.junit.internal.AssumptionViolatedException;\n+import org.junit.AssumptionViolatedException;\n \n /**\n  * JUnit assumptions for the environment (OS)."
    },
    {
        "commit_id": "207210263a27784bf3b61771d2f8364bda7bbb50",
        "commit_message": "HADOOP-17375. Fix the error of TestDynamometerInfra. (#2471)\n\nSigned-off-by: Akira Ajisaka <aajisaka@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/207210263a27784bf3b61771d2f8364bda7bbb50",
        "buggy_code": "private static final String HADOOP_BIN_VERSION_DEFAULT = \"3.1.3\";",
        "fixed_code": "private static final String HADOOP_BIN_VERSION_DEFAULT = \"3.1.4\";",
        "patch": "@@ -122,7 +122,7 @@ public class TestDynamometerInfra {\n   private static final String HADOOP_BIN_PATH_KEY = \"dyno.hadoop.bin.path\";\n   private static final String HADOOP_BIN_VERSION_KEY =\n       \"dyno.hadoop.bin.version\";\n-  private static final String HADOOP_BIN_VERSION_DEFAULT = \"3.1.3\";\n+  private static final String HADOOP_BIN_VERSION_DEFAULT = \"3.1.4\";\n   private static final String FSIMAGE_FILENAME = \"fsimage_0000000000000061740\";\n   private static final String VERSION_FILENAME = \"VERSION\";\n "
    },
    {
        "commit_id": "4f2873801073dc44a5d35dd6a33451c5c9a6cb7e",
        "commit_message": "HDFS-15931 : Fix non-static inner classes for better memory management (#2830). Contributed by Viraj Jasani\n\nSigned-off-by: Mingliang Liu <liuml07@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/4f2873801073dc44a5d35dd6a33451c5c9a6cb7e",
        "buggy_code": "public class RouterContext {",
        "fixed_code": "public static class RouterContext {",
        "patch": "@@ -152,7 +152,7 @@ public class MiniRouterDFSCluster {\n   /**\n    * Router context.\n    */\n-  public class RouterContext {\n+  public static class RouterContext {\n     private Router router;\n     private FileContext fileContext;\n     private String nameserviceId;"
    },
    {
        "commit_id": "4f2873801073dc44a5d35dd6a33451c5c9a6cb7e",
        "commit_message": "HDFS-15931 : Fix non-static inner classes for better memory management (#2830). Contributed by Viraj Jasani\n\nSigned-off-by: Mingliang Liu <liuml07@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/4f2873801073dc44a5d35dd6a33451c5c9a6cb7e",
        "buggy_code": "class LevelDbWriter extends BlockAliasMap.Writer<FileRegion> {",
        "fixed_code": "static class LevelDbWriter extends BlockAliasMap.Writer<FileRegion> {",
        "patch": "@@ -129,7 +129,7 @@ public Iterator<FileRegion> iterator() {\n     }\n   }\n \n-  class LevelDbWriter extends BlockAliasMap.Writer<FileRegion> {\n+  static class LevelDbWriter extends BlockAliasMap.Writer<FileRegion> {\n \n     private InMemoryAliasMapProtocol aliasMap;\n "
    },
    {
        "commit_id": "4f2873801073dc44a5d35dd6a33451c5c9a6cb7e",
        "commit_message": "HDFS-15931 : Fix non-static inner classes for better memory management (#2830). Contributed by Viraj Jasani\n\nSigned-off-by: Mingliang Liu <liuml07@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/4f2873801073dc44a5d35dd6a33451c5c9a6cb7e",
        "buggy_code": "private class ZoneTraverseInfo extends TraverseInfo {",
        "fixed_code": "private static class ZoneTraverseInfo extends TraverseInfo {",
        "patch": "@@ -835,7 +835,7 @@ protected void readUnlock() {\n     }\n   }\n \n-  private class ZoneTraverseInfo extends TraverseInfo {\n+  private static class ZoneTraverseInfo extends TraverseInfo {\n     private String ezKeyVerName;\n \n     ZoneTraverseInfo(String ezKeyVerName) {"
    },
    {
        "commit_id": "4f2873801073dc44a5d35dd6a33451c5c9a6cb7e",
        "commit_message": "HDFS-15931 : Fix non-static inner classes for better memory management (#2830). Contributed by Viraj Jasani\n\nSigned-off-by: Mingliang Liu <liuml07@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/4f2873801073dc44a5d35dd6a33451c5c9a6cb7e",
        "buggy_code": "public class DataNodeProperties {",
        "fixed_code": "public static class DataNodeProperties {",
        "patch": "@@ -599,7 +599,7 @@ protected MiniDFSCluster(Builder builder) throws IOException {\n                        builder.useConfiguredTopologyMappingClass);\n   }\n   \n-  public class DataNodeProperties {\n+  public static class DataNodeProperties {\n     final DataNode datanode;\n     final Configuration conf;\n     String[] dnArgs;"
    },
    {
        "commit_id": "6577bf1891b11c9271d73491b311059677dfb376",
        "commit_message": "YARN-10439. addendum fix for shaded guva.",
        "commit_url": "https://github.com/apache/hadoop/commit/6577bf1891b11c9271d73491b311059677dfb376",
        "buggy_code": "import com.google.common.annotations.VisibleForTesting;",
        "fixed_code": "import org.apache.hadoop.thirdparty.com.google.common.annotations.VisibleForTesting;",
        "patch": "@@ -18,7 +18,7 @@\n \n package org.apache.hadoop.yarn.service;\n \n-import com.google.common.annotations.VisibleForTesting;\n+import org.apache.hadoop.thirdparty.com.google.common.annotations.VisibleForTesting;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.CommonConfigurationKeysPublic;\n import org.apache.hadoop.ipc.Server;"
    },
    {
        "commit_id": "73394fabc7a6e4b3cfb28b13dedc3433f2e6cc49",
        "commit_message": "YARN-10686. Fix TestCapacitySchedulerAutoQueueCreation#testAutoQueueCreationFailsForEmptyPathWithAQCAndWeightMode. Contributed by Qi Zhu.",
        "commit_url": "https://github.com/apache/hadoop/commit/73394fabc7a6e4b3cfb28b13dedc3433f2e6cc49",
        "buggy_code": "appId, \"user\", \"root.\");",
        "fixed_code": "appId, \"user\", USER0);",
        "patch": "@@ -571,7 +571,7 @@ public void testAutoQueueCreationFailsForEmptyPathWithAQCAndWeightMode()\n \n       ApplicationId appId = BuilderUtils.newApplicationId(1, 1);\n       SchedulerEvent addAppEvent = new AppAddedSchedulerEvent(\n-          appId, \"user\", \"root.\");\n+          appId, \"user\", USER0);\n       newCS.handle(addAppEvent);\n \n       RMAppEvent event = new RMAppEvent(appId, RMAppEventType.APP_REJECTED,"
    },
    {
        "commit_id": "e9c98548e9e79ebcc627d5ca1797063e134bfeb7",
        "commit_message": "YARN-10689. Fix the finding bugs in extractFloatValueFromWeightConfig. (#2760)\n\nSigned-off-by: Akira Ajisaka <aajisaka@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/e9c98548e9e79ebcc627d5ca1797063e134bfeb7",
        "buggy_code": "return Float.valueOf(",
        "fixed_code": "return Float.parseFloat(",
        "patch": "@@ -775,7 +775,7 @@ private float extractFloatValueFromWeightConfig(String configureValue) {\n     if (!configuredWeightAsCapacity(configureValue)) {\n       return -1f;\n     } else {\n-      return Float.valueOf(\n+      return Float.parseFloat(\n           configureValue.substring(0, configureValue.indexOf(WEIGHT_SUFFIX)));\n     }\n   }"
    },
    {
        "commit_id": "970455c917c7c78838d932ab1ecd4fdce38ae679",
        "commit_message": "HDFS-15816. Fix shouldAvoidStaleDataNodesForWrite returns when no stale node in cluster. Contributed by Yang Yun.",
        "commit_url": "https://github.com/apache/hadoop/commit/970455c917c7c78838d932ab1ecd4fdce38ae679",
        "buggy_code": "return avoidStaleDataNodesForWrite &&",
        "fixed_code": "return avoidStaleDataNodesForWrite && numStaleNodes > 0 &&",
        "patch": "@@ -1321,7 +1321,7 @@ public List<DatanodeDescriptor> getEnteringMaintenanceNodes() {\n   public boolean shouldAvoidStaleDataNodesForWrite() {\n     // If # stale exceeds maximum staleness ratio, disable stale\n     // datanode avoidance on the write path\n-    return avoidStaleDataNodesForWrite &&\n+    return avoidStaleDataNodesForWrite && numStaleNodes > 0 &&\n         (numStaleNodes <= heartbeatManager.getLiveDatanodeCount()\n             * ratioUseStaleDataNodesForWrite);\n   }"
    },
    {
        "commit_id": "b2a565629dba125be5b330e84c313ba26b50e80f",
        "commit_message": "YARN-10671.Fix Typo in TestSchedulingRequestContainerAllocation. Contributed by  D M Murali Krishna Reddy.",
        "commit_url": "https://github.com/apache/hadoop/commit/b2a565629dba125be5b330e84c313ba26b50e80f",
        "buggy_code": "MockNM nm1 = rm.registerNode(\"192.168.0.1:1234:\", 100*GB, 100);",
        "fixed_code": "MockNM nm1 = rm.registerNode(\"192.168.0.1:1234\", 100*GB, 100);",
        "patch": "@@ -862,7 +862,7 @@ public void testInterAppConstraintsWithNamespaces() throws Exception {\n     try {\n       rm.start();\n \n-      MockNM nm1 = rm.registerNode(\"192.168.0.1:1234:\", 100*GB, 100);\n+      MockNM nm1 = rm.registerNode(\"192.168.0.1:1234\", 100*GB, 100);\n       MockNM nm2 = rm.registerNode(\"192.168.0.2:1234\", 100*GB, 100);\n       MockNM nm3 = rm.registerNode(\"192.168.0.3:1234\", 100*GB, 100);\n       MockNM nm4 = rm.registerNode(\"192.168.0.4:1234\", 100*GB, 100);"
    },
    {
        "commit_id": "099f58f8f41d6439643e625794c66ad932bae17b",
        "commit_message": "YARN-10681. Fix assertion failure message in BaseSLSRunnerTest. Contributed by Szilard Nemeth.",
        "commit_url": "https://github.com/apache/hadoop/commit/099f58f8f41d6439643e625794c66ad932bae17b",
        "buggy_code": "Assert.fail(\"TestSLSRunner catched exception from child thread \"",
        "fixed_code": "Assert.fail(\"TestSLSRunner caught exception from child thread \"",
        "patch": "@@ -126,7 +126,7 @@ public void uncaughtException(Thread t, Throwable e) {\n \n       if (!exceptionList.isEmpty()) {\n         sls.stop();\n-        Assert.fail(\"TestSLSRunner catched exception from child thread \"\n+        Assert.fail(\"TestSLSRunner caught exception from child thread \"\n             + \"(TaskRunner.TaskDefinition): \" + exceptionList);\n         break;\n       }"
    },
    {
        "commit_id": "8af56de1fa754616a71bb2b22a67e0d71bf3a995",
        "commit_message": "HADOOP-17560. Fix some spelling errors (#2730)\n\nCo-authored-by: jiaguodong5 <jiaguodong5@jd.com>",
        "commit_url": "https://github.com/apache/hadoop/commit/8af56de1fa754616a71bb2b22a67e0d71bf3a995",
        "buggy_code": "MountTable.ERROR_MSG_INVAILD_DEST_NS, e);",
        "fixed_code": "MountTable.ERROR_MSG_INVALID_DEST_NS, e);",
        "patch": "@@ -266,7 +266,7 @@ public void testValidation() throws IOException {\n       fail(\"Mount table entry should be created failed.\");\n     } catch (Exception e) {\n       GenericTestUtils.assertExceptionContains(\n-          MountTable.ERROR_MSG_INVAILD_DEST_NS, e);\n+          MountTable.ERROR_MSG_INVALID_DEST_NS, e);\n     }\n \n     destinations.clear();"
    },
    {
        "commit_id": "8af56de1fa754616a71bb2b22a67e0d71bf3a995",
        "commit_message": "HADOOP-17560. Fix some spelling errors (#2730)\n\nCo-authored-by: jiaguodong5 <jiaguodong5@jd.com>",
        "commit_url": "https://github.com/apache/hadoop/commit/8af56de1fa754616a71bb2b22a67e0d71bf3a995",
        "buggy_code": "public void testAppsQueryWithInvaildDeselects()",
        "fixed_code": "public void testAppsQueryWithInvalidDeselects()",
        "patch": "@@ -1201,7 +1201,7 @@ public void testAppsQueryAppTypes() throws JSONException, Exception {\n   }\n \n   @Test\n-  public void testAppsQueryWithInvaildDeselects()\n+  public void testAppsQueryWithInvalidDeselects()\n       throws JSONException, Exception {\n     try {\n       rm.start();"
    },
    {
        "commit_id": "5e719bf5886a001036c099a65638a6592b1b9b01",
        "commit_message": "YARN-10611. Fix that shaded should be used for google guava imports in YARN-10352. Contributed by Qi Zhu",
        "commit_url": "https://github.com/apache/hadoop/commit/5e719bf5886a001036c099a65638a6592b1b9b01",
        "buggy_code": "import com.google.common.collect.Iterators;",
        "fixed_code": "import org.apache.hadoop.thirdparty.com.google.common.collect.Iterators;",
        "patch": "@@ -25,7 +25,7 @@\n import java.util.Set;\n import java.util.concurrent.atomic.AtomicBoolean;\n \n-import com.google.common.collect.Iterators;\n+import org.apache.hadoop.thirdparty.com.google.common.collect.Iterators;\n \n import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp;"
    },
    {
        "commit_id": "42eb9ff68e3786dce44a89e78d9a5dc3603ec2fc",
        "commit_message": "HADOOP-17454. [s3a] Disable bucket existence check - set fs.s3a.bucket.probe to 0 (#2593)\n\nAlso fixes HADOOP-16995. ITestS3AConfiguration proxy tests failures when bucket probes == 0\r\nThe improvement should include the fix, ebcause the test would fail by default otherwise.\r\n\r\nChange-Id: I9a7e4b5e6d4391ebba096c15e84461c038a2ec59",
        "commit_url": "https://github.com/apache/hadoop/commit/42eb9ff68e3786dce44a89e78d9a5dc3603ec2fc",
        "buggy_code": "public static final int S3A_BUCKET_PROBE_DEFAULT = 2;",
        "fixed_code": "public static final int S3A_BUCKET_PROBE_DEFAULT = 0;",
        "patch": "@@ -499,7 +499,7 @@ private Constants() {\n    * will be validated using {@code S3AFileSystem.verifyBucketExistsV2()}.\n    * Value: {@value}\n    */\n-  public static final int S3A_BUCKET_PROBE_DEFAULT = 2;\n+  public static final int S3A_BUCKET_PROBE_DEFAULT = 0;\n \n   /**\n    * How long a directory listing in the MS is considered as authoritative."
    },
    {
        "commit_id": "717b8350687e0c5b435e954cc7519779b3f96851",
        "commit_message": "HADOOP-17397: ABFS: SAS Test updates for version and permission update\n\nDETAILS:\n\n    The previous commit for HADOOP-17397 was not the correct fix.  DelegationSASGenerator.getDelegationSAS\n    should return sp=p for the set-permission and set-acl operations.  The tests have also been updated as\n    follows:\n\n    1. When saoid and suoid are not specified, skoid must have an RBAC role assignment which grants\n       Microsoft.Storage/storageAccounts/blobServices/containers/blobs/modifyPermissions/action and sp=p\n       to set permissions or set ACL.\n\n    2. When saoid or suiod is specified, same as 1) but furthermore the saoid or suoid must be an owner of\n       the file or directory in order for the operation to succeed.\n\n    3. When saoid or suiod is specified, the ownership check is bypassed by also including 'o' (ownership)\n       in the SAS permission (for example, sp=op).  Note that 'o' grants the saoid or suoid the ability to\n       change the file or directory owner to themself, and they can also change the owning group. Generally\n       speaking, if a trusted authorizer would like to give a user the ability to change the permissions or\n       ACL, then that user should be the file or directory owner.\n\nTEST RESULTS:\n\n    namespace.enabled=true\n    auth.type=SharedKey\n    -------------------\n    $mvn -T 1C -Dparallel-tests=abfs -Dscale -DtestsThreadCount=8 clean verify\n    Tests run: 90, Failures: 0, Errors: 0, Skipped: 0\n    Tests run: 462, Failures: 0, Errors: 0, Skipped: 24\n    Tests run: 208, Failures: 0, Errors: 0, Skipped: 24\n\n    namespace.enabled=true\n    auth.type=OAuth\n    -------------------\n    $mvn -T 1C -Dparallel-tests=abfs -Dscale -DtestsThreadCount=8 clean verify\n    Tests run: 90, Failures: 0, Errors: 0, Skipped: 0\n    Tests run: 462, Failures: 0, Errors: 0, Skipped: 70\n    Tests run: 208, Failures: 0, Errors: 0, Skipped: 141",
        "commit_url": "https://github.com/apache/hadoop/commit/717b8350687e0c5b435e954cc7519779b3f96851",
        "buggy_code": "sp = \"op\";",
        "fixed_code": "sp = \"p\";",
        "patch": "@@ -88,7 +88,7 @@ public String getDelegationSAS(String accountName, String containerName, String\n         break;\n       case SASTokenProvider.SET_ACL_OPERATION:\n       case SASTokenProvider.SET_PERMISSION_OPERATION:\n-        sp = \"op\";\n+        sp = \"p\";\n         break;\n       case SASTokenProvider.SET_OWNER_OPERATION:\n         sp = \"o\";"
    },
    {
        "commit_id": "f83e07a20f8de7f668f3a6dd2e871df71d93316b",
        "commit_message": "HADOOP-17293. S3A to always probe S3 in S3A getFileStatus on non-auth paths\n\nThis reverts changes in HADOOP-13230 to use S3Guard TTL in choosing when\nto issue a HEAD request; fixing tests to compensate.\n\nNew org.apache.hadoop.fs.s3a.performance.OperationCost cost,\nS3GUARD_NONAUTH_FILE_STATUS_PROBE for use in cost tests.\n\nContributed by Steve Loughran.\n\nChange-Id: I418d55d2d2562a48b2a14ec7dee369db49b4e29e",
        "commit_url": "https://github.com/apache/hadoop/commit/f83e07a20f8de7f668f3a6dd2e871df71d93316b",
        "buggy_code": "return authMode ? 0 : 0;",
        "fixed_code": "return authMode ? 0 : 1;",
        "patch": "@@ -326,7 +326,7 @@ protected Path path() throws IOException {\n    * @return a number >= 0.\n    */\n   private int getFileStatusHeadCount() {\n-    return authMode ? 0 : 0;\n+    return authMode ? 0 : 1;\n   }\n \n   /**"
    },
    {
        "commit_id": "4347a5c9556e5399f5df059879749fb9df72e718",
        "commit_message": "HADOOP-17294. Fix typos existance to existence (#2357)",
        "commit_url": "https://github.com/apache/hadoop/commit/4347a5c9556e5399f5df059879749fb9df72e718",
        "buggy_code": "LOG.error(\"Failed checking for the existance of history intermediate \" +",
        "fixed_code": "LOG.error(\"Failed checking for the existence of history intermediate \" +",
        "patch": "@@ -227,7 +227,7 @@ protected void serviceInit(Configuration conf) throws Exception {\n       }\n       }\n     } catch (IOException e) {\n-      LOG.error(\"Failed checking for the existance of history intermediate \" +\n+      LOG.error(\"Failed checking for the existence of history intermediate \" +\n       \t\t\"done directory: [\" + doneDirPath + \"]\");\n       throw new YarnRuntimeException(e);\n     }"
    },
    {
        "commit_id": "d686a655bc36ab68f4dbf66e1c2d0690dd337bd6",
        "commit_message": "HDFS-15603. RBF: Fix getLocationsForPath twice in create operation. Contributed by wangzhaohui.",
        "commit_url": "https://github.com/apache/hadoop/commit/d686a655bc36ab68f4dbf66e1c2d0690dd337bd6",
        "buggy_code": "createLocation = rpcServer.getCreateLocation(src);",
        "fixed_code": "createLocation = rpcServer.getCreateLocation(src, locations);",
        "patch": "@@ -288,7 +288,7 @@ public HdfsFileStatus create(String src, FsPermission masked,\n         rpcServer.getLocationsForPath(src, true);\n     RemoteLocation createLocation = null;\n     try {\n-      createLocation = rpcServer.getCreateLocation(src);\n+      createLocation = rpcServer.getCreateLocation(src, locations);\n       return rpcClient.invokeSingle(createLocation, method,\n           HdfsFileStatus.class);\n     } catch (IOException ioe) {"
    },
    {
        "commit_id": "eae0035a2d747184b51f25ac4c4e674347af28c4",
        "commit_message": "HDFS-15530. RBF: Fix typo in DFS_ROUTER_QUOTA_CACHE_UPDATE_INTERVAL var definition. Contributed by Sha Fanghao.",
        "commit_url": "https://github.com/apache/hadoop/commit/eae0035a2d747184b51f25ac4c4e674347af28c4",
        "buggy_code": "routerConf.set(RBFConfigKeys.DFS_ROUTER_QUOTA_CACHE_UPATE_INTERVAL, \"2s\");",
        "fixed_code": "routerConf.set(RBFConfigKeys.DFS_ROUTER_QUOTA_CACHE_UPDATE_INTERVAL, \"2s\");",
        "patch": "@@ -99,7 +99,7 @@ public void setUp() throws Exception {\n         .quota()\n         .rpc()\n         .build();\n-    routerConf.set(RBFConfigKeys.DFS_ROUTER_QUOTA_CACHE_UPATE_INTERVAL, \"2s\");\n+    routerConf.set(RBFConfigKeys.DFS_ROUTER_QUOTA_CACHE_UPDATE_INTERVAL, \"2s\");\n \n     // override some hdfs settings that used in testing space quota\n     Configuration hdfsConf = new Configuration(false);"
    },
    {
        "commit_id": "9960c01a25c6025e81559a8cf32e9f3cea70d2cc",
        "commit_message": "HADOOP-17244. S3A directory delete tombstones dir markers prematurely. (#2280)\n\n\r\nThis changes directory tree deletion so that only files are incrementally deleted\r\nfrom S3Guard after the objects are deleted; the directories are left alone\r\nuntil metadataStore.deleteSubtree(path) is invoke.\r\n\r\nThis avoids directory tombstones being added above files/child directories,\r\nwhich stop the treewalk and delete phase from working.\r\n\r\nAlso:\r\n\r\n* Callback to delete objects splits files and dirs so that\r\nany problems deleting the dirs doesn't trigger s3guard updates\r\n* New statistic to measure #of objects deleted, alongside request count.\r\n* Callback listFilesAndEmptyDirectories renamed listFilesAndDirectoryMarkers\r\n  to clarify behavior.\r\n* Test enhancements to replicate the failure and verify the fix\r\n\r\nContributed by Steve Loughran",
        "commit_url": "https://github.com/apache/hadoop/commit/9960c01a25c6025e81559a8cf32e9f3cea70d2cc",
        "buggy_code": "LOG.debug(\"Get from table {} in region {}: {}. wantEmptyDirectory={}\",",
        "fixed_code": "LOG.debug(\"Get from table {} in region {}: {} ; wantEmptyDirectory={}\",",
        "patch": "@@ -717,7 +717,7 @@ public DDBPathMetadata get(Path path) throws IOException {\n   public DDBPathMetadata get(Path path, boolean wantEmptyDirectoryFlag)\n       throws IOException {\n     checkPath(path);\n-    LOG.debug(\"Get from table {} in region {}: {}. wantEmptyDirectory={}\",\n+    LOG.debug(\"Get from table {} in region {}: {} ; wantEmptyDirectory={}\",\n         tableName, region, path, wantEmptyDirectoryFlag);\n     DDBPathMetadata result = innerGet(path, wantEmptyDirectoryFlag);\n     LOG.debug(\"result of get {} is: {}\", path, result);"
    },
    {
        "commit_id": "9960c01a25c6025e81559a8cf32e9f3cea70d2cc",
        "commit_message": "HADOOP-17244. S3A directory delete tombstones dir markers prematurely. (#2280)\n\n\r\nThis changes directory tree deletion so that only files are incrementally deleted\r\nfrom S3Guard after the objects are deleted; the directories are left alone\r\nuntil metadataStore.deleteSubtree(path) is invoke.\r\n\r\nThis avoids directory tombstones being added above files/child directories,\r\nwhich stop the treewalk and delete phase from working.\r\n\r\nAlso:\r\n\r\n* Callback to delete objects splits files and dirs so that\r\nany problems deleting the dirs doesn't trigger s3guard updates\r\n* New statistic to measure #of objects deleted, alongside request count.\r\n* Callback listFilesAndEmptyDirectories renamed listFilesAndDirectoryMarkers\r\n  to clarify behavior.\r\n* Test enhancements to replicate the failure and verify the fix\r\n\r\nContributed by Steve Loughran",
        "commit_url": "https://github.com/apache/hadoop/commit/9960c01a25c6025e81559a8cf32e9f3cea70d2cc",
        "buggy_code": "public RemoteIterator<S3ALocatedFileStatus> listFilesAndEmptyDirectories(",
        "fixed_code": "public RemoteIterator<S3ALocatedFileStatus> listFilesAndDirectoryMarkers(",
        "patch": "@@ -333,7 +333,7 @@ public void deleteObjectAtPath(\n     }\n \n     @Override\n-    public RemoteIterator<S3ALocatedFileStatus> listFilesAndEmptyDirectories(\n+    public RemoteIterator<S3ALocatedFileStatus> listFilesAndDirectoryMarkers(\n             Path path,\n             S3AFileStatus status,\n             boolean collectTombstones,"
    },
    {
        "commit_id": "1b29c9bfeee0035dd042357038b963843169d44c",
        "commit_message": "HADOOP-17138. Fix spotbugs warnings surfaced after upgrade to 4.0.6. (#2155)",
        "commit_url": "https://github.com/apache/hadoop/commit/1b29c9bfeee0035dd042357038b963843169d44c",
        "buggy_code": "public void onSuccess(@Nullable V result) {",
        "fixed_code": "public void onSuccess(V result) {",
        "patch": "@@ -166,7 +166,7 @@ private void addResultCachingCallback(\n       Checkable<K, V> target, ListenableFuture<V> lf) {\n     Futures.addCallback(lf, new FutureCallback<V>() {\n       @Override\n-      public void onSuccess(@Nullable V result) {\n+      public void onSuccess(V result) {\n         synchronized (ThrottledAsyncChecker.this) {\n           checksInProgress.remove(target);\n           completedChecks.put(target, new LastCheckResult<>("
    },
    {
        "commit_id": "1b29c9bfeee0035dd042357038b963843169d44c",
        "commit_message": "HADOOP-17138. Fix spotbugs warnings surfaced after upgrade to 4.0.6. (#2155)",
        "commit_url": "https://github.com/apache/hadoop/commit/1b29c9bfeee0035dd042357038b963843169d44c",
        "buggy_code": "holder.held++;",
        "fixed_code": "holder.held = holder.held + 1;",
        "patch": "@@ -1238,7 +1238,7 @@ private void incrOpCount(FSEditLogOpCodes opCode,\n       holder = new Holder<Integer>(1);\n       opCounts.put(opCode, holder);\n     } else {\n-      holder.held++;\n+      holder.held = holder.held + 1;\n     }\n     counter.increment();\n   }"
    },
    {
        "commit_id": "1b29c9bfeee0035dd042357038b963843169d44c",
        "commit_message": "HADOOP-17138. Fix spotbugs warnings surfaced after upgrade to 4.0.6. (#2155)",
        "commit_url": "https://github.com/apache/hadoop/commit/1b29c9bfeee0035dd042357038b963843169d44c",
        "buggy_code": "appNum++;",
        "fixed_code": "appNum = appNum + 1;",
        "patch": "@@ -813,7 +813,7 @@ private void increaseQueueAppNum(String queue) throws YarnException {\n     if (appNum == null) {\n       appNum = 1;\n     } else {\n-      appNum++;\n+      appNum = appNum + 1;\n     }\n \n     queueAppNumMap.put(queueName, appNum);"
    },
    {
        "commit_id": "bed0a3a37404e9defda13a5bffe5609e72466e46",
        "commit_message": "HDFS-15436. Default mount table name used by ViewFileSystem should be configurable (#2100)\n\n* HDFS-15436. Default mount table name used by ViewFileSystem should be configurable\r\n\r\n* Replace Constants.CONFIG_VIEWFS_DEFAULT_MOUNT_TABLE use in tests\r\n\r\n* Address Uma's comments on PR#2100\r\n\r\n* Sort lists in test to match without concern to order\r\n\r\n* Address comments, fix checkstyle and fix failing tests\r\n\r\n* Fix checkstyle",
        "commit_url": "https://github.com/apache/hadoop/commit/bed0a3a37404e9defda13a5bffe5609e72466e46",
        "buggy_code": "mountTableName = Constants.CONFIG_VIEWFS_DEFAULT_MOUNT_TABLE;",
        "fixed_code": "mountTableName = ConfigUtil.getDefaultMountTableName(config);",
        "patch": "@@ -465,7 +465,7 @@ protected InodeTree(final Configuration config, final String viewName)\n       FileAlreadyExistsException, IOException {\n     String mountTableName = viewName;\n     if (mountTableName == null) {\n-      mountTableName = Constants.CONFIG_VIEWFS_DEFAULT_MOUNT_TABLE;\n+      mountTableName = ConfigUtil.getDefaultMountTableName(config);\n     }\n     homedirPrefix = ConfigUtil.getHomeDirValue(config, mountTableName);\n "
    },
    {
        "commit_id": "bed0a3a37404e9defda13a5bffe5609e72466e46",
        "commit_message": "HDFS-15436. Default mount table name used by ViewFileSystem should be configurable (#2100)\n\n* HDFS-15436. Default mount table name used by ViewFileSystem should be configurable\r\n\r\n* Replace Constants.CONFIG_VIEWFS_DEFAULT_MOUNT_TABLE use in tests\r\n\r\n* Address Uma's comments on PR#2100\r\n\r\n* Sort lists in test to match without concern to order\r\n\r\n* Address comments, fix checkstyle and fix failing tests\r\n\r\n* Fix checkstyle",
        "commit_url": "https://github.com/apache/hadoop/commit/bed0a3a37404e9defda13a5bffe5609e72466e46",
        "buggy_code": "? Constants.CONFIG_VIEWFS_DEFAULT_MOUNT_TABLE",
        "fixed_code": "? ConfigUtil.getDefaultMountTableName(conf)",
        "patch": "@@ -153,7 +153,7 @@ static void addMountLinksToFile(String mountTable, String[] sources,\n         String prefix =\n             new StringBuilder(Constants.CONFIG_VIEWFS_PREFIX).append(\".\")\n                 .append((mountTable == null\n-                    ? Constants.CONFIG_VIEWFS_DEFAULT_MOUNT_TABLE\n+                    ? ConfigUtil.getDefaultMountTableName(conf)\n                     : mountTable))\n                 .append(\".\").toString();\n         out.writeBytes(\"<configuration>\");"
    },
    {
        "commit_id": "6e04b00df1bf4f0a45571c9fc4361e4e8a05f7ee",
        "commit_message": "HDFS-12288. Fix DataNode's xceiver count calculation. Contributed by Lisheng Sun.",
        "commit_url": "https://github.com/apache/hadoop/commit/6e04b00df1bf4f0a45571c9fc4361e4e8a05f7ee",
        "buggy_code": "dn.getXceiverCount(),",
        "fixed_code": "dn.getActiveTransferThreadCount(),",
        "patch": "@@ -544,7 +544,7 @@ HeartbeatResponse sendHeartBeat(boolean requestBlockReportLease)\n         dn.getFSDataset().getCacheCapacity(),\n         dn.getFSDataset().getCacheUsed(),\n         dn.getXmitsInProgress(),\n-        dn.getXceiverCount(),\n+        dn.getActiveTransferThreadCount(),\n         numFailedVolumes,\n         volumeFailureSummary,\n         requestBlockReportLease,"
    },
    {
        "commit_id": "0b855b9f3570f98ff2f2802114241e10520aded8",
        "commit_message": "HDFS-15256. Fix typo in DataXceiverServer#run(). Contributed by Lisheng Sun.",
        "commit_url": "https://github.com/apache/hadoop/commit/0b855b9f3570f98ff2f2802114241e10520aded8",
        "buggy_code": "+ \" exceeds the limit of concurrent xcievers: \"",
        "fixed_code": "+ \" exceeds the limit of concurrent xceivers: \"",
        "patch": "@@ -229,7 +229,7 @@ public void run() {\n         int curXceiverCount = datanode.getXceiverCount();\n         if (curXceiverCount > maxXceiverCount) {\n           throw new IOException(\"Xceiver count \" + curXceiverCount\n-              + \" exceeds the limit of concurrent xcievers: \"\n+              + \" exceeds the limit of concurrent xceivers: \"\n               + maxXceiverCount);\n         }\n "
    },
    {
        "commit_id": "d9c4f1129c0814ab61fce6ea8baf4b272f84c252",
        "commit_message": "HDFS-15219. DFS Client will stuck when ResponseProcessor.run throw Error (#1902). Contributed by  zhengchenyu.",
        "commit_url": "https://github.com/apache/hadoop/commit/d9c4f1129c0814ab61fce6ea8baf4b272f84c252",
        "buggy_code": "} catch (Exception e) {",
        "fixed_code": "} catch (Throwable e) {",
        "patch": "@@ -1184,7 +1184,7 @@ public void run() {\n \n             one.releaseBuffer(byteArrayManager);\n           }\n-        } catch (Exception e) {\n+        } catch (Throwable e) {\n           if (!responderClosed) {\n             lastException.set(e);\n             errorState.setInternalError();"
    },
    {
        "commit_id": "38d87883b6d4fe6a974e99b937b03cab55bc3820",
        "commit_message": "YARN-10193. FS-CS converter: fix incorrect capacity conversion. Contributed by Peter Bacsko",
        "commit_url": "https://github.com/apache/hadoop/commit/38d87883b6d4fe6a974e99b937b03cab55bc3820",
        "buggy_code": "for (int i = 0; i < children.size() - 2; i++) {",
        "fixed_code": "for (int i = 0; i < children.size() - 1; i++) {",
        "patch": "@@ -359,7 +359,7 @@ private Map<String, BigDecimal> getCapacities(int totalWeight,\n       // fix last value if total != 100.000\n       if (!totalPct.equals(hundred)) {\n         BigDecimal tmp = new BigDecimal(0);\n-        for (int i = 0; i < children.size() - 2; i++) {\n+        for (int i = 0; i < children.size() - 1; i++) {\n           tmp = tmp.add(capacities.get(children.get(i).getQueueName()));\n         }\n "
    },
    {
        "commit_id": "38d87883b6d4fe6a974e99b937b03cab55bc3820",
        "commit_message": "YARN-10193. FS-CS converter: fix incorrect capacity conversion. Contributed by Peter Bacsko",
        "commit_url": "https://github.com/apache/hadoop/commit/38d87883b6d4fe6a974e99b937b03cab55bc3820",
        "buggy_code": "assertEquals(\"root.users capacity\", \"66.667\",",
        "fixed_code": "assertEquals(\"root.users capacity\", \"33.334\",",
        "patch": "@@ -296,7 +296,7 @@ public void testChildCapacity() {\n         csConfig.get(PREFIX + \"root.default.capacity\"));\n     assertEquals(\"root.admins capacity\", \"33.333\",\n         csConfig.get(PREFIX + \"root.admins.capacity\"));\n-    assertEquals(\"root.users capacity\", \"66.667\",\n+    assertEquals(\"root.users capacity\", \"33.334\",\n         csConfig.get(PREFIX + \"root.users.capacity\"));\n \n     // root.users"
    },
    {
        "commit_id": "c734d69a55693143d0aba2f7f5a793b11c8c50a5",
        "commit_message": "HADOOP-16898. Batch listing of multiple directories via an (unstable) interface\n\nContributed by Steve Loughran.\n\nThis moves the new API of HDFS-13616 into a interface which is implemented by\nHDFS RPC filesystem client (not WebHDFS or any other connector)\n\nThis new interface, BatchListingOperations, is in hadoop-common,\nso applications do not need to be compiled with HDFS on the classpath.\nThey must cast the FS into the interface.\n\ninstanceof can probe the client for having the new interface -the patch\nalso adds a new path capability to probe for this.\n\nThe FileSystem implementation is cut; tests updated as appropriate.\n\nAll new interfaces/classes/constants are marked as @unstable.\n\nChange-Id: I5623c51f2c75804f58f915dd7e60cb2cffdac681",
        "commit_url": "https://github.com/apache/hadoop/commit/c734d69a55693143d0aba2f7f5a793b11c8c50a5",
        "buggy_code": "@InterfaceStability.Stable",
        "fixed_code": "@InterfaceStability.Unstable",
        "patch": "@@ -35,7 +35,7 @@\n  * {@link #get()} will throw an Exception if there was a failure.\n  */\n @InterfaceAudience.Public\n-@InterfaceStability.Stable\n+@InterfaceStability.Unstable\n public class PartialListing<T extends FileStatus> {\n   private final Path listedPath;\n   private final List<T> partialListing;"
    },
    {
        "commit_id": "d0a7c790c62dbb63b4ce6d5cbe77a33376fa67b0",
        "commit_message": "HADOOP-16885. Fix hadoop-commons TestCopy failure\n\nFollowup to HADOOP-16885: Encryption zone file copy failure leaks a temp file\r\n\r\nMoving the delete() call broke a mocking test, which slipped through the review process.\r\n\r\nContributed by Steve Loughran.\r\n\r\nChange-Id: Ia13faf0f4fffb1c99ddd616d823e4f4d0b7b0cbb",
        "commit_url": "https://github.com/apache/hadoop/commit/d0a7c790c62dbb63b4ce6d5cbe77a33376fa67b0",
        "buggy_code": "verify(mockFs, never()).delete(eq(tmpPath), anyBoolean());",
        "fixed_code": "verify(mockFs).delete(eq(tmpPath), anyBoolean());",
        "patch": "@@ -121,7 +121,7 @@ public void testInterruptedCreate() throws Exception {\n \n     tryCopyStream(in, false);\n     verify(mockFs, never()).rename(any(Path.class), any(Path.class));\n-    verify(mockFs, never()).delete(eq(tmpPath), anyBoolean());\n+    verify(mockFs).delete(eq(tmpPath), anyBoolean());\n     verify(mockFs, never()).delete(eq(path), anyBoolean());\n     verify(mockFs, never()).close();\n   }"
    },
    {
        "commit_id": "e9eecedf6903b7cb40d44474df2b9d5125a3c514",
        "commit_message": "YARN-10148. addendum: Fix method call parameter order of setAdminAndSubmitACL in TestCapacitySchedulerQueueACLs. Contributed by Kinga Marton",
        "commit_url": "https://github.com/apache/hadoop/commit/e9eecedf6903b7cb40d44474df2b9d5125a3c514",
        "buggy_code": "setAdminAndSubmitACL(csConf, d1Path, queueD1Acl);",
        "fixed_code": "setAdminAndSubmitACL(csConf, queueD1Acl, d1Path);",
        "patch": "@@ -126,7 +126,7 @@ public void updateConfigWithDAndD1Queues(String rootAcl, String queueDAcl,\n     }\n \n     if (queueD1Acl != null) {\n-      setAdminAndSubmitACL(csConf, d1Path, queueD1Acl);\n+      setAdminAndSubmitACL(csConf, queueD1Acl, d1Path);\n     }\n     resourceManager.getResourceScheduler()\n         .reinitialize(csConf, resourceManager.getRMContext());"
    },
    {
        "commit_id": "a7d72c523ae9d23fad5f2fcc4b40610731ce454a",
        "commit_message": "YARN-10099. FS-CS converter: handle allow-undeclared-pools and user-as-default-queue properly and fix misc issues. Contributed by Peter Bacsko",
        "commit_url": "https://github.com/apache/hadoop/commit/a7d72c523ae9d23fad5f2fcc4b40610731ce454a",
        "buggy_code": "\"u:%user:%user;u:%user:%primary_group;u:%user:%secondary_group\");",
        "fixed_code": "\"u:%user:%user,u:%user:%primary_group,u:%user:%secondary_group\");",
        "patch": "@@ -224,7 +224,7 @@ public void testConvertMultiplePlacementRules() {\n     Map<String, String> properties = convert(false);\n \n     verifyMapping(properties,\n-        \"u:%user:%user;u:%user:%primary_group;u:%user:%secondary_group\");\n+        \"u:%user:%user,u:%user:%primary_group,u:%user:%secondary_group\");\n     verifyZeroInteractions(ruleHandler);\n   }\n "
    },
    {
        "commit_id": "825db8fe2ab37bd5a9a54485ea9ecbabf3766ed6",
        "commit_message": "YARN-10107. Fix GpuResourcePlugin#getNMResourceInfo to honor Auto Discovery Enabled\n\nContributed by Szilard Nemeth.",
        "commit_url": "https://github.com/apache/hadoop/commit/825db8fe2ab37bd5a9a54485ea9ecbabf3766ed6",
        "buggy_code": "private boolean isAutoDiscoveryEnabled() {",
        "fixed_code": "boolean isAutoDiscoveryEnabled() {",
        "patch": "@@ -136,7 +136,7 @@ public synchronized GpuDeviceInformation getGpuDeviceInformation()\n     return lastDiscoveredGpuInformation;\n   }\n \n-  private boolean isAutoDiscoveryEnabled() {\n+  boolean isAutoDiscoveryEnabled() {\n     String allowedDevicesStr = getConf().get(\n         YarnConfiguration.NM_GPU_ALLOWED_DEVICES,\n         YarnConfiguration.AUTOMATICALLY_DISCOVER_GPU_DEVICES);"
    },
    {
        "commit_id": "7030722e5d9f376245a9ab0a6a883538b6c55f82",
        "commit_message": "HDFS-15080. Fix the issue in reading persistent memory cached data with an offset. Contributed by Feilong He.",
        "commit_url": "https://github.com/apache/hadoop/commit/7030722e5d9f376245a9ab0a6a883538b6c55f82",
        "buggy_code": "addr, info.getBlockDataLength());",
        "fixed_code": "addr + seekOffset, info.getBlockDataLength() - seekOffset);",
        "patch": "@@ -822,7 +822,7 @@ private InputStream getBlockInputStreamWithCheckingPmemCache(\n       if (addr != -1) {\n         LOG.debug(\"Get InputStream by cache address.\");\n         return FsDatasetUtil.getDirectInputStream(\n-            addr, info.getBlockDataLength());\n+            addr + seekOffset, info.getBlockDataLength() - seekOffset);\n       }\n       LOG.debug(\"Get InputStream by cache file path.\");\n       return FsDatasetUtil.getInputStreamAndSeek("
    },
    {
        "commit_id": "d81d45ff2fc9a1c424222e021f9306bf64c916b2",
        "commit_message": "YARN-9956. Improved connection error message for YARN ApiServerClient.\n           Contributed by Prabhu Joseph",
        "commit_url": "https://github.com/apache/hadoop/commit/d81d45ff2fc9a1c424222e021f9306bf64c916b2",
        "buggy_code": "LOG.error(\"Error: {}\", e);",
        "fixed_code": "LOG.error(\"Error: \", e);",
        "patch": "@@ -247,7 +247,7 @@ public String run() throws Exception {\n                   StandardCharsets.US_ASCII);\n             } catch (GSSException | IllegalAccessException\n                 | NoSuchFieldException | ClassNotFoundException e) {\n-              LOG.error(\"Error: {}\", e);\n+              LOG.error(\"Error: \", e);\n               throw new AuthenticationException(e);\n             }\n           }"
    },
    {
        "commit_id": "aa7ab2719f745f6e2a5cfbca713bb49865cf52bd",
        "commit_message": "YARN-9991. Fix Application Tag prefix to userid. Contributed by Szilard Nemeth.",
        "commit_url": "https://github.com/apache/hadoop/commit/aa7ab2719f745f6e2a5cfbca713bb49865cf52bd",
        "buggy_code": "\"application-tag-based-placement\";",
        "fixed_code": "RM_PREFIX + \"application-tag-based-placement\";",
        "patch": "@@ -1860,7 +1860,7 @@ public static boolean isAclEnabled(Configuration conf) {\n   public static final boolean DEFAULT_PROCFS_USE_SMAPS_BASED_RSS_ENABLED =\n       false;\n   private static final String APPLICATION_TAG_BASED_PLACEMENT_PREFIX =\n-          \"application-tag-based-placement\";\n+      RM_PREFIX + \"application-tag-based-placement\";\n   public static final String APPLICATION_TAG_BASED_PLACEMENT_ENABLED =\n           APPLICATION_TAG_BASED_PLACEMENT_PREFIX + \".enable\";\n   public static final boolean DEFAULT_APPLICATION_TAG_BASED_PLACEMENT_ENABLED ="
    },
    {
        "commit_id": "516377bfa6faa21f50b7e7c3889e4196c6d464b8",
        "commit_message": "YARN-9965. Fix NodeManager failing to start when Hdfs Auxillary Jar is set. Contributed by Prabhu Joseph.",
        "commit_url": "https://github.com/apache/hadoop/commit/516377bfa6faa21f50b7e7c3889e4196c6d464b8",
        "buggy_code": "return new Path(targetDirPath + Path.SEPARATOR + \"*\");",
        "fixed_code": "return targetDirPath;",
        "patch": "@@ -351,7 +351,7 @@ private Path maybeDownloadJars(String sName, String className, String\n     FileStatus[] allSubDirs = localLFS.util().listStatus(nmAuxDir);\n     for (FileStatus sub : allSubDirs) {\n       if (sub.getPath().getName().equals(downloadDest.getName())) {\n-        return new Path(targetDirPath + Path.SEPARATOR + \"*\");\n+        return targetDirPath;\n       } else {\n         if (sub.getPath().getName().contains(className) &&\n             !sub.getPath().getName().endsWith(DEL_SUFFIX)) {"
    },
    {
        "commit_id": "b25a37c3229e1a66699d649f6caf80ffc71db5b8",
        "commit_message": "HDFS-14962. RBF: ConnectionPool#newConnection() error log wrong protocol class (#1699). Contributed by  Yuxuan Wang.",
        "commit_url": "https://github.com/apache/hadoop/commit/b25a37c3229e1a66699d649f6caf80ffc71db5b8",
        "buggy_code": "+ ((proto != null) ? proto.getClass().getName() : \"null\");",
        "fixed_code": "+ ((proto != null) ? proto.getName() : \"null\");",
        "patch": "@@ -374,7 +374,7 @@ protected static <T> ConnectionContext newConnection(Configuration conf,\n       throws IOException {\n     if (!PROTO_MAP.containsKey(proto)) {\n       String msg = \"Unsupported protocol for connection to NameNode: \"\n-          + ((proto != null) ? proto.getClass().getName() : \"null\");\n+          + ((proto != null) ? proto.getName() : \"null\");\n       LOG.error(msg);\n       throw new IllegalStateException(msg);\n     }"
    },
    {
        "commit_id": "83d148074f9299de02d5c896a3ed4e11292cba73",
        "commit_message": "YARN-9915: Fix FindBug issue in QueueMetrics. Contributed by Prabhu Joseph.",
        "commit_url": "https://github.com/apache/hadoop/commit/83d148074f9299de02d5c896a3ed4e11292cba73",
        "buggy_code": "customResources.put(resource.getName(), new Long(0));",
        "fixed_code": "customResources.put(resource.getName(), Long.valueOf(0));",
        "patch": "@@ -465,7 +465,7 @@ private void registerCustomResources() {\n       2; i < resources.length; i++) {\n       ResourceInformation resource =\n         resources[i];\n-      customResources.put(resource.getName(), new Long(0));\n+      customResources.put(resource.getName(), Long.valueOf(0));\n     }\n \n     registerCustomResources(customResources, ALLOCATED_RESOURCE_METRIC_PREFIX,"
    },
    {
        "commit_id": "dee9e97075e67f53d033df522372064ca19d6b51",
        "commit_message": "Revert \"HADOOP-15870. S3AInputStream.remainingInFile should use nextReadPos.\"\n\nThis reverts commit 7a4b3d42c4e36e468c2a46fd48036a6fed547853.\n\nThe patch broke TestRouterWebHDFSContractSeek as it turns out that\nWebHDFSInputStream.available() is always 0.",
        "commit_url": "https://github.com/apache/hadoop/commit/dee9e97075e67f53d033df522372064ca19d6b51",
        "buggy_code": "@Parameterized.Parameters(name = \"{0}-{1}\")",
        "fixed_code": "@Parameterized.Parameters",
        "patch": "@@ -80,7 +80,7 @@ public class ITestS3AContractSeek extends AbstractContractSeekTest {\n    * which S3A Supports.\n    * @return a list of seek policies to test.\n    */\n-  @Parameterized.Parameters(name = \"{0}-{1}\")\n+  @Parameterized.Parameters\n   public static Collection<Object[]> params() {\n     return Arrays.asList(new Object[][]{\n         {INPUT_FADV_SEQUENTIAL, Default_JSSE},"
    },
    {
        "commit_id": "5f4641a120331d049a55c519a0d15da18c820fed",
        "commit_message": "HDFS-14238. A log in NNThroughputBenchmark should change log level to INFO instead of ERROR. Contributed by Shen Yinjie.",
        "commit_url": "https://github.com/apache/hadoop/commit/5f4641a120331d049a55c519a0d15da18c820fed",
        "buggy_code": "LOG.error(\"Log level = \" + logLevel.toString());",
        "fixed_code": "LOG.info(\"Log level = \" + logLevel.toString());",
        "patch": "@@ -147,7 +147,7 @@ void close() {\n   }\n \n   static void setNameNodeLoggingLevel(Level logLevel) {\n-    LOG.error(\"Log level = \" + logLevel.toString());\n+    LOG.info(\"Log level = \" + logLevel.toString());\n     // change log level to NameNode logs\n     DFSTestUtil.setNameNodeLogLevel(logLevel);\n     GenericTestUtils.setLogLevel(LogManager.getLogger("
    },
    {
        "commit_id": "8de4374427e77d5d9b79a710ca9225f749556eda",
        "commit_message": "HDDS-2158. Fixing Json Injection Issue in JsonUtils. (#1486)",
        "commit_url": "https://github.com/apache/hadoop/commit/8de4374427e77d5d9b79a710ca9225f749556eda",
        "buggy_code": "WRITER = mapper.writer();",
        "fixed_code": "WRITER = mapper.writerWithDefaultPrettyPrinter();",
        "patch": "@@ -54,7 +54,7 @@ public class ContainerInfo implements Comparator<ContainerInfo>,\n     mapper.setVisibility(PropertyAccessor.FIELD, JsonAutoDetect.Visibility.ANY);\n     mapper\n         .setVisibility(PropertyAccessor.GETTER, JsonAutoDetect.Visibility.NONE);\n-    WRITER = mapper.writer();\n+    WRITER = mapper.writerWithDefaultPrettyPrinter();\n   }\n \n   private HddsProtos.LifeCycleState state;"
    },
    {
        "commit_id": "8de4374427e77d5d9b79a710ca9225f749556eda",
        "commit_message": "HDDS-2158. Fixing Json Injection Issue in JsonUtils. (#1486)",
        "commit_url": "https://github.com/apache/hadoop/commit/8de4374427e77d5d9b79a710ca9225f749556eda",
        "buggy_code": "JsonUtils.toJsonString(token.encodeToUrlString())));",
        "fixed_code": "token.encodeToUrlString()));",
        "patch": "@@ -71,7 +71,7 @@ public Void call() throws Exception {\n     }\n \n     System.out.printf(\"%s\", JsonUtils.toJsonStringWithDefaultPrettyPrinter(\n-        JsonUtils.toJsonString(token.encodeToUrlString())));\n+        token.encodeToUrlString()));\n     return null;\n   }\n }"
    },
    {
        "commit_id": "8de4374427e77d5d9b79a710ca9225f749556eda",
        "commit_message": "HDDS-2158. Fixing Json Injection Issue in JsonUtils. (#1486)",
        "commit_url": "https://github.com/apache/hadoop/commit/8de4374427e77d5d9b79a710ca9225f749556eda",
        "buggy_code": "JsonUtils.toJsonString(token.toString())));",
        "fixed_code": "token.toString()));",
        "patch": "@@ -65,7 +65,7 @@ public Void call() throws Exception {\n     token.decodeFromUrlString(encodedToken);\n \n     System.out.printf(\"%s\", JsonUtils.toJsonStringWithDefaultPrettyPrinter(\n-        JsonUtils.toJsonString(token.toString())));\n+        token.toString()));\n     return null;\n   }\n }"
    },
    {
        "commit_id": "c5665b23ca92a8e18c4e9d24413c13f7cb7fd5fe",
        "commit_message": "HDDS-2228. Fix NPE in OzoneDelegationTokenManager#addPersistedDelegat\u2026 (#1571)",
        "commit_url": "https://github.com/apache/hadoop/commit/c5665b23ca92a8e18c4e9d24413c13f7cb7fd5fe",
        "buggy_code": "s3SecretManager);",
        "fixed_code": "s3SecretManager, certClient);",
        "patch": "@@ -627,7 +627,7 @@ private OzoneDelegationTokenSecretManager createDelegationTokenSecretManager(\n \n     return new OzoneDelegationTokenSecretManager(conf, tokenMaxLifetime,\n         tokenRenewInterval, tokenRemoverScanInterval, omRpcAddressTxt,\n-        s3SecretManager);\n+        s3SecretManager, certClient);\n   }\n \n   private OzoneBlockTokenSecretManager createBlockTokenSecretManager("
    },
    {
        "commit_id": "53ed78bcdb716d0351a934ac18661ef9fa6a03d4",
        "commit_message": "HDDS-2224. Fix loadup cache for cache cleanup policy NEVER. (#1567)",
        "commit_url": "https://github.com/apache/hadoop/commit/53ed78bcdb716d0351a934ac18661ef9fa6a03d4",
        "buggy_code": "cache.put(new CacheKey<>(kv.getKey()),",
        "fixed_code": "cache.loadInitial(new CacheKey<>(kv.getKey()),",
        "patch": "@@ -104,7 +104,7 @@ public TypedTable(\n           // We should build cache after OM restart when clean up policy is\n           // NEVER. Setting epoch value -1, so that when it is marked for\n           // delete, this will be considered for cleanup.\n-          cache.put(new CacheKey<>(kv.getKey()),\n+          cache.loadInitial(new CacheKey<>(kv.getKey()),\n               new CacheValue<>(Optional.of(kv.getValue()), EPOCH_DEFAULT));\n         }\n       }"
    },
    {
        "commit_id": "6ef6594c7ee09b561e42c16ce4e91c0479908ad8",
        "commit_message": "HDFS-14492. Snapshot memory leak. Contributed by Wei-Chiu Chuang. (#1370)\n\n* HDFS-14492. Snapshot memory leak. Contributed by Wei-Chiu Chuang.\r\n\r\nChange-Id: I9e5e450c07ad70aa1905973896c4f627042dbd37\r\n\r\n* Fix checkstyle\r\n\r\nChange-Id: I16d4bd4f03a971e1ed36cf57d89dc42357ef8fbf",
        "commit_url": "https://github.com/apache/hadoop/commit/6ef6594c7ee09b561e42c16ce4e91c0479908ad8",
        "buggy_code": "assertEquals(0, diffList.asList().size());",
        "fixed_code": "assertEquals(null, diffList);",
        "patch": "@@ -527,7 +527,7 @@ public void testDeleteEarliestSnapshot2() throws Exception {\n     assertEquals(snapshot1.getId(), diffList.getLast().getSnapshotId());\n     diffList = fsdir.getINode(metaChangeDir.toString()).asDirectory()\n         .getDiffs();\n-    assertEquals(0, diffList.asList().size());\n+    assertEquals(null, diffList);\n     \n     // check 2. noChangeDir and noChangeFile are still there\n     final INodeDirectory noChangeDirNode = "
    },
    {
        "commit_id": "b1e55cfb557056306db92b4a74f7b0288fd193ee",
        "commit_message": "HDFS-14461. RBF: Fix intermittently failing kerberos related unit test. Contributed by Xiaoqiao He.",
        "commit_url": "https://github.com/apache/hadoop/commit/b1e55cfb557056306db92b4a74f7b0288fd193ee",
        "buggy_code": "RouterHDFSContract.createCluster(false, 1, initSecurity());",
        "fixed_code": "RouterHDFSContract.createCluster(false, 1, true);",
        "patch": "@@ -46,7 +46,7 @@ public class TestRouterHDFSContractDelegationToken\n \n   @BeforeClass\n   public static void createCluster() throws Exception {\n-    RouterHDFSContract.createCluster(false, 1, initSecurity());\n+    RouterHDFSContract.createCluster(false, 1, true);\n   }\n \n   @AfterClass"
    },
    {
        "commit_id": "43203b466ddf6c9478b07be7e749257476ed9ca8",
        "commit_message": "HDFS-14868. RBF: Fix typo in TestRouterQuota. Contributed by Jinglun.\n\nReviewed-by: Ayush Saxena <ayushsaxena@apache.org>\nSigned-off-by: Wei-Chiu Chuang <weichiu@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/43203b466ddf6c9478b07be7e749257476ed9ca8",
        "buggy_code": "public void testStorageSpaceQuotaaExceed() throws Exception {",
        "fixed_code": "public void testStorageSpaceQuotaExceed() throws Exception {",
        "patch": "@@ -177,7 +177,7 @@ public void testNamespaceQuotaExceed() throws Exception {\n   }\n \n   @Test\n-  public void testStorageSpaceQuotaaExceed() throws Exception {\n+  public void testStorageSpaceQuotaExceed() throws Exception {\n     long ssQuota = 3071;\n     final FileSystem nnFs1 = nnContext1.getFileSystem();\n     final FileSystem nnFs2 = nnContext2.getFileSystem();"
    },
    {
        "commit_id": "5553887d9592d8ef59e5a2871919ced195edf42c",
        "commit_message": "HDDS-1949. Missing or error-prone test cleanup.\nContributed by Doroszlai, Attila.",
        "commit_url": "https://github.com/apache/hadoop/commit/5553887d9592d8ef59e5a2871919ced195edf42c",
        "buggy_code": "cluster.stop();",
        "fixed_code": "cluster.shutdown();",
        "patch": "@@ -332,7 +332,7 @@ public void testSCMSafeModeRestrictedOp() throws Exception {\n \n   @Test(timeout = 300_000)\n   public void testSCMSafeModeDisabled() throws Exception {\n-    cluster.stop();\n+    cluster.shutdown();\n \n     // If safe mode is disabled, cluster should not be in safe mode even if\n     // min number of datanodes are not started."
    },
    {
        "commit_id": "56248f9d87fdf65df6103f52f47dc6e8b9969abc",
        "commit_message": "HADOOP-16556. Fix some alerts raised by LGTM.\n\nContributed by Malcolm Taylor.\n\nChange-Id: Ic60c3f4681dd9d48b3afcba7520bd1e4d3cc4231",
        "commit_url": "https://github.com/apache/hadoop/commit/56248f9d87fdf65df6103f52f47dc6e8b9969abc",
        "buggy_code": "if (num < 0 || num > params.length) {",
        "fixed_code": "if (num < 0 || num >= params.length) {",
        "patch": "@@ -281,7 +281,7 @@ static String replaceParameters(String format,\n         if (paramNum != null) {\n           try {\n             int num = Integer.parseInt(paramNum);\n-            if (num < 0 || num > params.length) {\n+            if (num < 0 || num >= params.length) {\n               throw new BadFormatString(\"index \" + num + \" from \" + format +\n                                         \" is outside of the valid range 0 to \" +\n                                         (params.length - 1));"
    },
    {
        "commit_id": "56248f9d87fdf65df6103f52f47dc6e8b9969abc",
        "commit_message": "HADOOP-16556. Fix some alerts raised by LGTM.\n\nContributed by Malcolm Taylor.\n\nChange-Id: Ic60c3f4681dd9d48b3afcba7520bd1e4d3cc4231",
        "commit_url": "https://github.com/apache/hadoop/commit/56248f9d87fdf65df6103f52f47dc6e8b9969abc",
        "buggy_code": "LOG.warn(\"Failed to get current user {}, {}\", e);",
        "fixed_code": "LOG.warn(\"Failed to get current user, {}\", e);",
        "patch": "@@ -926,7 +926,7 @@ public void logCurrentHadoopUser() {\n       UserGroupInformation realUser = currentUser.getRealUser();\n       LOG.info(\"Real User = {}\" , realUser);\n     } catch (IOException e) {\n-      LOG.warn(\"Failed to get current user {}, {}\", e);\n+      LOG.warn(\"Failed to get current user, {}\", e);\n     }\n   }\n "
    },
    {
        "commit_id": "56248f9d87fdf65df6103f52f47dc6e8b9969abc",
        "commit_message": "HADOOP-16556. Fix some alerts raised by LGTM.\n\nContributed by Malcolm Taylor.\n\nChange-Id: Ic60c3f4681dd9d48b3afcba7520bd1e4d3cc4231",
        "commit_url": "https://github.com/apache/hadoop/commit/56248f9d87fdf65df6103f52f47dc6e8b9969abc",
        "buggy_code": "LOG.warn(\"[{}:{}] response [{}] {}\", new Object[]{method, path, status, message}, throwable);",
        "fixed_code": "LOG.warn(\"[{}:{}] response [{}] {}\", method, path, status, message, throwable);",
        "patch": "@@ -92,7 +92,7 @@ protected void log(Response.Status status, Throwable throwable) {\n     String path = MDC.get(\"path\");\n     String message = getOneLineMessage(throwable);\n     AUDIT_LOG.warn(\"FAILED [{}:{}] response [{}] {}\", new Object[]{method, path, status, message});\n-    LOG.warn(\"[{}:{}] response [{}] {}\", new Object[]{method, path, status, message}, throwable);\n+    LOG.warn(\"[{}:{}] response [{}] {}\", method, path, status, message, throwable);\n   }\n \n }"
    },
    {
        "commit_id": "8ab7020e641e65deb002a10732d23bb22802c09d",
        "commit_message": "HDFS-14779. Fix logging error in TestEditLog#testMultiStreamsLoadEditWithConfMaxTxns",
        "commit_url": "https://github.com/apache/hadoop/commit/8ab7020e641e65deb002a10732d23bb22802c09d",
        "buggy_code": "e.getMessage());",
        "fixed_code": "e);",
        "patch": "@@ -366,7 +366,7 @@ public void testMultiStreamsLoadEditWithConfMaxTxns()\n           readFsImage.loadEdits(editStreams, namesystem, 100, null, null);\n         } catch (Exception e){\n           LOG.error(\"There appears to be an out-of-order edit in the edit log\",\n-                  e.getMessage());\n+              e);\n           fail(\"no exception should be thrown\");\n         } finally {\n           if (readFsImage != null) {"
    },
    {
        "commit_id": "aa6995fde289719e0b300e11568c5e68c36b5d05",
        "commit_message": "HDFS-13201. Fix prompt message in testPolicyAndStateCantBeNull. Contributed by chencan.",
        "commit_url": "https://github.com/apache/hadoop/commit/aa6995fde289719e0b300e11568c5e68c36b5d05",
        "buggy_code": "fail(\"Null policy should fail\");",
        "fixed_code": "fail(\"Null policy state should fail\");",
        "patch": "@@ -43,7 +43,7 @@ public void testPolicyAndStateCantBeNull() {\n     try {\n       new ErasureCodingPolicyInfo(SystemErasureCodingPolicies\n           .getByID(RS_6_3_POLICY_ID), null);\n-      fail(\"Null policy should fail\");\n+      fail(\"Null policy state should fail\");\n     } catch (NullPointerException expected) {\n     }\n   }"
    },
    {
        "commit_id": "fc229b6490a152036b6424c7c0ac5c3df9525e57",
        "commit_message": "HDDS-1832 : Improve logging for PipelineActions handling in SCM and datanode. (Change to Error logging)\n\nSigned-off-by: Anu Engineer <aengineer@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/fc229b6490a152036b6424c7c0ac5c3df9525e57",
        "buggy_code": "LOG.info(",
        "fixed_code": "LOG.error(",
        "patch": "@@ -558,7 +558,7 @@ private void triggerPipelineClose(RaftGroupId groupId, String detail,\n     if (triggerHB) {\n       context.getParent().triggerHeartbeat();\n     }\n-    LOG.info(\n+    LOG.error(\n         \"pipeline Action \" + action.getAction() + \"  on pipeline \" + pipelineID\n             + \".Reason : \" + action.getClosePipeline().getDetailedReason());\n   }"
    },
    {
        "commit_id": "fc229b6490a152036b6424c7c0ac5c3df9525e57",
        "commit_message": "HDDS-1832 : Improve logging for PipelineActions handling in SCM and datanode. (Change to Error logging)\n\nSigned-off-by: Anu Engineer <aengineer@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/fc229b6490a152036b6424c7c0ac5c3df9525e57",
        "buggy_code": "LOG.info(\"Received pipeline action {} for {} from datanode {}. \" +",
        "fixed_code": "LOG.error(\"Received pipeline action {} for {} from datanode {}. \" +",
        "patch": "@@ -57,7 +57,7 @@ public void onMessage(PipelineActionsFromDatanode report,\n           pipelineID = PipelineID.\n               getFromProtobuf(action.getClosePipeline().getPipelineID());\n           Pipeline pipeline = pipelineManager.getPipeline(pipelineID);\n-          LOG.info(\"Received pipeline action {} for {} from datanode {}. \" +\n+          LOG.error(\"Received pipeline action {} for {} from datanode {}. \" +\n                   \"Reason : {}\", action.getAction(), pipeline,\n               report.getDatanodeDetails(),\n               action.getClosePipeline().getDetailedReason());"
    },
    {
        "commit_id": "ac6c4f0b290477017798491a4bd77fa9f107871c",
        "commit_message": "MAPREDUCE-7197. Fix order of actual and expected expression in assert statements. Contributed by Adam Antal",
        "commit_url": "https://github.com/apache/hadoop/commit/ac6c4f0b290477017798491a4bd77fa9f107871c",
        "buggy_code": "Assert.assertEquals(false, jr.isUber());",
        "fixed_code": "Assert.assertFalse(jr.isUber());",
        "patch": "@@ -285,7 +285,7 @@ private void verifyJobReport(JobReport jr) {\n     Assert.assertEquals(1, amInfo.getContainerId().getApplicationAttemptId()\n         .getAttemptId());\n     Assert.assertTrue(amInfo.getStartTime() > 0);\n-    Assert.assertEquals(false, jr.isUber());\n+    Assert.assertFalse(jr.isUber());\n   }\n   \n   private void verifyTaskAttemptReport(TaskAttemptReport tar) {"
    },
    {
        "commit_id": "ac6c4f0b290477017798491a4bd77fa9f107871c",
        "commit_message": "MAPREDUCE-7197. Fix order of actual and expected expression in assert statements. Contributed by Adam Antal",
        "commit_url": "https://github.com/apache/hadoop/commit/ac6c4f0b290477017798491a4bd77fa9f107871c",
        "buggy_code": "assertEquals(job.waitForCompletion(true), true);",
        "fixed_code": "assertTrue(job.waitForCompletion(true));",
        "patch": "@@ -90,7 +90,7 @@ public void testNewApis() throws Exception {\n     job.setOutputValueClass(IntWritable.class);\n     FileInputFormat.addInputPath(job, inDir);\n     FileOutputFormat.setOutputPath(job, outDir);\n-    assertEquals(job.waitForCompletion(true), true);\n+    assertTrue(job.waitForCompletion(true));\n \n     String output = readOutput(outDir, conf);\n     assertEquals(\"The\\t1\\nbrown\\t1\\nfox\\t2\\nhas\\t1\\nmany\\t1\\n\" +"
    },
    {
        "commit_id": "ac6c4f0b290477017798491a4bd77fa9f107871c",
        "commit_message": "MAPREDUCE-7197. Fix order of actual and expected expression in assert statements. Contributed by Adam Antal",
        "commit_url": "https://github.com/apache/hadoop/commit/ac6c4f0b290477017798491a4bd77fa9f107871c",
        "buggy_code": "assertEquals(true, cache.checkTotalMemoryUsed());",
        "fixed_code": "assertTrue(cache.checkTotalMemoryUsed());",
        "patch": "@@ -244,7 +244,7 @@ public void run() {\n       }\n       getInfoThread.join();\n       removeMapThread.join();\n-      assertEquals(true, cache.checkTotalMemoryUsed());\n+      assertTrue(cache.checkTotalMemoryUsed());\n     }      \n   }\n "
    },
    {
        "commit_id": "a63023f2610438b9a142db3feb14236fe188b42d",
        "commit_message": "HDDS-1901. Fix Ozone HTTP WebConsole Authentication. Contributed by Xiaoyu Yao. (#1228)",
        "commit_url": "https://github.com/apache/hadoop/commit/a63023f2610438b9a142db3feb14236fe188b42d",
        "buggy_code": "\"hdds.scm.http.kerberos.keytab.file\";",
        "fixed_code": "\"hdds.scm.http.kerberos.keytab\";",
        "patch": "@@ -365,7 +365,7 @@ public final class ScmConfigKeys {\n       \"hdds.scm.http.kerberos.principal\";\n   public static final String\n       HDDS_SCM_HTTP_KERBEROS_KEYTAB_FILE_KEY =\n-      \"hdds.scm.http.kerberos.keytab.file\";\n+      \"hdds.scm.http.kerberos.keytab\";\n \n   // Network topology\n   public static final String OZONE_SCM_NETWORK_TOPOLOGY_SCHEMA_FILE ="
    },
    {
        "commit_id": "a63023f2610438b9a142db3feb14236fe188b42d",
        "commit_message": "HDDS-1901. Fix Ozone HTTP WebConsole Authentication. Contributed by Xiaoyu Yao. (#1228)",
        "commit_url": "https://github.com/apache/hadoop/commit/a63023f2610438b9a142db3feb14236fe188b42d",
        "buggy_code": "\"ozone.om.http.kerberos.keytab.file\";",
        "fixed_code": "\"ozone.om.http.kerberos.keytab\";",
        "patch": "@@ -213,7 +213,7 @@ private OMConfigKeys() {\n   public static final String OZONE_OM_KERBEROS_PRINCIPAL_KEY = \"ozone.om\"\n       + \".kerberos.principal\";\n   public static final String OZONE_OM_HTTP_KERBEROS_KEYTAB_FILE =\n-      \"ozone.om.http.kerberos.keytab.file\";\n+      \"ozone.om.http.kerberos.keytab\";\n   public static final String OZONE_OM_HTTP_KERBEROS_PRINCIPAL_KEY\n       = \"ozone.om.http.kerberos.principal\";\n   // Delegation token related keys"
    },
    {
        "commit_id": "ec1d453846ca7446b5372b11372311b65bef8a4b",
        "commit_message": "HDDS-1788. Fix kerberos principal error in Ozone Recon. (#1201)",
        "commit_url": "https://github.com/apache/hadoop/commit/ec1d453846ca7446b5372b11372311b65bef8a4b",
        "buggy_code": "OzoneConfigurationProvider.setConfiguration(ozoneConfiguration);",
        "fixed_code": "ConfigurationProvider.setConfiguration(ozoneConfiguration);",
        "patch": "@@ -64,7 +64,7 @@ public static void main(String[] args) {\n   @Override\n   public Void call() throws Exception {\n     OzoneConfiguration ozoneConfiguration = createOzoneConfiguration();\n-    OzoneConfigurationProvider.setConfiguration(ozoneConfiguration);\n+    ConfigurationProvider.setConfiguration(ozoneConfiguration);\n \n     injector =  Guice.createInjector(new\n         ReconControllerModule(), new ReconRestServletModule() {"
    },
    {
        "commit_id": "a6f47b5876e51e888058e3731e6c15ea2656f2f7",
        "commit_message": "HDDS-1875. Fix failures in TestS3MultipartUploadAbortResponse. (#1188)",
        "commit_url": "https://github.com/apache/hadoop/commit/a6f47b5876e51e888058e3731e6c15ea2656f2f7",
        "buggy_code": "return new S3MultipartUploadAbortResponse(multipartKey, Time.now(),",
        "fixed_code": "return new S3MultipartUploadAbortResponse(multipartKey, timeStamp,",
        "patch": "@@ -113,7 +113,7 @@ public S3MultipartUploadAbortResponse createS3AbortMPUResponse(\n         .setAbortMultiPartUploadResponse(\n             MultipartUploadAbortResponse.newBuilder().build()).build();\n \n-    return new S3MultipartUploadAbortResponse(multipartKey, Time.now(),\n+    return new S3MultipartUploadAbortResponse(multipartKey, timeStamp,\n             omMultipartKeyInfo,\n             omResponse);\n   }"
    },
    {
        "commit_id": "a6f47b5876e51e888058e3731e6c15ea2656f2f7",
        "commit_message": "HDDS-1875. Fix failures in TestS3MultipartUploadAbortResponse. (#1188)",
        "commit_url": "https://github.com/apache/hadoop/commit/a6f47b5876e51e888058e3731e6c15ea2656f2f7",
        "buggy_code": "keyName, 1);",
        "fixed_code": "keyName, 2);",
        "patch": "@@ -82,7 +82,7 @@ public void testAddDBToBatchWithParts() throws Exception {\n     PartKeyInfo part1 = createPartKeyInfo(volumeName, bucketName,\n         keyName, 1);\n     PartKeyInfo part2 = createPartKeyInfo(volumeName, bucketName,\n-        keyName, 1);\n+        keyName, 2);\n \n     addPart(1, part1, omMultipartKeyInfo);\n     addPart(2, part2, omMultipartKeyInfo);"
    },
    {
        "commit_id": "ecc8acfd242ab933d2bd616fffacacca9011a6b1",
        "commit_message": "HDFS-14673. The console log is noisy when using DNSDomainNameResolver to resolve NameNode.",
        "commit_url": "https://github.com/apache/hadoop/commit/ecc8acfd242ab933d2bd616fffacacca9011a6b1",
        "buggy_code": "LOG.info(\"Namenode domain name will be resolved with {}\",",
        "fixed_code": "LOG.debug(\"Namenode domain name will be resolved with {}\",",
        "patch": "@@ -234,7 +234,7 @@ Collection<InetSocketAddress> getResolvedHostsIfNecessary(\n           conf, nameNodeUri, HdfsClientConfigKeys.Failover.RESOLVE_SERVICE_KEY);\n     // If the address needs to be resolved, get all of the IP addresses\n     // from this address and pass them into the proxy\n-    LOG.info(\"Namenode domain name will be resolved with {}\",\n+    LOG.debug(\"Namenode domain name will be resolved with {}\",\n         dnr.getClass().getName());\n     for (InetSocketAddress address : addressesOfNns) {\n       String[] resolvedHostNames = dnr.getAllResolvedHostnameByDomainName("
    },
    {
        "commit_id": "b15ef7dc3d91c6d50fa515158104fba29f43e6b0",
        "commit_message": "HADOOP-16384: S3A: Avoid inconsistencies between DDB and S3.\n\nContributed by Steve Loughran\n\nContains\n\n- HADOOP-16397. Hadoop S3Guard Prune command to support a -tombstone option.\n- HADOOP-16406. ITestDynamoDBMetadataStore.testProvisionTable times out intermittently\n\nThis patch doesn't fix the underlying problem but it\n\n* changes some tests to clean up better\n* does a lot more in logging operations in against DDB, if enabled\n* adds an entry point to dump the state of the metastore and s3 tables (precursor to fsck)\n* adds a purge entry point to help clean up after a test run has got a store into a mess\n* s3guard prune command adds -tombstone option to only clear tombstones\n\nThe outcome is that tests should pass consistently and if problems occur we have better diagnostics.\n\nChange-Id: I3eca3f5529d7f6fec398c0ff0472919f08f054eb",
        "commit_url": "https://github.com/apache/hadoop/commit/b15ef7dc3d91c6d50fa515158104fba29f43e6b0",
        "buggy_code": "BulkOperationState.OperationType.Put, path);",
        "fixed_code": "BulkOperationState.OperationType.Commit, path);",
        "patch": "@@ -538,7 +538,7 @@ public CompleteMultipartUploadResult commitUpload(\n   public BulkOperationState initiateCommitOperation(\n       Path path) throws IOException {\n     return S3Guard.initiateBulkWrite(owner.getMetadataStore(),\n-        BulkOperationState.OperationType.Put, path);\n+        BulkOperationState.OperationType.Commit, path);\n   }\n \n   /**"
    },
    {
        "commit_id": "6a3433bffdbdefc5aa66705085bcf6fa089721b2",
        "commit_message": "HADOOP-16357. TeraSort Job failing on S3 DirectoryStagingCommitter: destination path exists.\n\nContributed by Steve Loughran.\n\nThis patch\n\n* changes the default for the staging committer to append, as we get for the classic FileOutputFormat committer\n* adds a check for the dest path being a file not a dir\n* adds tests for this\n* Changes AbstractCommitTerasortIT. to not use the simple parser, so fails if the file is present.\n\nChange-Id: Id53742958ed1cf321ff96c9063505d64f3254f53",
        "commit_url": "https://github.com/apache/hadoop/commit/6a3433bffdbdefc5aa66705085bcf6fa089721b2",
        "buggy_code": "public static final String DEFAULT_CONFLICT_MODE = CONFLICT_MODE_FAIL;",
        "fixed_code": "public static final String DEFAULT_CONFLICT_MODE = CONFLICT_MODE_APPEND;",
        "patch": "@@ -198,7 +198,7 @@ private CommitConstants() {\n   public static final String CONFLICT_MODE_REPLACE = \"replace\";\n \n   /** Default conflict mode: {@value}. */\n-  public static final String DEFAULT_CONFLICT_MODE = CONFLICT_MODE_FAIL;\n+  public static final String DEFAULT_CONFLICT_MODE = CONFLICT_MODE_APPEND;\n \n   /**\n    * Number of threads in committers for parallel operations on files"
    },
    {
        "commit_id": "fc0656dd300f037cb8f97a4c1fac4bf942af3d0a",
        "commit_message": "HADOOP-16418. [Dynamometer] Fix checkstyle and findbugs warnings. Contributed by Erik Krogen.",
        "commit_url": "https://github.com/apache/hadoop/commit/fc0656dd300f037cb8f97a4c1fac4bf942af3d0a",
        "buggy_code": "BlockInfo blockInfo = new BlockInfo(it.next());",
        "fixed_code": "BlockInfo blockInfo = it.next();",
        "patch": "@@ -76,7 +76,7 @@ public void reduce(IntWritable key, Iterable<BlockInfo> values,\n \n     Text out = new Text();\n     while (it.hasNext()) {\n-      BlockInfo blockInfo = new BlockInfo(it.next());\n+      BlockInfo blockInfo = it.next();\n       String blockLine = blockInfo.getBlockId() + \",\"\n           + blockInfo.getBlockGenerationStamp() + \",\" + blockInfo.getSize();\n       out.set(blockLine);"
    },
    {
        "commit_id": "c3ca348b81bdf6aa0857a1d4d140c6c9d64be490",
        "commit_message": "HDFS-14620. RBF: Fix 'not a super user' error when disabling a namespace in kerberos with superuser principal. Contributed by luhuachao.",
        "commit_url": "https://github.com/apache/hadoop/commit/c3ca348b81bdf6aa0857a1d4d140c6c9d64be490",
        "buggy_code": "if (ugi.getUserName().equals(superUser)) {",
        "fixed_code": "if (ugi.getShortUserName().equals(superUser)) {",
        "patch": "@@ -121,7 +121,7 @@ public void checkSuperuserPrivilege() throws  AccessControlException {\n     }\n \n     // Is this by the Router user itself?\n-    if (ugi.getUserName().equals(superUser)) {\n+    if (ugi.getShortUserName().equals(superUser)) {\n       return;\n     }\n "
    },
    {
        "commit_id": "fcabc8f0e4097cce934308fdd28cd3bcdbb66877",
        "commit_message": "HDFS-14335. RBF: Fix heartbeat typos in the Router. Contributed by CR Hota.",
        "commit_url": "https://github.com/apache/hadoop/commit/fcabc8f0e4097cce934308fdd28cd3bcdbb66877",
        "buggy_code": ".getRouter().getNamenodeHearbeatServices();",
        "fixed_code": ".getRouter().getNamenodeHeartbeatServices();",
        "patch": "@@ -341,7 +341,7 @@ public void testNoNamenodesAvailable() throws Exception{\n     for (RouterContext routerContext : cluster.getRouters()) {\n       // Manually trigger the heartbeat\n       Collection<NamenodeHeartbeatService> heartbeatServices = routerContext\n-          .getRouter().getNamenodeHearbeatServices();\n+          .getRouter().getNamenodeHeartbeatServices();\n       for (NamenodeHeartbeatService service : heartbeatServices) {\n         service.periodicInvoke();\n       }"
    },
    {
        "commit_id": "fcabc8f0e4097cce934308fdd28cd3bcdbb66877",
        "commit_message": "HDFS-14335. RBF: Fix heartbeat typos in the Router. Contributed by CR Hota.",
        "commit_url": "https://github.com/apache/hadoop/commit/fcabc8f0e4097cce934308fdd28cd3bcdbb66877",
        "buggy_code": ".getRouter().getNamenodeHearbeatServices();",
        "fixed_code": ".getRouter().getNamenodeHeartbeatServices();",
        "patch": "@@ -113,7 +113,7 @@ public void testNamenodeMonitoring() throws Exception {\n     }\n \n     Collection<NamenodeHeartbeatService> heartbeatServices = routerContext\n-        .getRouter().getNamenodeHearbeatServices();\n+        .getRouter().getNamenodeHeartbeatServices();\n     // manually trigger the heartbeat\n     for (NamenodeHeartbeatService service : heartbeatServices) {\n       service.periodicInvoke();"
    },
    {
        "commit_id": "7bbe01a1960f1f4168b30545d448ff58ff557de4",
        "commit_message": "HDFS-14259. RBF: Fix safemode message for Router. Contributed by Ranith Sadar.",
        "commit_url": "https://github.com/apache/hadoop/commit/7bbe01a1960f1f4168b30545d448ff58ff557de4",
        "buggy_code": "if (!getRouter().isRouterState(RouterServiceState.SAFEMODE)) {",
        "fixed_code": "if (getRouter().isRouterState(RouterServiceState.SAFEMODE)) {",
        "patch": "@@ -234,7 +234,7 @@ public long getProvidedCapacity() {\n   @Override\n   public String getSafemode() {\n     try {\n-      if (!getRouter().isRouterState(RouterServiceState.SAFEMODE)) {\n+      if (getRouter().isRouterState(RouterServiceState.SAFEMODE)) {\n         return \"Safe mode is ON. \" + this.getSafeModeTip();\n       }\n     } catch (IOException e) {"
    },
    {
        "commit_id": "01b4126b4e8124edfde20ba4733c6300bb994251",
        "commit_message": "HDFS-14152. RBF: Fix a typo in RouterAdmin usage. Contributed by Ayush Saxena.",
        "commit_url": "https://github.com/apache/hadoop/commit/01b4126b4e8124edfde20ba4733c6300bb994251",
        "buggy_code": "usage.append(\"Usage: hdfs routeradmin :\\n\");",
        "fixed_code": "usage.append(\"Usage: hdfs dfsrouteradmin :\\n\");",
        "patch": "@@ -109,7 +109,7 @@ private String getUsage(String cmd) {\n           {\"-add\", \"-update\", \"-rm\", \"-ls\", \"-setQuota\", \"-clrQuota\",\n               \"-safemode\", \"-nameservice\", \"-getDisabledNameservices\"};\n       StringBuilder usage = new StringBuilder();\n-      usage.append(\"Usage: hdfs routeradmin :\\n\");\n+      usage.append(\"Usage: hdfs dfsrouteradmin :\\n\");\n       for (int i = 0; i < commands.length; i++) {\n         usage.append(getUsage(commands[i]));\n         if (i + 1 < commands.length) {"
    },
    {
        "commit_id": "01b4126b4e8124edfde20ba4733c6300bb994251",
        "commit_message": "HDFS-14152. RBF: Fix a typo in RouterAdmin usage. Contributed by Ayush Saxena.",
        "commit_url": "https://github.com/apache/hadoop/commit/01b4126b4e8124edfde20ba4733c6300bb994251",
        "buggy_code": "String expected = \"Usage: hdfs routeradmin :\\n\"",
        "fixed_code": "String expected = \"Usage: hdfs dfsrouteradmin :\\n\"",
        "patch": "@@ -549,7 +549,7 @@ public void testInvalidArgumentMessage() throws Exception {\n \n     argv = new String[] {\"-Random\"};\n     assertEquals(-1, ToolRunner.run(admin, argv));\n-    String expected = \"Usage: hdfs routeradmin :\\n\"\n+    String expected = \"Usage: hdfs dfsrouteradmin :\\n\"\n         + \"\\t[-add <source> <nameservice1, nameservice2, ...> <destination> \"\n         + \"[-readonly] [-order HASH|LOCAL|RANDOM|HASH_ALL] \"\n         + \"-owner <owner> -group <group> -mode <mode>]\\n\""
    },
    {
        "commit_id": "3c1a1ceea9e35ac53376276139416b728ed57f10",
        "commit_message": "HDFS-14487. Missing Space in Client Error Message (Contributed by Shweta Yakkali via Daniel Templeton)\n\nChange-Id: I0f8ce74a35ab24fe94fd0e57d8247bb3fa575e6f",
        "commit_url": "https://github.com/apache/hadoop/commit/3c1a1ceea9e35ac53376276139416b728ed57f10",
        "buggy_code": "throw new IOException(\"Unable to close file because the last block\"",
        "fixed_code": "throw new IOException(\"Unable to close file because the last block \"",
        "patch": "@@ -965,7 +965,7 @@ protected void completeFile(ExtendedBlock last) throws IOException {\n         }\n         try {\n           if (retries == 0) {\n-            throw new IOException(\"Unable to close file because the last block\"\n+            throw new IOException(\"Unable to close file because the last block \"\n                 + last + \" does not have enough number of replicas.\");\n           }\n           retries--;"
    },
    {
        "commit_id": "9ebbda342f2adbbce30820a6f8374d310e361ff8",
        "commit_message": "HADOOP-16372. Fix typo in DFSUtil getHttpPolicy method\n\nCloses #967",
        "commit_url": "https://github.com/apache/hadoop/commit/9ebbda342f2adbbce30820a6f8374d310e361ff8",
        "buggy_code": "throw new HadoopIllegalArgumentException(\"Unregonized value '\"",
        "fixed_code": "throw new HadoopIllegalArgumentException(\"Unrecognized value '\"",
        "patch": "@@ -1476,7 +1476,7 @@ public static HttpConfig.Policy getHttpPolicy(Configuration conf) {\n         DFSConfigKeys.DFS_HTTP_POLICY_DEFAULT);\n     HttpConfig.Policy policy = HttpConfig.Policy.fromString(policyStr);\n     if (policy == null) {\n-      throw new HadoopIllegalArgumentException(\"Unregonized value '\"\n+      throw new HadoopIllegalArgumentException(\"Unrecognized value '\"\n           + policyStr + \"' for \" + DFSConfigKeys.DFS_HTTP_POLICY_KEY);\n     }\n "
    },
    {
        "commit_id": "9deac3b6bf46ff8875cdf2dfa6f7064f9379bccd",
        "commit_message": "HDDS-1657. Fix parallelStream usage in volume and key native acl. Contributed by Ajay Kumar. (#926)",
        "commit_url": "https://github.com/apache/hadoop/commit/9deac3b6bf46ff8875cdf2dfa6f7064f9379bccd",
        "buggy_code": "acls.parallelStream().forEach(a -> builder.addAcl(OzoneAcl.toProtobuf(a)));",
        "fixed_code": "acls.forEach(a -> builder.addAcl(OzoneAcl.toProtobuf(a)));",
        "patch": "@@ -1377,7 +1377,7 @@ public boolean setAcl(OzoneObj obj, List<OzoneAcl> acls) throws IOException {\n     SetAclRequest.Builder builder = SetAclRequest.newBuilder()\n         .setObj(OzoneObj.toProtobuf(obj));\n \n-    acls.parallelStream().forEach(a -> builder.addAcl(OzoneAcl.toProtobuf(a)));\n+    acls.forEach(a -> builder.addAcl(OzoneAcl.toProtobuf(a)));\n \n     OMRequest omRequest = createOMRequest(Type.SetAcl)\n         .setSetAclRequest(builder.build())"
    },
    {
        "commit_id": "ec92ca6575e0074ed4983fa8b34324bdbeb23499",
        "commit_message": "HDDS-1598. Fix Ozone checkstyle issues on trunk. Contributed by Elek, Marton. (#854)",
        "commit_url": "https://github.com/apache/hadoop/commit/ec92ca6575e0074ed4983fa8b34324bdbeb23499",
        "buggy_code": "private final long STORAGE_CAPACITY = 100L;",
        "fixed_code": "private static final long STORAGE_CAPACITY = 100L;",
        "patch": "@@ -53,7 +53,7 @@ public class TestSCMContainerPlacementRackAware {\n   // policy prohibit fallback\n   private SCMContainerPlacementRackAware policyNoFallback;\n   // node storage capacity\n-  private final long STORAGE_CAPACITY = 100L;\n+  private static final long STORAGE_CAPACITY = 100L;\n \n   @Before\n   public void setup() {"
    },
    {
        "commit_id": "597fa47ad125c0871f5c4deb3a883e5b3341c67b",
        "commit_message": "YARN-9529. Log correct cpu controller path on error while initializing CGroups. (Contributed by Jonathan Hung)",
        "commit_url": "https://github.com/apache/hadoop/commit/597fa47ad125c0871f5c4deb3a883e5b3341c67b",
        "buggy_code": "+ \"to cgroup at: \" + controllerPath);",
        "fixed_code": "+ \"to cgroup at: \" + f.getPath());",
        "patch": "@@ -476,7 +476,7 @@ private void initializeControllerPaths() throws IOException {\n         controllerPaths.put(CONTROLLER_CPU, controllerPath);\n       } else {\n         throw new IOException(\"Not able to enforce cpu weights; cannot write \"\n-            + \"to cgroup at: \" + controllerPath);\n+            + \"to cgroup at: \" + f.getPath());\n       }\n     } else {\n       throw new IOException(\"Not able to enforce cpu weights; cannot find \""
    },
    {
        "commit_id": "e424392a62418fad401fe80bf6517e375911c08c",
        "commit_message": "HDFS-14438. Fix typo in OfflineEditsVisitorFactory. Contributed by bianqi.",
        "commit_url": "https://github.com/apache/hadoop/commit/e424392a62418fad401fe80bf6517e375911c08c",
        "buggy_code": "throw new IOException(\"Unknown proccesor \" + processor +",
        "fixed_code": "throw new IOException(\"Unknown processor \" + processor +",
        "patch": "@@ -66,7 +66,7 @@ static public OfflineEditsVisitor getEditsVisitor(String filename,\n       } else if(StringUtils.equalsIgnoreCase(\"stats\", processor)) {\n         vis = new StatisticsEditsVisitor(out);\n       } else {\n-        throw new IOException(\"Unknown proccesor \" + processor +\n+        throw new IOException(\"Unknown processor \" + processor +\n           \" (valid processors: xml, binary, stats)\");\n       }\n       out = fout = null;"
    },
    {
        "commit_id": "a703dae25e3c75a4e6086efd4b620ef956e6fe54",
        "commit_message": "HADOOP-16222. Fix new deprecations after guava 27.0 update in trunk. Contributed by Gabor Bota.",
        "commit_url": "https://github.com/apache/hadoop/commit/a703dae25e3c75a4e6086efd4b620ef956e6fe54",
        "buggy_code": "this(UTF8.class, new Writable[strings.length]);",
        "fixed_code": "this(Text.class, new Writable[strings.length]);",
        "patch": "@@ -58,7 +58,7 @@ public ArrayWritable(Class<? extends Writable> valueClass, Writable[] values) {\n   }\n \n   public ArrayWritable(String[] strings) {\n-    this(UTF8.class, new Writable[strings.length]);\n+    this(Text.class, new Writable[strings.length]);\n     for (int i = 0; i < strings.length; i++) {\n       values[i] = new UTF8(strings[i]);\n     }"
    },
    {
        "commit_id": "a703dae25e3c75a4e6086efd4b620ef956e6fe54",
        "commit_message": "HADOOP-16222. Fix new deprecations after guava 27.0 update in trunk. Contributed by Gabor Bota.",
        "commit_url": "https://github.com/apache/hadoop/commit/a703dae25e3c75a4e6086efd4b620ef956e6fe54",
        "buggy_code": "return Files.toString(new File(path), Charsets.UTF_8).trim();",
        "fixed_code": "return Files.asCharSource(new File(path), Charsets.UTF_8).read().trim();",
        "patch": "@@ -172,7 +172,7 @@ public static String resolveConfIndirection(String valInConf)\n       return valInConf;\n     }\n     String path = valInConf.substring(1).trim();\n-    return Files.toString(new File(path), Charsets.UTF_8).trim();\n+    return Files.asCharSource(new File(path), Charsets.UTF_8).read().trim();\n   }\n \n   /**"
    },
    {
        "commit_id": "a703dae25e3c75a4e6086efd4b620ef956e6fe54",
        "commit_message": "HADOOP-16222. Fix new deprecations after guava 27.0 update in trunk. Contributed by Gabor Bota.",
        "commit_url": "https://github.com/apache/hadoop/commit/a703dae25e3c75a4e6086efd4b620ef956e6fe54",
        "buggy_code": "UTF8.class, arrayWritable.getValueClass());",
        "fixed_code": "Text.class, arrayWritable.getValueClass());",
        "patch": "@@ -106,7 +106,7 @@ public void testArrayWritableStringConstructor() {\n     String[] original = { \"test1\", \"test2\", \"test3\" };\n     ArrayWritable arrayWritable = new ArrayWritable(original);\n     assertEquals(\"testArrayWritableStringConstructor class error!!!\", \n-        UTF8.class, arrayWritable.getValueClass());\n+        Text.class, arrayWritable.getValueClass());\n     assertArrayEquals(\"testArrayWritableStringConstructor toString error!!!\",\n       original, arrayWritable.toStrings());\n   }"
    },
    {
        "commit_id": "a703dae25e3c75a4e6086efd4b620ef956e6fe54",
        "commit_message": "HADOOP-16222. Fix new deprecations after guava 27.0 update in trunk. Contributed by Gabor Bota.",
        "commit_url": "https://github.com/apache/hadoop/commit/a703dae25e3c75a4e6086efd4b620ef956e6fe54",
        "buggy_code": "Files.write(\"hello world\", TEST_FILE, Charsets.UTF_8);",
        "fixed_code": "Files.asCharSink(TEST_FILE, Charsets.UTF_8).write(\"hello world\");",
        "patch": "@@ -131,7 +131,7 @@ public void testConfIndirection() throws IOException {\n     assertEquals(\"x\", ZKUtil.resolveConfIndirection(\"x\"));\n     \n     TEST_FILE.getParentFile().mkdirs();\n-    Files.write(\"hello world\", TEST_FILE, Charsets.UTF_8);\n+    Files.asCharSink(TEST_FILE, Charsets.UTF_8).write(\"hello world\");\n     assertEquals(\"hello world\", ZKUtil.resolveConfIndirection(\n         \"@\" + TEST_FILE.getAbsolutePath()));\n     "
    },
    {
        "commit_id": "1ddb48872f6a4985f4d0baadbb183899226cff68",
        "commit_message": "HADOOP-16265. Fix bug causing Configuration#getTimeDuration to use incorrect units when the default value is used. Contributed by starphin.",
        "commit_url": "https://github.com/apache/hadoop/commit/1ddb48872f6a4985f4d0baadbb183899226cff68",
        "buggy_code": "return defaultValue;",
        "fixed_code": "return returnUnit.convert(defaultValue, defaultUnit);",
        "patch": "@@ -1840,7 +1840,7 @@ public long getTimeDuration(String name, long defaultValue,\n       TimeUnit defaultUnit, TimeUnit returnUnit) {\n     String vStr = get(name);\n     if (null == vStr) {\n-      return defaultValue;\n+      return returnUnit.convert(defaultValue, defaultUnit);\n     } else {\n       return getTimeDurationHelper(name, vStr, defaultUnit, returnUnit);\n     }"
    },
    {
        "commit_id": "1943db557124439f9f41c18a618455ccf4c3e6cc",
        "commit_message": "HADOOP-16237. Fix new findbugs issues after updating guava to 27.0-jre.\n\nAuthor:    Gabor Bota <gabor.bota@cloudera.com>",
        "commit_url": "https://github.com/apache/hadoop/commit/1943db557124439f9f41c18a618455ccf4c3e6cc",
        "buggy_code": "private static DocumentClient client;",
        "fixed_code": "private static volatile DocumentClient client;",
        "patch": "@@ -49,7 +49,7 @@ public class CosmosDBDocumentStoreReader<TimelineDoc extends TimelineDocument>\n       .getLogger(CosmosDBDocumentStoreReader.class);\n   private static final int DEFAULT_DOCUMENTS_SIZE = 1;\n \n-  private static DocumentClient client;\n+  private static volatile DocumentClient client;\n   private final String databaseName;\n   private final static String COLLECTION_LINK = \"/dbs/%s/colls/%s\";\n   private final static String SELECT_TOP_FROM_COLLECTION = \"SELECT TOP %d * \" +"
    },
    {
        "commit_id": "1943db557124439f9f41c18a618455ccf4c3e6cc",
        "commit_message": "HADOOP-16237. Fix new findbugs issues after updating guava to 27.0-jre.\n\nAuthor:    Gabor Bota <gabor.bota@cloudera.com>",
        "commit_url": "https://github.com/apache/hadoop/commit/1943db557124439f9f41c18a618455ccf4c3e6cc",
        "buggy_code": "private static DocumentClient client;",
        "fixed_code": "private static volatile DocumentClient client;",
        "patch": "@@ -51,7 +51,7 @@ public class CosmosDBDocumentStoreWriter<TimelineDoc extends TimelineDocument>\n   private static final Logger LOG = LoggerFactory\n       .getLogger(CosmosDBDocumentStoreWriter.class);\n \n-  private static DocumentClient client;\n+  private static volatile DocumentClient client;\n   private final String databaseName;\n   private static final PerNodeAggTimelineCollectorMetrics METRICS =\n       PerNodeAggTimelineCollectorMetrics.getInstance();"
    },
    {
        "commit_id": "813cee1a18b2df05dff90e4a2183546bc05cd712",
        "commit_message": "HDFS-14420. Fix typo in KeyShell console. Contributed by Hu Xiaodong.",
        "commit_url": "https://github.com/apache/hadoop/commit/813cee1a18b2df05dff90e4a2183546bc05cd712",
        "buggy_code": "\"value:\\natttribute \\\"\" + attr + \"\\\" was repeated\\n\");",
        "fixed_code": "\"value:\\nattribute \\\"\" + attr + \"\\\" was repeated\\n\");",
        "patch": "@@ -139,7 +139,7 @@ protected int init(String[] args) throws IOException {\n         }\n         if (attributes.containsKey(attr)) {\n           getOut().println(\"\\nEach attribute must correspond to only one \" +\n-              \"value:\\natttribute \\\"\" + attr + \"\\\" was repeated\\n\");\n+              \"value:\\nattribute \\\"\" + attr + \"\\\" was repeated\\n\");\n           return 1;\n         }\n         attributes.put(attr, val);"
    },
    {
        "commit_id": "260d843b258b5930091a1bc92f3fca8fabf3bfd2",
        "commit_message": "HDFS-14416. Fix TestHdfsConfigFields for field dfs.client.failover.resolver.useFQDN. Contributed by Fengnan Li.",
        "commit_url": "https://github.com/apache/hadoop/commit/260d843b258b5930091a1bc92f3fca8fabf3bfd2",
        "buggy_code": "String  RESOLVE_ADDRESS_TO_FQDN = PREFIX + \"useFQDN\";",
        "fixed_code": "String  RESOLVE_ADDRESS_TO_FQDN = PREFIX + \"resolver.useFQDN\";",
        "patch": "@@ -291,7 +291,7 @@ interface Failover {\n     String  RESOLVE_ADDRESS_NEEDED_KEY = PREFIX + \"resolve-needed\";\n     boolean RESOLVE_ADDRESS_NEEDED_DEFAULT = false;\n     String RESOLVE_SERVICE_KEY = PREFIX + \"resolver.impl\";\n-    String  RESOLVE_ADDRESS_TO_FQDN = PREFIX + \"useFQDN\";\n+    String  RESOLVE_ADDRESS_TO_FQDN = PREFIX + \"resolver.useFQDN\";\n     boolean RESOLVE_ADDRESS_TO_FQDN_DEFAULT = true;\n   }\n "
    },
    {
        "commit_id": "67020f09502a4f07342dee457e47bb52b03441ae",
        "commit_message": "HDFS-14407. Fix misuse of SLF4j logging API in DatasetVolumeChecker#checkAllVolumes. Contributed by Wanqiang Ji.",
        "commit_url": "https://github.com/apache/hadoop/commit/67020f09502a4f07342dee457e47bb52b03441ae",
        "buggy_code": "LOG.warn(\"checkAllVolumes timed out after {} ms\" +",
        "fixed_code": "LOG.warn(\"checkAllVolumes timed out after {} ms\",",
        "patch": "@@ -242,7 +242,7 @@ public void call(Set<FsVolumeSpi> ignored1,\n     // Wait until our timeout elapses, after which we give up on\n     // the remaining volumes.\n     if (!latch.await(maxAllowedTimeForCheckMs, TimeUnit.MILLISECONDS)) {\n-      LOG.warn(\"checkAllVolumes timed out after {} ms\" +\n+      LOG.warn(\"checkAllVolumes timed out after {} ms\",\n           maxAllowedTimeForCheckMs);\n     }\n "
    },
    {
        "commit_id": "73f7b04e2b8f9a4b06a1e5b5c62eadd074555205",
        "commit_message": "HDDS-1302. Fix SCM CLI does not list container with id 1.",
        "commit_url": "https://github.com/apache/hadoop/commit/73f7b04e2b8f9a4b06a1e5b5c62eadd074555205",
        "buggy_code": ".filter(id -> id.getId() >= startId)",
        "fixed_code": ".filter(id -> id.getId() > startId)",
        "patch": "@@ -200,7 +200,7 @@ public List<ContainerInfo> listContainer(ContainerID startContainerID,\n       Collections.sort(containersIds);\n \n       return containersIds.stream()\n-          .filter(id -> id.getId() >= startId)\n+          .filter(id -> id.getId() > startId)\n           .limit(count)\n           .map(id -> {\n             try {"
    },
    {
        "commit_id": "926d548caabdfcfbf7a75dcf0657e8dde6d9710a",
        "commit_message": "HDDS-1281. Fix the findbug issue caused by HDDS-1163. Contributed by Aravindan Vijayan.",
        "commit_url": "https://github.com/apache/hadoop/commit/926d548caabdfcfbf7a75dcf0657e8dde6d9710a",
        "buggy_code": "containerId, containerData);",
        "fixed_code": "containerId);",
        "patch": "@@ -677,7 +677,7 @@ public void check() throws StorageContainerException {\n \n     KeyValueContainerCheck checker =\n         new KeyValueContainerCheck(containerData.getMetadataPath(), config,\n-            containerId, containerData);\n+            containerId);\n \n     switch (level) {\n     case FAST_CHECK:"
    },
    {
        "commit_id": "926d548caabdfcfbf7a75dcf0657e8dde6d9710a",
        "commit_message": "HDDS-1281. Fix the findbug issue caused by HDDS-1163. Contributed by Aravindan Vijayan.",
        "commit_url": "https://github.com/apache/hadoop/commit/926d548caabdfcfbf7a75dcf0657e8dde6d9710a",
        "buggy_code": "containerID, containerData);",
        "fixed_code": "containerID);",
        "patch": "@@ -111,7 +111,7 @@ public TestKeyValueContainerCheck(String metadataImpl) {\n \n     KeyValueContainerCheck kvCheck =\n         new KeyValueContainerCheck(containerData.getMetadataPath(), conf,\n-            containerID, containerData);\n+            containerID);\n \n     // first run checks on a Open Container\n     error = kvCheck.fastCheck();"
    },
    {
        "commit_id": "86d508c7c724bd31a81066126ffc675d36d9df0a",
        "commit_message": "HDDS-1087. Fix TestDefaultCertificateClient#testSignDataStream. Contributed by Xiaoyu Yao. (#596)",
        "commit_url": "https://github.com/apache/hadoop/commit/86d508c7c724bd31a81066126ffc675d36d9df0a",
        "buggy_code": "String data = RandomStringUtils.random(100);",
        "fixed_code": "String data = RandomStringUtils.random(100, UTF);",
        "patch": "@@ -152,7 +152,7 @@ private X509Certificate generateX509Cert(KeyPair keyPair) throws Exception {\n \n   @Test\n   public void testSignDataStream() throws Exception {\n-    String data = RandomStringUtils.random(100);\n+    String data = RandomStringUtils.random(100, UTF);\n     // Expect error when there is no private key to sign.\n     LambdaTestUtils.intercept(IOException.class, \"Error while \" +\n             \"signing the stream\","
    },
    {
        "commit_id": "b4aa24d3c5ad1b9309a58795e4b48e567695c4e4",
        "commit_message": "HDDS-1173. Fix a data corruption bug in BlockOutputStream. Contributed by Shashikant Banerjee.",
        "commit_url": "https://github.com/apache/hadoop/commit/b4aa24d3c5ad1b9309a58795e4b48e567695c4e4",
        "buggy_code": ".setType(HddsProtos.ReplicationType.STAND_ALONE)",
        "fixed_code": ".setType(HddsProtos.ReplicationType.RATIS)",
        "patch": "@@ -142,7 +142,7 @@ public void testBlockDeletion() throws Exception {\n \n     OmKeyArgs keyArgs = new OmKeyArgs.Builder().setVolumeName(volumeName)\n         .setBucketName(bucketName).setKeyName(keyName).setDataSize(0)\n-        .setType(HddsProtos.ReplicationType.STAND_ALONE)\n+        .setType(HddsProtos.ReplicationType.RATIS)\n         .setFactor(HddsProtos.ReplicationFactor.ONE).build();\n     List<OmKeyLocationInfoGroup> omKeyLocationInfoGroupList =\n         om.lookupKey(keyArgs).getKeyLocationVersions();"
    },
    {
        "commit_id": "490206e4b4fbd4869940c9689e414bdc977aa405",
        "commit_message": "HDDS-1155.Fix failing unit test methods of TestDeadNodeHandler.\nContributed by Nandakumar.",
        "commit_url": "https://github.com/apache/hadoop/commit/490206e4b4fbd4869940c9689e414bdc977aa405",
        "buggy_code": ".allocateContainer(HddsProtos.ReplicationType.STAND_ALONE,",
        "fixed_code": ".allocateContainer(HddsProtos.ReplicationType.RATIS,",
        "patch": "@@ -445,7 +445,7 @@ public static CommandStatusReportsProto createCommandStatusReport(\n       allocateContainer(ContainerManager containerManager)\n       throws IOException {\n     return containerManager\n-        .allocateContainer(HddsProtos.ReplicationType.STAND_ALONE,\n+        .allocateContainer(HddsProtos.ReplicationType.RATIS,\n             HddsProtos.ReplicationFactor.THREE, \"root\");\n \n   }"
    },
    {
        "commit_id": "c1e5b1921235316780dc439293b5fa1d3718ba3a",
        "commit_message": "HDDS-1147. Fix failing unit tests in TestOzoneManager.\nContributed by Nandakumar.",
        "commit_url": "https://github.com/apache/hadoop/commit/c1e5b1921235316780dc439293b5fa1d3718ba3a",
        "buggy_code": "OMResponse omResponse = handleError(submitRequest(omRequest));",
        "fixed_code": "OMResponse omResponse = submitRequest(omRequest);",
        "patch": "@@ -292,7 +292,7 @@ public boolean checkVolumeAccess(String volume, OzoneAclInfo userAcl) throws\n         .setCheckVolumeAccessRequest(req)\n         .build();\n \n-    OMResponse omResponse = handleError(submitRequest(omRequest));\n+    OMResponse omResponse = submitRequest(omRequest);\n \n     if (omResponse.getStatus() == ACCESS_DENIED) {\n       return false;"
    },
    {
        "commit_id": "c1e5b1921235316780dc439293b5fa1d3718ba3a",
        "commit_message": "HDDS-1147. Fix failing unit tests in TestOzoneManager.\nContributed by Nandakumar.",
        "commit_url": "https://github.com/apache/hadoop/commit/c1e5b1921235316780dc439293b5fa1d3718ba3a",
        "buggy_code": "throw new OMException(\"Key not found\",",
        "fixed_code": "throw new OMException(\"Key already exists\",",
        "patch": "@@ -634,7 +634,7 @@ public void renameKey(OmKeyArgs args, String toKeyName) throws IOException {\n             \"Rename key failed for volume:{} bucket:{} fromKey:{} toKey:{}. \"\n                 + \"Key: {} already exists.\", volumeName, bucketName,\n             fromKeyName, toKeyName, toKeyName);\n-        throw new OMException(\"Key not found\",\n+        throw new OMException(\"Key already exists\",\n             OMException.ResultCodes.KEY_ALREADY_EXISTS);\n       }\n "
    },
    {
        "commit_id": "d33f0666f66e40ddcf453705566e64d5bfaf684a",
        "commit_message": "HDDS-1141. Update DBCheckpointSnapshot to DBCheckpoint. \n\n* HDDS-1141.Update DBCheckpointSnapshot to DBCheckpoint.\r\n\r\n* fix test failures in TestOzoneConfigurationFields",
        "commit_url": "https://github.com/apache/hadoop/commit/d33f0666f66e40ddcf453705566e64d5bfaf684a",
        "buggy_code": "public interface DBCheckpointSnapshot {",
        "fixed_code": "public interface DBCheckpoint {",
        "patch": "@@ -25,7 +25,7 @@\n /**\n  * Generic DB Checkpoint interface.\n  */\n-public interface DBCheckpointSnapshot {\n+public interface DBCheckpoint {\n \n   /**\n    * Get Snapshot location."
    },
    {
        "commit_id": "d33f0666f66e40ddcf453705566e64d5bfaf684a",
        "commit_message": "HDDS-1141. Update DBCheckpointSnapshot to DBCheckpoint. \n\n* HDDS-1141.Update DBCheckpointSnapshot to DBCheckpoint.\r\n\r\n* fix test failures in TestOzoneConfigurationFields",
        "commit_url": "https://github.com/apache/hadoop/commit/d33f0666f66e40ddcf453705566e64d5bfaf684a",
        "buggy_code": "DBCheckpointSnapshot getCheckpointSnapshot(boolean flush) throws IOException;",
        "fixed_code": "DBCheckpoint getCheckpoint(boolean flush) throws IOException;",
        "patch": "@@ -143,6 +143,6 @@ <KEY, VALUE> void move(KEY sourceKey, KEY destKey, VALUE value,\n    * @return An object that encapsulates the checkpoint information along with\n    * location.\n    */\n-  DBCheckpointSnapshot getCheckpointSnapshot(boolean flush) throws IOException;\n+  DBCheckpoint getCheckpoint(boolean flush) throws IOException;\n \n }"
    },
    {
        "commit_id": "d33f0666f66e40ddcf453705566e64d5bfaf684a",
        "commit_message": "HDDS-1141. Update DBCheckpointSnapshot to DBCheckpoint. \n\n* HDDS-1141.Update DBCheckpointSnapshot to DBCheckpoint.\r\n\r\n* fix test failures in TestOzoneConfigurationFields",
        "commit_url": "https://github.com/apache/hadoop/commit/d33f0666f66e40ddcf453705566e64d5bfaf684a",
        "buggy_code": "addServlet(\"dbSnapshot\", \"/dbSnapshot\", OMDbSnapshotServlet.class);",
        "fixed_code": "addServlet(\"dbCheckpoint\", \"/dbCheckpoint\", OMDBCheckpointServlet.class);",
        "patch": "@@ -32,7 +32,7 @@ public OzoneManagerHttpServer(Configuration conf, OzoneManager om)\n       throws IOException {\n     super(conf, \"ozoneManager\");\n     addServlet(\"serviceList\", \"/serviceList\", ServiceListJSONServlet.class);\n-    addServlet(\"dbSnapshot\", \"/dbSnapshot\", OMDbSnapshotServlet.class);\n+    addServlet(\"dbCheckpoint\", \"/dbCheckpoint\", OMDBCheckpointServlet.class);\n     getWebAppContext().setAttribute(OzoneConsts.OM_CONTEXT_ATTRIBUTE, om);\n   }\n "
    },
    {
        "commit_id": "e8d7e3b4e67e475f836b06180bd1f760d327f4bf",
        "commit_message": "HDDS-1139 : Fix findbugs issues caused by HDDS-1085. Contributed by Aravindan Vijayan.",
        "commit_url": "https://github.com/apache/hadoop/commit/e8d7e3b4e67e475f836b06180bd1f760d327f4bf",
        "buggy_code": "class RocksDBCheckpointSnapshot implements DBCheckpointSnapshot {",
        "fixed_code": "static class RocksDBCheckpointSnapshot implements DBCheckpointSnapshot {",
        "patch": "@@ -93,7 +93,7 @@ public RocksDBCheckpointSnapshot createCheckpointSnapshot(String parentDir)\n     return null;\n   }\n \n-  class RocksDBCheckpointSnapshot implements DBCheckpointSnapshot {\n+  static class RocksDBCheckpointSnapshot implements DBCheckpointSnapshot {\n \n     private Path checkpointLocation;\n     private long checkpointTimestamp;"
    },
    {
        "commit_id": "0f2b65c3da44f81be0f1973233e4cc10819c5e7b",
        "commit_message": "HADOOP-16116. Fix Spelling Mistakes - DECOMISSIONED. Contributed by BELUGA BEHR.",
        "commit_url": "https://github.com/apache/hadoop/commit/0f2b65c3da44f81be0f1973233e4cc10819c5e7b",
        "buggy_code": "public void testDecomissionedNMsMetricsOnRMRestart() throws Exception {",
        "fixed_code": "public void testDecommissionedNMsMetricsOnRMRestart() throws Exception {",
        "patch": "@@ -2019,7 +2019,7 @@ private void assertQueueMetrics(QueueMetrics qm, int appsSubmitted,\n   }\n \n   @Test (timeout = 60000)\n-  public void testDecomissionedNMsMetricsOnRMRestart() throws Exception {\n+  public void testDecommissionedNMsMetricsOnRMRestart() throws Exception {\n     conf.set(YarnConfiguration.RM_NODES_EXCLUDE_FILE_PATH,\n       hostFile.getAbsolutePath());\n     writeToHostsFile(\"\");"
    },
    {
        "commit_id": "75e15cc0c4c237e7f94e8cd2ea1dde0773e954b4",
        "commit_message": "HDDS-1103.Fix rat/findbug/checkstyle errors in ozone/hdds projects.\nContributed by Elek, Marton.",
        "commit_url": "https://github.com/apache/hadoop/commit/75e15cc0c4c237e7f94e8cd2ea1dde0773e954b4",
        "buggy_code": "TimeoutFuture<V> timeoutFutureRef;",
        "fixed_code": "private TimeoutFuture<V> timeoutFutureRef;",
        "patch": "@@ -94,7 +94,7 @@ private TimeoutFuture(ListenableFuture<V> delegate) {\n    */\n   private static final class Fire<V> implements Runnable {\n     @Nullable\n-    TimeoutFuture<V> timeoutFutureRef;\n+    private TimeoutFuture<V> timeoutFutureRef;\n \n     Fire(\n         TimeoutFuture<V> timeoutFuture) {"
    },
    {
        "commit_id": "75e15cc0c4c237e7f94e8cd2ea1dde0773e954b4",
        "commit_message": "HDDS-1103.Fix rat/findbug/checkstyle errors in ozone/hdds projects.\nContributed by Elek, Marton.",
        "commit_url": "https://github.com/apache/hadoop/commit/75e15cc0c4c237e7f94e8cd2ea1dde0773e954b4",
        "buggy_code": "private static String SCM_ID = UUID.randomUUID().toString();",
        "fixed_code": "private static final String SCM_ID = UUID.randomUUID().toString();",
        "patch": "@@ -76,7 +76,7 @@ public class TestReadRetries {\n   private static StorageContainerLocationProtocolClientSideTranslatorPB\n       storageContainerLocationClient;\n \n-  private static String SCM_ID = UUID.randomUUID().toString();\n+  private static final String SCM_ID = UUID.randomUUID().toString();\n \n \n   /**"
    },
    {
        "commit_id": "0395f22145d90d38895a7a3e220a15718b1e2399",
        "commit_message": "HDDS-1068. Improve the error propagation for ozone sh.\nContributed by Elek, Marton.",
        "commit_url": "https://github.com/apache/hadoop/commit/0395f22145d90d38895a7a3e220a15718b1e2399",
        "buggy_code": "public void testCreateDuplicateVolume() throws OzoneException, IOException {",
        "fixed_code": "public void testCreateDuplicateVolume() throws Exception {",
        "patch": "@@ -107,7 +107,7 @@ public void testCreateVolume() throws Exception {\n   }\n \n   @Test\n-  public void testCreateDuplicateVolume() throws OzoneException, IOException {\n+  public void testCreateDuplicateVolume() throws Exception {\n     TestVolume.runTestCreateDuplicateVolume(client);\n   }\n "
    },
    {
        "commit_id": "df7b7dadf94dbc196297c607ca87cbc87d72ee4c",
        "commit_message": "HDDS-1073. Fix FindBugs issues on OzoneBucketStub#createMultipartKey. Contributed by Aravindan Vijayan.",
        "commit_url": "https://github.com/apache/hadoop/commit/df7b7dadf94dbc196297c607ca87cbc87d72ee4c",
        "buggy_code": "if (multipartUploadID == null || multipartUploadID != uploadID) {",
        "fixed_code": "if (multipartUploadID == null || !multipartUploadID.equals(uploadID)) {",
        "patch": "@@ -176,7 +176,7 @@ public OzoneOutputStream createMultipartKey(String key, long size,\n                                               int partNumber, String uploadID)\n       throws IOException {\n     String multipartUploadID = multipartUploadIdMap.get(key);\n-    if (multipartUploadID == null || multipartUploadID != uploadID) {\n+    if (multipartUploadID == null || !multipartUploadID.equals(uploadID)) {\n       throw new IOException(\"NO_SUCH_MULTIPART_UPLOAD_ERROR\");\n     } else {\n       ByteArrayOutputStream byteArrayOutputStream ="
    },
    {
        "commit_id": "0faa5701d92d07129e4b66cb2d2ad9dc559c63d7",
        "commit_message": "HDDS-964. Fix test failure in TestOmMetrics. Contributed by Ajay Kumar.",
        "commit_url": "https://github.com/apache/hadoop/commit/0faa5701d92d07129e4b66cb2d2ad9dc559c63d7",
        "buggy_code": "ozoneManager.start();",
        "fixed_code": "ozoneManager.restart();",
        "patch": "@@ -246,7 +246,7 @@ public void restartStorageContainerManager()\n   @Override\n   public void restartOzoneManager() throws IOException {\n     ozoneManager.stop();\n-    ozoneManager.start();\n+    ozoneManager.restart();\n   }\n \n   @Override"
    },
    {
        "commit_id": "9920506b3d55cae2bf5eecf361d331f0ea83c426",
        "commit_message": "HDDS-547. Fix secure docker and configs. Contributed by Xiaoyu Yao.",
        "commit_url": "https://github.com/apache/hadoop/commit/9920506b3d55cae2bf5eecf361d331f0ea83c426",
        "buggy_code": "import static org.apache.hadoop.ozone.OzoneConfigKeys.OZONE_METADATA_DIRS;",
        "fixed_code": "import static org.apache.hadoop.hdds.HddsConfigKeys.OZONE_METADATA_DIRS;",
        "patch": "@@ -48,7 +48,7 @@\n import static org.apache.hadoop.hdds.HddsConfigKeys.HDDS_X509_MAX_DURATION_DEFAULT;\n import static org.apache.hadoop.hdds.HddsConfigKeys.HDDS_X509_SIGNATURE_ALGO;\n import static org.apache.hadoop.hdds.HddsConfigKeys.HDDS_X509_SIGNATURE_ALGO_DEFAULT;\n-import static org.apache.hadoop.ozone.OzoneConfigKeys.OZONE_METADATA_DIRS;\n+import static org.apache.hadoop.hdds.HddsConfigKeys.OZONE_METADATA_DIRS;\n \n /**\n  * A class that deals with all Security related configs in HDDS."
    },
    {
        "commit_id": "9920506b3d55cae2bf5eecf361d331f0ea83c426",
        "commit_message": "HDDS-547. Fix secure docker and configs. Contributed by Xiaoyu Yao.",
        "commit_url": "https://github.com/apache/hadoop/commit/9920506b3d55cae2bf5eecf361d331f0ea83c426",
        "buggy_code": "import static org.apache.hadoop.ozone.OzoneConfigKeys.OZONE_METADATA_DIRS;",
        "fixed_code": "import static org.apache.hadoop.hdds.HddsConfigKeys.OZONE_METADATA_DIRS;",
        "patch": "@@ -46,7 +46,7 @@\n import java.util.Date;\n import java.util.UUID;\n \n-import static org.apache.hadoop.ozone.OzoneConfigKeys.OZONE_METADATA_DIRS;\n+import static org.apache.hadoop.hdds.HddsConfigKeys.OZONE_METADATA_DIRS;\n \n /**\n  * Test Class for Root Certificate generation."
    },
    {
        "commit_id": "9920506b3d55cae2bf5eecf361d331f0ea83c426",
        "commit_message": "HDDS-547. Fix secure docker and configs. Contributed by Xiaoyu Yao.",
        "commit_url": "https://github.com/apache/hadoop/commit/9920506b3d55cae2bf5eecf361d331f0ea83c426",
        "buggy_code": "import static org.apache.hadoop.ozone.OzoneConfigKeys.OZONE_METADATA_DIRS;",
        "fixed_code": "import static org.apache.hadoop.hdds.HddsConfigKeys.OZONE_METADATA_DIRS;",
        "patch": "@@ -19,7 +19,7 @@\n \n package org.apache.hadoop.hdds.security.x509.keys;\n \n-import static org.apache.hadoop.ozone.OzoneConfigKeys.OZONE_METADATA_DIRS;\n+import static org.apache.hadoop.hdds.HddsConfigKeys.OZONE_METADATA_DIRS;\n import java.security.KeyPair;\n import java.security.NoSuchAlgorithmException;\n import java.security.NoSuchProviderException;"
    },
    {
        "commit_id": "e47135d9d9759a2eeef99fc8b8df70fcd2f143d6",
        "commit_message": "HDDS-70. Fix config names for secure ksm and scm. Contributed by Ajay Kumar.",
        "commit_url": "https://github.com/apache/hadoop/commit/e47135d9d9759a2eeef99fc8b8df70fcd2f143d6",
        "buggy_code": "@KerberosInfo(serverPrincipal = ScmConfigKeys.OZONE_SCM_KERBEROS_PRINCIPAL_KEY)",
        "fixed_code": "@KerberosInfo(serverPrincipal = ScmConfigKeys.HDDS_SCM_KERBEROS_PRINCIPAL_KEY)",
        "patch": "@@ -33,7 +33,7 @@\n  * ScmBlockLocationProtocol is used by an HDFS node to find the set of nodes\n  * to read/write a block.\n  */\n-@KerberosInfo(serverPrincipal = ScmConfigKeys.OZONE_SCM_KERBEROS_PRINCIPAL_KEY)\n+@KerberosInfo(serverPrincipal = ScmConfigKeys.HDDS_SCM_KERBEROS_PRINCIPAL_KEY)\n public interface ScmBlockLocationProtocol {\n \n   /**"
    },
    {
        "commit_id": "e47135d9d9759a2eeef99fc8b8df70fcd2f143d6",
        "commit_message": "HDDS-70. Fix config names for secure ksm and scm. Contributed by Ajay Kumar.",
        "commit_url": "https://github.com/apache/hadoop/commit/e47135d9d9759a2eeef99fc8b8df70fcd2f143d6",
        "buggy_code": "serverPrincipal = ScmConfigKeys.OZONE_SCM_KERBEROS_PRINCIPAL_KEY)",
        "fixed_code": "serverPrincipal = ScmConfigKeys.HDDS_SCM_KERBEROS_PRINCIPAL_KEY)",
        "patch": "@@ -33,7 +33,7 @@\n     \"org.apache.hadoop.ozone.protocol.StorageContainerLocationProtocol\",\n     protocolVersion = 1)\n @KerberosInfo(\n-    serverPrincipal = ScmConfigKeys.OZONE_SCM_KERBEROS_PRINCIPAL_KEY)\n+    serverPrincipal = ScmConfigKeys.HDDS_SCM_KERBEROS_PRINCIPAL_KEY)\n @InterfaceAudience.Private\n public interface StorageContainerLocationProtocolPB\n     extends StorageContainerLocationProtocolService.BlockingInterface {"
    },
    {
        "commit_id": "e47135d9d9759a2eeef99fc8b8df70fcd2f143d6",
        "commit_message": "HDDS-70. Fix config names for secure ksm and scm. Contributed by Ajay Kumar.",
        "commit_url": "https://github.com/apache/hadoop/commit/e47135d9d9759a2eeef99fc8b8df70fcd2f143d6",
        "buggy_code": "serverPrincipal = ScmConfigKeys.OZONE_SCM_KERBEROS_PRINCIPAL_KEY)",
        "fixed_code": "serverPrincipal = ScmConfigKeys.HDDS_SCM_KERBEROS_PRINCIPAL_KEY)",
        "patch": "@@ -44,7 +44,7 @@\n  * Protoc file that defines this protocol.\n  */\n @KerberosInfo(\n-    serverPrincipal = ScmConfigKeys.OZONE_SCM_KERBEROS_PRINCIPAL_KEY)\n+    serverPrincipal = ScmConfigKeys.HDDS_SCM_KERBEROS_PRINCIPAL_KEY)\n @InterfaceAudience.Private\n public interface StorageContainerDatanodeProtocol {\n   /**"
    },
    {
        "commit_id": "e47135d9d9759a2eeef99fc8b8df70fcd2f143d6",
        "commit_message": "HDDS-70. Fix config names for secure ksm and scm. Contributed by Ajay Kumar.",
        "commit_url": "https://github.com/apache/hadoop/commit/e47135d9d9759a2eeef99fc8b8df70fcd2f143d6",
        "buggy_code": "serverPrincipal = ScmConfigKeys.OZONE_SCM_KERBEROS_PRINCIPAL_KEY,",
        "fixed_code": "serverPrincipal = ScmConfigKeys.HDDS_SCM_KERBEROS_PRINCIPAL_KEY,",
        "patch": "@@ -33,7 +33,7 @@\n     \"org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol\",\n     protocolVersion = 1)\n @KerberosInfo(\n-    serverPrincipal = ScmConfigKeys.OZONE_SCM_KERBEROS_PRINCIPAL_KEY,\n+    serverPrincipal = ScmConfigKeys.HDDS_SCM_KERBEROS_PRINCIPAL_KEY,\n     clientPrincipal = DFSConfigKeys.DFS_DATANODE_KERBEROS_PRINCIPAL_KEY)\n public interface StorageContainerDatanodeProtocolPB extends\n     StorageContainerDatanodeProtocolService.BlockingInterface {"
    },
    {
        "commit_id": "e47135d9d9759a2eeef99fc8b8df70fcd2f143d6",
        "commit_message": "HDDS-70. Fix config names for secure ksm and scm. Contributed by Ajay Kumar.",
        "commit_url": "https://github.com/apache/hadoop/commit/e47135d9d9759a2eeef99fc8b8df70fcd2f143d6",
        "buggy_code": "@KerberosInfo(serverPrincipal = ScmConfigKeys.OZONE_SCM_KERBEROS_PRINCIPAL_KEY)",
        "fixed_code": "@KerberosInfo(serverPrincipal = ScmConfigKeys.HDDS_SCM_KERBEROS_PRINCIPAL_KEY)",
        "patch": "@@ -43,7 +43,7 @@\n  * includes: {@link org.apache.hadoop.ozone.client.rpc.RpcClient} for RPC and\n  * {@link  org.apache.hadoop.ozone.client.rest.RestClient} for REST.\n  */\n-@KerberosInfo(serverPrincipal = ScmConfigKeys.OZONE_SCM_KERBEROS_PRINCIPAL_KEY)\n+@KerberosInfo(serverPrincipal = ScmConfigKeys.HDDS_SCM_KERBEROS_PRINCIPAL_KEY)\n public interface ClientProtocol {\n \n   /**"
    },
    {
        "commit_id": "396ffba1aa108d0f36625d6c788381b77b03b9ba",
        "commit_message": "HDDS-968. Fix TestObjectPut failures. Contributed by Bharat Viswanadham.",
        "commit_url": "https://github.com/apache/hadoop/commit/396ffba1aa108d0f36625d6c788381b77b03b9ba",
        "buggy_code": "if (!uploadID.equals(\"\")) {",
        "fixed_code": "if (uploadID != null && !uploadID.equals(\"\")) {",
        "patch": "@@ -118,7 +118,7 @@ public Response put(\n \n     OzoneOutputStream output = null;\n \n-    if (!uploadID.equals(\"\")) {\n+    if (uploadID != null && !uploadID.equals(\"\")) {\n       // If uploadID is specified, it is a request for upload part\n       return createMultipartKey(bucketName, keyPath, length,\n           partNumber, uploadID, body);"
    },
    {
        "commit_id": "6e35f7130fb3fb17665e818f838ed750425348c0",
        "commit_message": "YARN-9166. Fix logging for preemption of Opportunistic containers for Guaranteed containers. Contributed by Abhishek Modi.",
        "commit_url": "https://github.com/apache/hadoop/commit/6e35f7130fb3fb17665e818f838ed750425348c0",
        "buggy_code": "\"resumed\";",
        "fixed_code": "\"killed\";",
        "patch": "@@ -518,7 +518,7 @@ private void reclaimOpportunisticContainerResources(Container container) {\n     // Kill the opportunistic containers that were chosen.\n     for (Container contToReclaim : extraOppContainersToReclaim) {\n       String preemptionAction = usePauseEventForPreemption == true ? \"paused\" :\n-          \"resumed\";\n+          \"killed\";\n       LOG.info(\n           \"Container {} will be {} to start the \"\n               + \"execution of guaranteed container {}.\","
    },
    {
        "commit_id": "fa8550337d082afba025fd2714c6f78721a4e729",
        "commit_message": "HDFS-14149. [SBN read] Fix annotations on new interfaces/classes for SBN reads. Contributed by Chao Sun.",
        "commit_url": "https://github.com/apache/hadoop/commit/fa8550337d082afba025fd2714c6f78721a4e729",
        "buggy_code": "@InterfaceStability.Stable",
        "fixed_code": "@InterfaceStability.Evolving",
        "patch": "@@ -34,7 +34,7 @@\n  * to client.\n  */\n @InterfaceAudience.Private\n-@InterfaceStability.Stable\n+@InterfaceStability.Evolving\n public interface AlignmentContext {\n \n   /**"
    },
    {
        "commit_id": "fa8550337d082afba025fd2714c6f78721a4e729",
        "commit_message": "HDFS-14149. [SBN read] Fix annotations on new interfaces/classes for SBN reads. Contributed by Chao Sun.",
        "commit_url": "https://github.com/apache/hadoop/commit/fa8550337d082afba025fd2714c6f78721a4e729",
        "buggy_code": "@InterfaceStability.Stable",
        "fixed_code": "@InterfaceStability.Evolving",
        "patch": "@@ -34,7 +34,7 @@\n  * state alignment info from server(s).\n  */\n @InterfaceAudience.Private\n-@InterfaceStability.Stable\n+@InterfaceStability.Evolving\n public class ClientGSIContext implements AlignmentContext {\n \n   private final LongAccumulator lastSeenStateId ="
    },
    {
        "commit_id": "fa8550337d082afba025fd2714c6f78721a4e729",
        "commit_message": "HDFS-14149. [SBN read] Fix annotations on new interfaces/classes for SBN reads. Contributed by Chao Sun.",
        "commit_url": "https://github.com/apache/hadoop/commit/fa8550337d082afba025fd2714c6f78721a4e729",
        "buggy_code": "@InterfaceStability.Stable",
        "fixed_code": "@InterfaceStability.Evolving",
        "patch": "@@ -37,7 +37,7 @@\n  * state alignment info to clients.\n  */\n @InterfaceAudience.Private\n-@InterfaceStability.Stable\n+@InterfaceStability.Evolving\n class GlobalStateIdContext implements AlignmentContext {\n   /**\n    * Estimated number of journal transactions a typical NameNode can execute"
    },
    {
        "commit_id": "3bb92a1d9a7a3e71bbd3b96d9adfd0e2db4485bc",
        "commit_message": "HDFS-14094. [SBN read] Fix the order of logging arguments in ObserverReadProxyProvider. Contributed by Ayush Saxena.",
        "commit_url": "https://github.com/apache/hadoop/commit/3bb92a1d9a7a3e71bbd3b96d9adfd0e2db4485bc",
        "buggy_code": "failedObserverCount, standbyCount, activeCount, method.getName());",
        "fixed_code": "failedObserverCount, method.getName(), standbyCount, activeCount);",
        "patch": "@@ -302,7 +302,7 @@ public Object invoke(Object proxy, final Method method, final Object[] args)\n         // If we get here, it means all observers have failed.\n         LOG.warn(\"{} observers have failed for read request {}; also found \" +\n             \"{} standby and {} active. Falling back to active.\",\n-            failedObserverCount, standbyCount, activeCount, method.getName());\n+            failedObserverCount, method.getName(), standbyCount, activeCount);\n       }\n \n       // Either all observers have failed, or that it is a write request."
    },
    {
        "commit_id": "085f10e75dea5446861253cf63aced337536481c",
        "commit_message": "HADOOP-15947. Fix ITestDynamoDBMetadataStore test error issues. Contributed by Gabor Bota.",
        "commit_url": "https://github.com/apache/hadoop/commit/085f10e75dea5446861253cf63aced337536481c",
        "buggy_code": "return (metas.isEmpty() || dirPathMeta == null)",
        "fixed_code": "return (metas.isEmpty() && dirPathMeta == null)",
        "patch": "@@ -633,7 +633,7 @@ public DirListingMetadata listChildren(final Path path) throws IOException {\n           LOG.trace(\"Listing table {} in region {} for {} returning {}\",\n               tableName, region, path, metas);\n \n-          return (metas.isEmpty() || dirPathMeta == null)\n+          return (metas.isEmpty() && dirPathMeta == null)\n               ? null\n               : new DirListingMetadata(path, metas, isAuthoritative,\n               dirPathMeta.getLastUpdated());"
    },
    {
        "commit_id": "0d8406135f8b8ac427fd7f49f5faf20064ace121",
        "commit_message": "YARN-9054. Fix FederationStateStoreFacade#buildGetSubClustersCacheRequest. Contributed by Bibin A Chundatt.",
        "commit_url": "https://github.com/apache/hadoop/commit/0d8406135f8b8ac427fd7f49f5faf20064ace121",
        "buggy_code": "private void deRegisterSubCluster(SubClusterId subClusterId)",
        "fixed_code": "public void deRegisterSubCluster(SubClusterId subClusterId)",
        "patch": "@@ -172,7 +172,7 @@ public void deregisterAllSubClusters() throws YarnException {\n     }\n   }\n \n-  private void deRegisterSubCluster(SubClusterId subClusterId)\n+  public void deRegisterSubCluster(SubClusterId subClusterId)\n       throws YarnException {\n     stateStore.deregisterSubCluster(SubClusterDeregisterRequest\n         .newInstance(subClusterId, SubClusterState.SC_UNREGISTERED));"
    },
    {
        "commit_id": "f3f5e7ad005a88afad6fa09602073eaa450e21ed",
        "commit_message": "HDFS-14042. Fix NPE when PROVIDED storage is missing. Contributed by Virajith Jalaparti.",
        "commit_url": "https://github.com/apache/hadoop/commit/f3f5e7ad005a88afad6fa09602073eaa450e21ed",
        "buggy_code": "node.updateHeartbeatState(reports, cacheCapacity, cacheUsed,",
        "fixed_code": "blockManager.updateHeartbeatState(node, reports, cacheCapacity, cacheUsed,",
        "patch": "@@ -251,7 +251,7 @@ synchronized void updateLifeline(final DatanodeDescriptor node,\n     // updateHeartbeat, because we don't want to modify the\n     // heartbeatedSinceRegistration flag.  Arrival of a lifeline message does\n     // not count as arrival of the first heartbeat.\n-    node.updateHeartbeatState(reports, cacheCapacity, cacheUsed,\n+    blockManager.updateHeartbeatState(node, reports, cacheCapacity, cacheUsed,\n         xceiverCount, failedVolumes, volumeFailureSummary);\n     stats.add(node);\n   }"
    },
    {
        "commit_id": "50f40e0536f38517aa33e8859f299bcf19f2f319",
        "commit_message": "HDDS-794. addendum patch to fix compilation failure. Contributed by Shashikant Banerjee.",
        "commit_url": "https://github.com/apache/hadoop/commit/50f40e0536f38517aa33e8859f299bcf19f2f319",
        "buggy_code": "data.length);",
        "fixed_code": "bufferSize);",
        "patch": "@@ -139,7 +139,7 @@ public static void writeData(File chunkFile, ChunkInfo chunkInfo,\n       }\n     }\n     log.debug(\"Write Chunk completed for chunkFile: {}, size {}\", chunkFile,\n-        data.length);\n+        bufferSize);\n   }\n \n   /**"
    },
    {
        "commit_id": "0b62983c5a9361eb832784f134f140f9926c9ec6",
        "commit_message": "YARN-8826. Fix lingering timeline collector after serviceStop in TimelineCollectorManager. Contributed by Prabha Manepalli.",
        "commit_url": "https://github.com/apache/hadoop/commit/0b62983c5a9361eb832784f134f140f9926c9ec6",
        "buggy_code": "if (collectors != null && collectors.size() > 1) {",
        "fixed_code": "if (collectors != null && collectors.size() > 0) {",
        "patch": "@@ -220,7 +220,7 @@ public boolean containsTimelineCollector(ApplicationId appId) {\n \n   @Override\n   protected void serviceStop() throws Exception {\n-    if (collectors != null && collectors.size() > 1) {\n+    if (collectors != null && collectors.size() > 0) {\n       synchronized (collectors) {\n         for (TimelineCollector c : collectors.values()) {\n           c.serviceStop();"
    },
    {
        "commit_id": "f6498af0d7618c580ecfbc77aff9946362efe4f3",
        "commit_message": "HDDS-705. addendum patch to fix find bug issue. Contributed by Bharat Viswanadham.",
        "commit_url": "https://github.com/apache/hadoop/commit/f6498af0d7618c580ecfbc77aff9946362efe4f3",
        "buggy_code": "\"NoSuchObject\", \"The specified key does not exist\", HTTP_NOT_FOUND);",
        "fixed_code": "\"NoSuchKey\", \"The specified key does not exist\", HTTP_NOT_FOUND);",
        "patch": "@@ -50,7 +50,7 @@ private S3ErrorTable() {\n       \"is invalid.\", HTTP_NOT_FOUND);\n \n   public static final OS3Exception NO_SUCH_KEY = new OS3Exception(\n-      \"NoSuchObject\", \"The specified key does not exist\", HTTP_NOT_FOUND);\n+      \"NoSuchKey\", \"The specified key does not exist\", HTTP_NOT_FOUND);\n \n   /**\n    * Create a new instance of Error."
    },
    {
        "commit_id": "97a41b3dbeb42653c82559cd07ec4b7d7d709377",
        "commit_message": "HDDS-705. OS3Exception resource name should be the actual resource name.\nContributed by Bharat Viswanadham.\n\nRecommitting after making sure that patch is clean.",
        "commit_url": "https://github.com/apache/hadoop/commit/97a41b3dbeb42653c82559cd07ec4b7d7d709377",
        "buggy_code": "ex = S3ErrorTable.newError(ex, S3ErrorTable.Resource.BUCKET);",
        "fixed_code": "ex = S3ErrorTable.newError(ex, \"bucket\");",
        "patch": "@@ -32,7 +32,7 @@ public void testOS3Exception() {\n     OS3Exception ex = new OS3Exception(\"AccessDenied\", \"Access Denied\",\n         403);\n     String requestId = OzoneUtils.getRequestID();\n-    ex = S3ErrorTable.newError(ex, S3ErrorTable.Resource.BUCKET);\n+    ex = S3ErrorTable.newError(ex, \"bucket\");\n     ex.setRequestId(requestId);\n     String val = ex.toXml();\n     String formatString = \"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n\" +"
    },
    {
        "commit_id": "9146d33e1843524885938f60c77b47d4f52e80fb",
        "commit_message": "HDDS-670. Fix OzoneFS directory rename.",
        "commit_url": "https://github.com/apache/hadoop/commit/9146d33e1843524885938f60c77b47d4f52e80fb",
        "buggy_code": "if (dst.toString().startsWith(src.toString())) {",
        "fixed_code": "if (dst.toString().startsWith(src.toString() + OZONE_URI_DELIMITER)) {",
        "patch": "@@ -349,7 +349,7 @@ public boolean rename(Path src, Path dst) throws IOException {\n     }\n \n     if (srcStatus.isDirectory()) {\n-      if (dst.toString().startsWith(src.toString())) {\n+      if (dst.toString().startsWith(src.toString() + OZONE_URI_DELIMITER)) {\n         LOG.trace(\"Cannot rename a directory to a subdirectory of self\");\n         return false;\n       }"
    },
    {
        "commit_id": "3bfd214a59a60263aff67850c4d646c64fd76a01",
        "commit_message": "YARN-8810.  Fixed a YARN service bug in comparing ConfigFile object.\n            Contributed by Chandni Singh",
        "commit_url": "https://github.com/apache/hadoop/commit/3bfd214a59a60263aff67850c4d646c64fd76a01",
        "buggy_code": "currentDef.getConfiguration())) {",
        "fixed_code": "targetDef.getConfiguration())) {",
        "patch": "@@ -88,7 +88,7 @@ public List<Component> findTargetComponentSpecs(Service currentDef,\n       }\n \n       if (!Objects.equals(currentDef.getConfiguration(),\n-          currentDef.getConfiguration())) {\n+          targetDef.getConfiguration())) {\n         return targetDef.getComponents();\n       }\n "
    },
    {
        "commit_id": "cc5cc60c4162a2d788c80ebbbe69ca49f3eb90e6",
        "commit_message": "Fixing issue due to commit 2b2399d6 after rebase onto trunk.",
        "commit_url": "https://github.com/apache/hadoop/commit/cc5cc60c4162a2d788c80ebbbe69ca49f3eb90e6",
        "buggy_code": "import org.apache.commons.lang.ArrayUtils;",
        "fixed_code": "import org.apache.commons.lang3.ArrayUtils;",
        "patch": "@@ -40,7 +40,7 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-import org.apache.commons.lang.ArrayUtils;\n+import org.apache.commons.lang3.ArrayUtils;\n import org.apache.hadoop.classification.InterfaceStability;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.BlockLocation;"
    },
    {
        "commit_id": "b54b0c1b676c616aef9574e4e88ea30c314c79dc",
        "commit_message": "HADOOP-15659. Code changes for bug fix and new tests.\nContributed by Da Zhou.",
        "commit_url": "https://github.com/apache/hadoop/commit/b54b0c1b676c616aef9574e4e88ea30c314c79dc",
        "buggy_code": "import com.fasterxml.jackson.annotation.JsonProperty;",
        "fixed_code": "import org.codehaus.jackson.annotate.JsonProperty;",
        "patch": "@@ -18,7 +18,7 @@\n \n package org.apache.hadoop.fs.azurebfs.contracts.services;\n \n-import com.fasterxml.jackson.annotation.JsonProperty;\n+import org.codehaus.jackson.annotate.JsonProperty;\n \n import org.apache.hadoop.classification.InterfaceStability;\n "
    },
    {
        "commit_id": "b54b0c1b676c616aef9574e4e88ea30c314c79dc",
        "commit_message": "HADOOP-15659. Code changes for bug fix and new tests.\nContributed by Da Zhou.",
        "commit_url": "https://github.com/apache/hadoop/commit/b54b0c1b676c616aef9574e4e88ea30c314c79dc",
        "buggy_code": "import com.fasterxml.jackson.annotation.JsonProperty;",
        "fixed_code": "import org.codehaus.jackson.annotate.JsonProperty;",
        "patch": "@@ -20,7 +20,7 @@\n \n import java.util.List;\n \n-import com.fasterxml.jackson.annotation.JsonProperty;\n+import org.codehaus.jackson.annotate.JsonProperty;\n \n import org.apache.hadoop.classification.InterfaceStability;\n "
    },
    {
        "commit_id": "b54b0c1b676c616aef9574e4e88ea30c314c79dc",
        "commit_message": "HADOOP-15659. Code changes for bug fix and new tests.\nContributed by Da Zhou.",
        "commit_url": "https://github.com/apache/hadoop/commit/b54b0c1b676c616aef9574e4e88ea30c314c79dc",
        "buggy_code": "this.readAheadQueueDepth = (readAheadQueueDepth >= 0) ? readAheadQueueDepth : 2 * Runtime.getRuntime().availableProcessors();",
        "fixed_code": "this.readAheadQueueDepth = (readAheadQueueDepth >= 0) ? readAheadQueueDepth : Runtime.getRuntime().availableProcessors();",
        "patch": "@@ -64,7 +64,7 @@ public AbfsInputStream(\n     this.path = path;\n     this.contentLength = contentLength;\n     this.bufferSize = bufferSize;\n-    this.readAheadQueueDepth = (readAheadQueueDepth >= 0) ? readAheadQueueDepth : 2 * Runtime.getRuntime().availableProcessors();\n+    this.readAheadQueueDepth = (readAheadQueueDepth >= 0) ? readAheadQueueDepth : Runtime.getRuntime().availableProcessors();\n     this.eTag = eTag;\n     this.tolerateOobAppends = false;\n     this.readAheadEnabled = true;"
    },
    {
        "commit_id": "b54b0c1b676c616aef9574e4e88ea30c314c79dc",
        "commit_message": "HADOOP-15659. Code changes for bug fix and new tests.\nContributed by Da Zhou.",
        "commit_url": "https://github.com/apache/hadoop/commit/b54b0c1b676c616aef9574e4e88ea30c314c79dc",
        "buggy_code": "if (result.getStatusCode() > HttpURLConnection.HTTP_BAD_REQUEST) {",
        "fixed_code": "if (result.getStatusCode() >= HttpURLConnection.HTTP_BAD_REQUEST) {",
        "patch": "@@ -121,7 +121,7 @@ void execute() throws AzureBlobFileSystemException {\n       }\n     }\n \n-    if (result.getStatusCode() > HttpURLConnection.HTTP_BAD_REQUEST) {\n+    if (result.getStatusCode() >= HttpURLConnection.HTTP_BAD_REQUEST) {\n       throw new AbfsRestOperationException(result.getStatusCode(), result.getStorageErrorCode(),\n           result.getStorageErrorMessage(), null, result);\n     }"
    },
    {
        "commit_id": "b54b0c1b676c616aef9574e4e88ea30c314c79dc",
        "commit_message": "HADOOP-15659. Code changes for bug fix and new tests.\nContributed by Da Zhou.",
        "commit_url": "https://github.com/apache/hadoop/commit/b54b0c1b676c616aef9574e4e88ea30c314c79dc",
        "buggy_code": "try(final FSDataOutputStream stream = fs.create(TEST_FILE)) {",
        "fixed_code": "try (FSDataOutputStream stream = fs.create(TEST_FILE)) {",
        "patch": "@@ -108,7 +108,7 @@ public void testWriteWithBufferOffset() throws Exception {\n \n     final byte[] b = new byte[1024 * 1000];\n     new Random().nextBytes(b);\n-    try(final FSDataOutputStream stream = fs.create(TEST_FILE)) {\n+    try (FSDataOutputStream stream = fs.create(TEST_FILE)) {\n       stream.write(b, TEST_OFFSET, b.length - TEST_OFFSET);\n     }\n "
    },
    {
        "commit_id": "9a265fa673ef1b8774cfd69c76cdd29bf344e79d",
        "commit_message": "YARN-8782. Fix exception message in Resource.throwExceptionWhenArrayOutOfBound. Contributed by Gergely Pollak.",
        "commit_url": "https://github.com/apache/hadoop/commit/9a265fa673ef1b8774cfd69c76cdd29bf344e79d",
        "buggy_code": "+ \"Acceptable index range is [0,%d), please check double check \"",
        "fixed_code": "+ \"Acceptable index range is [0,%d), please double check \"",
        "patch": "@@ -397,7 +397,7 @@ public void setResourceValue(int index, long value)\n   protected void throwExceptionWhenArrayOutOfBound(int index) {\n     String exceptionMsg = String.format(\n         \"Trying to access ResourceInformation for given index=%d. \"\n-            + \"Acceptable index range is [0,%d), please check double check \"\n+            + \"Acceptable index range is [0,%d), please double check \"\n             + \"configured resources in resource-types.xml\",\n         index, ResourceUtils.getNumberOfKnownResourceTypes());\n "
    },
    {
        "commit_id": "c44088ac190e515b099183aeed4f9d6f8bee7da6",
        "commit_message": "YARN-8739. Fix jenkins issues for Node Attributes branch. Contributed by Sunil Govindan.",
        "commit_url": "https://github.com/apache/hadoop/commit/c44088ac190e515b099183aeed4f9d6f8bee7da6",
        "buggy_code": "public NodesToAttributesMappingResponse mapAttributesToNodes(",
        "fixed_code": "NodesToAttributesMappingResponse mapAttributesToNodes(",
        "patch": "@@ -150,7 +150,7 @@ public RefreshClusterMaxPriorityResponse refreshClusterMaxPriority(\n \n   @Private\n   @Idempotent\n-  public NodesToAttributesMappingResponse mapAttributesToNodes(\n+  NodesToAttributesMappingResponse mapAttributesToNodes(\n       NodesToAttributesMappingRequest request) throws YarnException,\n       IOException;\n }"
    },
    {
        "commit_id": "c44088ac190e515b099183aeed4f9d6f8bee7da6",
        "commit_message": "YARN-8739. Fix jenkins issues for Node Attributes branch. Contributed by Sunil Govindan.",
        "commit_url": "https://github.com/apache/hadoop/commit/c44088ac190e515b099183aeed4f9d6f8bee7da6",
        "buggy_code": "Mockito.verify(store.getFs(),Mockito.times(",
        "fixed_code": "Mockito.verify(store.getFs(), Mockito.times(",
        "patch": "@@ -360,7 +360,7 @@ private void verifyMkdirsCount(FileSystemNodeLabelsStore store,\n     Mockito.when(store.getFs().exists(Mockito.any(\n         Path.class))).thenReturn(existsRetVal);\n     store.init(conf, mgr);\n-    Mockito.verify(store.getFs(),Mockito.times(\n+    Mockito.verify(store.getFs(), Mockito.times(\n         expectedNumOfCalls)).mkdirs(Mockito.any(Path\n         .class));\n   }"
    },
    {
        "commit_id": "a6590c1f1f7cacd3843265f6e6227f1221205865",
        "commit_message": "YARN-8117. Fix TestRMWebServicesNodes test failure. Contributed by Bibin A Chundatt.",
        "commit_url": "https://github.com/apache/hadoop/commit/a6590c1f1f7cacd3843265f6e6227f1221205865",
        "buggy_code": "assertEquals(\"incorrect number of elements\", 19, nodeInfo.length());",
        "fixed_code": "assertEquals(\"incorrect number of elements\", 20, nodeInfo.length());",
        "patch": "@@ -740,7 +740,7 @@ public void verifyNodesXML(NodeList nodes, RMNode nm)\n \n   public void verifyNodeInfo(JSONObject nodeInfo, RMNode nm)\n       throws JSONException, Exception {\n-    assertEquals(\"incorrect number of elements\", 19, nodeInfo.length());\n+    assertEquals(\"incorrect number of elements\", 20, nodeInfo.length());\n \n     JSONObject resourceInfo = nodeInfo.getJSONObject(\"resourceUtilization\");\n     verifyNodeInfoGeneric(nm, nodeInfo.getString(\"state\"),"
    },
    {
        "commit_id": "eed8415dc18fa7415ebd105350bd0532b3b1b6bb",
        "commit_message": "YARN-8535. Fix DistributedShell unit tests. Contributed by Abhishek Modi.",
        "commit_url": "https://github.com/apache/hadoop/commit/eed8415dc18fa7415ebd105350bd0532b3b1b6bb",
        "buggy_code": "LOG.info(\"Application completed. Signalling finish to RM\");",
        "fixed_code": "LOG.info(\"Application completed. Signalling finished to RM\");",
        "patch": "@@ -944,7 +944,7 @@ protected boolean finish() {\n \n     // When the application completes, it should send a finish application\n     // signal to the RM\n-    LOG.info(\"Application completed. Signalling finish to RM\");\n+    LOG.info(\"Application completed. Signalling finished to RM\");\n \n     FinalApplicationStatus appStatus;\n     boolean success = true;"
    },
    {
        "commit_id": "3fa46394214181ed1cc7f06b886282bbdf67a10f",
        "commit_message": "YARN-8723. Fix a typo in CS init error message when resource calculator is not correctly set. Contributed by Abhishek Modi.",
        "commit_url": "https://github.com/apache/hadoop/commit/3fa46394214181ed1cc7f06b886282bbdf67a10f",
        "buggy_code": "+ \" DomainantResourceCalculator instead to make effective use of\"",
        "fixed_code": "+ \" DominantResourceCalculator instead to make effective use of\"",
        "patch": "@@ -348,7 +348,7 @@ void initScheduler(Configuration configuration) throws\n         throw new YarnRuntimeException(\"RM uses DefaultResourceCalculator which\"\n             + \" used only memory as resource-type but invalid resource-types\"\n             + \" specified \" + ResourceUtils.getResourceTypes() + \". Use\"\n-            + \" DomainantResourceCalculator instead to make effective use of\"\n+            + \" DominantResourceCalculator instead to make effective use of\"\n             + \" these resource-types\");\n       }\n       this.usePortForNodeName = this.conf.getUsePortForNodeName();"
    },
    {
        "commit_id": "c5629d546d64091a14560df488a7f797a150337e",
        "commit_message": "HDDS-382. Remove RatisTestHelper#RatisTestSuite constructor argument and fix checkstyle in ContainerTestHelper, GenericTestUtils\nContributed by Nandakumar.",
        "commit_url": "https://github.com/apache/hadoop/commit/c5629d546d64091a14560df488a7f797a150337e",
        "buggy_code": "throw new IOException (\"not implemented\" + pipeline.getType());",
        "fixed_code": "throw new IOException(\"not implemented\" + pipeline.getType());",
        "patch": "@@ -154,7 +154,7 @@ public XceiverClientSpi call() throws Exception {\n               break;\n             case CHAINED:\n             default:\n-              throw new IOException (\"not implemented\" + pipeline.getType());\n+              throw new IOException(\"not implemented\" + pipeline.getType());\n             }\n             client.connect();\n             return client;"
    },
    {
        "commit_id": "c5629d546d64091a14560df488a7f797a150337e",
        "commit_message": "HDDS-382. Remove RatisTestHelper#RatisTestSuite constructor argument and fix checkstyle in ContainerTestHelper, GenericTestUtils\nContributed by Nandakumar.",
        "commit_url": "https://github.com/apache/hadoop/commit/c5629d546d64091a14560df488a7f797a150337e",
        "buggy_code": "suite = new RatisTestHelper.RatisTestSuite(TestBucketsRatis.class);",
        "fixed_code": "suite = new RatisTestHelper.RatisTestSuite();",
        "patch": "@@ -61,7 +61,7 @@ public static Collection<Object[]> clientProtocol() {\n \n   @BeforeClass\n   public static void init() throws Exception {\n-    suite = new RatisTestHelper.RatisTestSuite(TestBucketsRatis.class);\n+    suite = new RatisTestHelper.RatisTestSuite();\n     conf = suite.getConf();\n   }\n "
    },
    {
        "commit_id": "c5629d546d64091a14560df488a7f797a150337e",
        "commit_message": "HDDS-382. Remove RatisTestHelper#RatisTestSuite constructor argument and fix checkstyle in ContainerTestHelper, GenericTestUtils\nContributed by Nandakumar.",
        "commit_url": "https://github.com/apache/hadoop/commit/c5629d546d64091a14560df488a7f797a150337e",
        "buggy_code": "suite = new RatisTestHelper.RatisTestSuite(TestBucketsRatis.class);",
        "fixed_code": "suite = new RatisTestHelper.RatisTestSuite();",
        "patch": "@@ -57,7 +57,7 @@ public class TestKeysRatis {\n \n   @BeforeClass\n   public static void init() throws Exception {\n-    suite = new RatisTestHelper.RatisTestSuite(TestBucketsRatis.class);\n+    suite = new RatisTestHelper.RatisTestSuite();\n     path = GenericTestUtils.getTempPath(TestKeysRatis.class.getSimpleName());\n     ozoneCluster = suite.getCluster();\n     ozoneCluster.waitForClusterToBeReady();"
    },
    {
        "commit_id": "65e7469712be6cf393e29ef73cc94727eec81227",
        "commit_message": "YARN-8242. YARN NM: OOM error while reading back the state store on recovery. Contributed by Pradeep Ambati and Kanwaljeet Sachdev",
        "commit_url": "https://github.com/apache/hadoop/commit/65e7469712be6cf393e29ef73cc94727eec81227",
        "buggy_code": "public List<RecoveredContainerState> loadContainersState()",
        "fixed_code": "public RecoveryIterator<RecoveredContainerState> getContainerStateIterator()",
        "patch": "@@ -65,7 +65,7 @@ public void removeApplication(ApplicationId appId) throws IOException {\n   }\n \n   @Override\n-  public List<RecoveredContainerState> loadContainersState()\n+  public RecoveryIterator<RecoveredContainerState> getContainerStateIterator()\n       throws IOException {\n     throw new UnsupportedOperationException(\n         \"Recovery not supported by this state store\");"
    },
    {
        "commit_id": "1697a0230696e1ed6d9c19471463b44a6d791dfa",
        "commit_message": "YARN-8612. Fix NM Collector Service Port issue in YarnConfiguration. Contributed by Prabha Manepalli.",
        "commit_url": "https://github.com/apache/hadoop/commit/1697a0230696e1ed6d9c19471463b44a6d791dfa",
        "buggy_code": "\"0.0.0.0:\" + DEFAULT_NM_LOCALIZER_PORT;",
        "fixed_code": "\"0.0.0.0:\" + DEFAULT_NM_COLLECTOR_SERVICE_PORT;",
        "patch": "@@ -1216,7 +1216,7 @@ public static boolean isAclEnabled(Configuration conf) {\n       NM_PREFIX + \"collector-service.address\";\n   public static final int DEFAULT_NM_COLLECTOR_SERVICE_PORT = 8048;\n   public static final String DEFAULT_NM_COLLECTOR_SERVICE_ADDRESS =\n-      \"0.0.0.0:\" + DEFAULT_NM_LOCALIZER_PORT;\n+      \"0.0.0.0:\" + DEFAULT_NM_COLLECTOR_SERVICE_PORT;\n \n   /** Interval in between cache cleanups.*/\n   public static final String NM_LOCALIZER_CACHE_CLEANUP_INTERVAL_MS ="
    },
    {
        "commit_id": "d951af22b42a22c09cfeecd3c866d5f1cd412120",
        "commit_message": "HADOOP-15552. Addendum patch to fix the build break in Ozone File system.\nContributed by Anu Engineer.",
        "commit_url": "https://github.com/apache/hadoop/commit/d951af22b42a22c09cfeecd3c866d5f1cd412120",
        "buggy_code": "getLog().info(\"FS details {}\", getFileSystem());",
        "fixed_code": "getLogger().info(\"FS details {}\", getFileSystem());",
        "patch": "@@ -50,7 +50,7 @@ protected AbstractFSContract createContract(Configuration conf) {\n \n   @Override\n   public void teardown() throws Exception {\n-    getLog().info(\"FS details {}\", getFileSystem());\n+    getLogger().info(\"FS details {}\", getFileSystem());\n     super.teardown();\n   }\n "
    },
    {
        "commit_id": "db3f227d8aeeea8b5bb473fed9ca4f6a17b0fca5",
        "commit_message": "HDFS-13076: [SPS]: Resolve conflicts after rebasing HDFS-10285 branch to trunk. Contributed by Rakesh R.",
        "commit_url": "https://github.com/apache/hadoop/commit/db3f227d8aeeea8b5bb473fed9ca4f6a17b0fca5",
        "buggy_code": "if (org.apache.commons.lang.StringUtils.isBlank(modeVal)) {",
        "fixed_code": "if (org.apache.commons.lang3.StringUtils.isBlank(modeVal)) {",
        "patch": "@@ -5078,7 +5078,7 @@ public boolean createSPSManager(final Configuration conf,\n         DFSConfigKeys.DFS_STORAGE_POLICY_ENABLED_KEY,\n         DFSConfigKeys.DFS_STORAGE_POLICY_ENABLED_DEFAULT);\n     String modeVal = spsMode;\n-    if (org.apache.commons.lang.StringUtils.isBlank(modeVal)) {\n+    if (org.apache.commons.lang3.StringUtils.isBlank(modeVal)) {\n       modeVal = conf.get(DFSConfigKeys.DFS_STORAGE_POLICY_SATISFIER_MODE_KEY,\n           DFSConfigKeys.DFS_STORAGE_POLICY_SATISFIER_MODE_DEFAULT);\n     }"
    },
    {
        "commit_id": "4402f3f8557527d5c6cdad6f5bdcbd707b8cbf52",
        "commit_message": "HDFS-13097: [SPS]: Fix the branch review comments(Part1). Contributed by Surendra Singh.",
        "commit_url": "https://github.com/apache/hadoop/commit/4402f3f8557527d5c6cdad6f5bdcbd707b8cbf52",
        "buggy_code": "public boolean isStoragePolicySatisfierRunning() throws IOException {",
        "fixed_code": "public boolean isInternalSatisfierRunning() throws IOException {",
        "patch": "@@ -2498,7 +2498,7 @@ public void satisfyStoragePolicy(String path) throws IOException {\n   }\n \n   @Override\n-  public boolean isStoragePolicySatisfierRunning() throws IOException {\n+  public boolean isInternalSatisfierRunning() throws IOException {\n     checkOperation(OperationCategory.READ, false);\n     return false;\n   }"
    },
    {
        "commit_id": "4402f3f8557527d5c6cdad6f5bdcbd707b8cbf52",
        "commit_message": "HDFS-13097: [SPS]: Fix the branch review comments(Part1). Contributed by Surendra Singh.",
        "commit_url": "https://github.com/apache/hadoop/commit/4402f3f8557527d5c6cdad6f5bdcbd707b8cbf52",
        "buggy_code": "String XATTR_SATISFY_STORAGE_POLICY = \"user.hdfs.sps.xattr\";",
        "fixed_code": "String XATTR_SATISFY_STORAGE_POLICY = \"user.hdfs.sps\";",
        "patch": "@@ -365,7 +365,7 @@ enum BlockUCState {\n   String XATTR_ERASURECODING_POLICY =\n       \"system.hdfs.erasurecoding.policy\";\n \n-  String XATTR_SATISFY_STORAGE_POLICY = \"user.hdfs.sps.xattr\";\n+  String XATTR_SATISFY_STORAGE_POLICY = \"user.hdfs.sps\";\n \n   Path MOVER_ID_PATH = new Path(\"/system/mover.id\");\n "
    },
    {
        "commit_id": "4402f3f8557527d5c6cdad6f5bdcbd707b8cbf52",
        "commit_message": "HDFS-13097: [SPS]: Fix the branch review comments(Part1). Contributed by Surendra Singh.",
        "commit_url": "https://github.com/apache/hadoop/commit/4402f3f8557527d5c6cdad6f5bdcbd707b8cbf52",
        "buggy_code": ".isStoragePolicySatisfierRunning();",
        "fixed_code": ".isInternalSatisfierRunning();",
        "patch": "@@ -661,7 +661,7 @@ static int run(Map<URI, List<Path>> namenodes, Configuration conf)\n           boolean spsRunning;\n           try {\n             spsRunning = nnc.getDistributedFileSystem().getClient()\n-                .isStoragePolicySatisfierRunning();\n+                .isInternalSatisfierRunning();\n           } catch (RemoteException e) {\n             IOException cause = e.unwrapRemoteException();\n             if (cause instanceof StandbyException) {"
    },
    {
        "commit_id": "4402f3f8557527d5c6cdad6f5bdcbd707b8cbf52",
        "commit_message": "HDFS-13097: [SPS]: Fix the branch review comments(Part1). Contributed by Surendra Singh.",
        "commit_url": "https://github.com/apache/hadoop/commit/4402f3f8557527d5c6cdad6f5bdcbd707b8cbf52",
        "buggy_code": "fsd.getBlockManager().getStoragePolicySatisfier()",
        "fixed_code": "fsd.getBlockManager().getSPSManager().getInternalSPSService()",
        "patch": "@@ -209,7 +209,7 @@ static List<XAttr> unprotectedRemoveXAttrs(\n       for (XAttr xattr : toRemove) {\n         if (XATTR_SATISFY_STORAGE_POLICY\n             .equals(XAttrHelper.getPrefixedName(xattr))) {\n-          fsd.getBlockManager().getStoragePolicySatisfier()\n+          fsd.getBlockManager().getSPSManager().getInternalSPSService()\n               .clearQueue(inode.getId());\n           break;\n         }"
    },
    {
        "commit_id": "4402f3f8557527d5c6cdad6f5bdcbd707b8cbf52",
        "commit_message": "HDFS-13097: [SPS]: Fix the branch review comments(Part1). Contributed by Surendra Singh.",
        "commit_url": "https://github.com/apache/hadoop/commit/4402f3f8557527d5c6cdad6f5bdcbd707b8cbf52",
        "buggy_code": "if (namesystem.getBlockManager().isSPSEnabled()) {",
        "fixed_code": "if (namesystem.getBlockManager().getSPSManager().isEnabled()) {",
        "patch": "@@ -1401,7 +1401,7 @@ public final void addToInodeMap(INode inode) {\n       if (!inode.isSymlink()) {\n         final XAttrFeature xaf = inode.getXAttrFeature();\n         addEncryptionZone((INodeWithAdditionalFields) inode, xaf);\n-        if (namesystem.getBlockManager().isSPSEnabled()) {\n+        if (namesystem.getBlockManager().getSPSManager().isEnabled()) {\n           addStoragePolicySatisfier((INodeWithAdditionalFields) inode, xaf);\n         }\n       }"
    },
    {
        "commit_id": "4402f3f8557527d5c6cdad6f5bdcbd707b8cbf52",
        "commit_message": "HDFS-13097: [SPS]: Fix the branch review comments(Part1). Contributed by Surendra Singh.",
        "commit_url": "https://github.com/apache/hadoop/commit/4402f3f8557527d5c6cdad6f5bdcbd707b8cbf52",
        "buggy_code": ".isStoragePolicySatisfierRunning();",
        "fixed_code": ".isInternalSatisfierRunning();",
        "patch": "@@ -73,7 +73,7 @@ public static void main(String[] args) throws Exception {\n \n       boolean spsRunning;\n       spsRunning = nnc.getDistributedFileSystem().getClient()\n-          .isStoragePolicySatisfierRunning();\n+          .isInternalSatisfierRunning();\n       if (spsRunning) {\n         throw new RuntimeException(\n             \"Startup failed due to StoragePolicySatisfier\""
    },
    {
        "commit_id": "4402f3f8557527d5c6cdad6f5bdcbd707b8cbf52",
        "commit_message": "HDFS-13097: [SPS]: Fix the branch review comments(Part1). Contributed by Surendra Singh.",
        "commit_url": "https://github.com/apache/hadoop/commit/4402f3f8557527d5c6cdad6f5bdcbd707b8cbf52",
        "buggy_code": "if(dfs.getClient().isStoragePolicySatisfierRunning()){",
        "fixed_code": "if(dfs.getClient().isInternalSatisfierRunning()){",
        "patch": "@@ -374,7 +374,7 @@ public int run(Configuration conf, List<String> args) throws IOException {\n       }\n       final DistributedFileSystem dfs = AdminHelper.getDFS(conf);\n       try {\n-        if(dfs.getClient().isStoragePolicySatisfierRunning()){\n+        if(dfs.getClient().isInternalSatisfierRunning()){\n           System.out.println(\"yes\");\n         }else{\n           System.out.println(\"no\");"
    },
    {
        "commit_id": "0e820f16af309cc8476edba448dd548686431133",
        "commit_message": "HDFS-12214: [SPS]: Fix review comments of StoragePolicySatisfier feature. Contributed by Rakesh R.",
        "commit_url": "https://github.com/apache/hadoop/commit/0e820f16af309cc8476edba448dd548686431133",
        "buggy_code": "public static enum Status {",
        "fixed_code": "public enum Status {",
        "patch": "@@ -40,7 +40,7 @@ public class BlocksStorageMovementResult {\n    * IN_PROGRESS - If all or some of the blocks associated to track id are\n    * still moving.\n    */\n-  public static enum Status {\n+  public enum Status {\n     SUCCESS, FAILURE, IN_PROGRESS;\n   }\n "
    },
    {
        "commit_id": "0e820f16af309cc8476edba448dd548686431133",
        "commit_message": "HDFS-12214: [SPS]: Fix review comments of StoragePolicySatisfier feature. Contributed by Rakesh R.",
        "commit_url": "https://github.com/apache/hadoop/commit/0e820f16af309cc8476edba448dd548686431133",
        "buggy_code": "conf.setBoolean(DFSConfigKeys.DFS_STORAGE_POLICY_SATISFIER_ACTIVATE_KEY,",
        "fixed_code": "conf.setBoolean(DFSConfigKeys.DFS_STORAGE_POLICY_SATISFIER_ENABLED_KEY,",
        "patch": "@@ -69,7 +69,7 @@ private static void initConf(Configuration conf) {\n     conf.setLong(DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_KEY,\n         1L);\n     conf.setLong(DFSConfigKeys.DFS_BALANCER_MOVEDWINWIDTH_KEY, 2000L);\n-    conf.setBoolean(DFSConfigKeys.DFS_STORAGE_POLICY_SATISFIER_ACTIVATE_KEY,\n+    conf.setBoolean(DFSConfigKeys.DFS_STORAGE_POLICY_SATISFIER_ENABLED_KEY,\n         true);\n   }\n "
    },
    {
        "commit_id": "0e820f16af309cc8476edba448dd548686431133",
        "commit_message": "HDFS-12214: [SPS]: Fix review comments of StoragePolicySatisfier feature. Contributed by Rakesh R.",
        "commit_url": "https://github.com/apache/hadoop/commit/0e820f16af309cc8476edba448dd548686431133",
        "buggy_code": "DFSConfigKeys.DFS_STORAGE_POLICY_SATISFIER_ACTIVATE_KEY, false);",
        "fixed_code": "DFSConfigKeys.DFS_STORAGE_POLICY_SATISFIER_ENABLED_KEY, false);",
        "patch": "@@ -97,7 +97,7 @@ public class TestStorageMover {\n         DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_KEY, 2L);\n     DEFAULT_CONF.setLong(DFSConfigKeys.DFS_MOVER_MOVEDWINWIDTH_KEY, 2000L);\n     DEFAULT_CONF.setBoolean(\n-        DFSConfigKeys.DFS_STORAGE_POLICY_SATISFIER_ACTIVATE_KEY, false);\n+        DFSConfigKeys.DFS_STORAGE_POLICY_SATISFIER_ENABLED_KEY, false);\n \n     DEFAULT_POLICIES = BlockStoragePolicySuite.createDefaultSuite();\n     HOT = DEFAULT_POLICIES.getPolicy(HdfsConstants.HOT_STORAGE_POLICY_NAME);"
    },
    {
        "commit_id": "0e820f16af309cc8476edba448dd548686431133",
        "commit_message": "HDFS-12214: [SPS]: Fix review comments of StoragePolicySatisfier feature. Contributed by Rakesh R.",
        "commit_url": "https://github.com/apache/hadoop/commit/0e820f16af309cc8476edba448dd548686431133",
        "buggy_code": "bsmAttemptedItems.deactivate();",
        "fixed_code": "bsmAttemptedItems.stop();",
        "patch": "@@ -47,7 +47,7 @@ public void setup() throws Exception {\n   @After\n   public void teardown() {\n     if (bsmAttemptedItems != null) {\n-      bsmAttemptedItems.deactivate();\n+      bsmAttemptedItems.stop();\n       bsmAttemptedItems.stopGracefully();\n     }\n   }"
    },
    {
        "commit_id": "23f394240e1568a38025e63e9dc0842e8c5235f7",
        "commit_message": "YARN-8610.  Fixed initiate upgrade error message.\n            Contributed by Chandni Singh",
        "commit_url": "https://github.com/apache/hadoop/commit/23f394240e1568a38025e63e9dc0842e8c5235f7",
        "buggy_code": "+ \" state, upgrade can not be invoked when service is STABLE.\";",
        "fixed_code": "+ \" state and upgrade can only be initiated when service is STABLE.\";",
        "patch": "@@ -257,7 +257,7 @@ public int initiateUpgrade(Service service) throws YarnException,\n     if (!liveService.getState().equals(ServiceState.STABLE)) {\n       String message = service.getName() + \" is at \" +\n           liveService.getState()\n-          + \" state, upgrade can not be invoked when service is STABLE.\";\n+          + \" state and upgrade can only be initiated when service is STABLE.\";\n       LOG.error(message);\n       throw new YarnException(message);\n     }"
    },
    {
        "commit_id": "17e26163ec1b71cd13a6a82150aca94283f10ed1",
        "commit_message": "HADOOP-15586. Fix wrong log statement in AbstractService. (Szilard Nemeth via Haibo Chen)",
        "commit_url": "https://github.com/apache/hadoop/commit/17e26163ec1b71cd13a6a82150aca94283f10ed1",
        "buggy_code": "LOG.debug(\"noteFailure {}\" + exception);",
        "fixed_code": "LOG.debug(\"noteFailure\", exception);",
        "patch": "@@ -254,7 +254,7 @@ public final void close() throws IOException {\n    * @param exception the exception\n    */\n   protected final void noteFailure(Exception exception) {\n-    LOG.debug(\"noteFailure {}\" + exception);\n+    LOG.debug(\"noteFailure\", exception);\n     if (exception == null) {\n       //make sure failure logic doesn't itself cause problems\n       return;"
    },
    {
        "commit_id": "5074ca93afb4fbd1c367852ba55d1e89b38a2133",
        "commit_message": "HDDS-254. Fix TestStorageContainerManager#testBlockDeletingThrottling. Contributed by Lokesh Jain",
        "commit_url": "https://github.com/apache/hadoop/commit/5074ca93afb4fbd1c367852ba55d1e89b38a2133",
        "buggy_code": "conf.getTimeDuration(ScmConfigKeys.OZONE_SCM_HEARTBEAT_INTERVAL,",
        "fixed_code": "conf.setTimeDuration(ScmConfigKeys.OZONE_SCM_HEARTBEAT_INTERVAL,",
        "patch": "@@ -392,7 +392,7 @@ private void configureSCM() {\n \n     private void configureSCMheartbeat() {\n       if (hbInterval.isPresent()) {\n-        conf.getTimeDuration(ScmConfigKeys.OZONE_SCM_HEARTBEAT_INTERVAL,\n+        conf.setTimeDuration(ScmConfigKeys.OZONE_SCM_HEARTBEAT_INTERVAL,\n             hbInterval.get(), TimeUnit.MILLISECONDS);\n \n       } else {"
    },
    {
        "commit_id": "88625f5cd90766136a9ebd76a8d84b45a37e6c99",
        "commit_message": "HADOOP-15531. Use commons-text instead of commons-lang in some classes to fix deprecation warnings. Contributed by Takanobu Asanuma.",
        "commit_url": "https://github.com/apache/hadoop/commit/88625f5cd90766136a9ebd76a8d84b45a37e6c99",
        "buggy_code": "import org.apache.commons.lang3.StringEscapeUtils;",
        "fixed_code": "import org.apache.commons.text.StringEscapeUtils;",
        "patch": "@@ -18,7 +18,7 @@\n \n package org.apache.hadoop.conf;\n \n-import org.apache.commons.lang3.StringEscapeUtils;\n+import org.apache.commons.text.StringEscapeUtils;\n \n import java.util.Collection;\n import java.util.Enumeration;"
    },
    {
        "commit_id": "88625f5cd90766136a9ebd76a8d84b45a37e6c99",
        "commit_message": "HADOOP-15531. Use commons-text instead of commons-lang in some classes to fix deprecation warnings. Contributed by Takanobu Asanuma.",
        "commit_url": "https://github.com/apache/hadoop/commit/88625f5cd90766136a9ebd76a8d84b45a37e6c99",
        "buggy_code": "import org.apache.commons.lang3.StringEscapeUtils;",
        "fixed_code": "import org.apache.commons.text.StringEscapeUtils;",
        "patch": "@@ -31,7 +31,7 @@\n import javax.servlet.http.HttpServletRequest;\n import javax.servlet.http.HttpServletResponse;\n \n-import org.apache.commons.lang3.StringEscapeUtils;\n+import org.apache.commons.text.StringEscapeUtils;\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.classification.InterfaceAudience;"
    },
    {
        "commit_id": "88625f5cd90766136a9ebd76a8d84b45a37e6c99",
        "commit_message": "HADOOP-15531. Use commons-text instead of commons-lang in some classes to fix deprecation warnings. Contributed by Takanobu Asanuma.",
        "commit_url": "https://github.com/apache/hadoop/commit/88625f5cd90766136a9ebd76a8d84b45a37e6c99",
        "buggy_code": "import static org.apache.commons.lang3.StringEscapeUtils.escapeJava;",
        "fixed_code": "import static org.apache.commons.text.StringEscapeUtils.escapeJava;",
        "patch": "@@ -17,7 +17,7 @@\n  */\n package org.apache.hadoop.hdfs.server.namenode;\n \n-import static org.apache.commons.lang3.StringEscapeUtils.escapeJava;\n+import static org.apache.commons.text.StringEscapeUtils.escapeJava;\n import static org.apache.hadoop.fs.CommonConfigurationKeysPublic.FS_TRASH_INTERVAL_DEFAULT;\n import static org.apache.hadoop.fs.CommonConfigurationKeysPublic.FS_TRASH_INTERVAL_KEY;\n import static org.apache.hadoop.fs.CommonConfigurationKeysPublic.HADOOP_CALLER_CONTEXT_ENABLED_DEFAULT;"
    },
    {
        "commit_id": "88625f5cd90766136a9ebd76a8d84b45a37e6c99",
        "commit_message": "HADOOP-15531. Use commons-text instead of commons-lang in some classes to fix deprecation warnings. Contributed by Takanobu Asanuma.",
        "commit_url": "https://github.com/apache/hadoop/commit/88625f5cd90766136a9ebd76a8d84b45a37e6c99",
        "buggy_code": "import org.apache.commons.lang3.text.WordUtils;",
        "fixed_code": "import org.apache.commons.text.WordUtils;",
        "patch": "@@ -22,7 +22,7 @@\n import java.util.LinkedList;\n import java.util.List;\n \n-import org.apache.commons.lang3.text.WordUtils;\n+import org.apache.commons.text.WordUtils;\n import org.apache.hadoop.classification.InterfaceAudience;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.conf.Configured;"
    },
    {
        "commit_id": "88625f5cd90766136a9ebd76a8d84b45a37e6c99",
        "commit_message": "HADOOP-15531. Use commons-text instead of commons-lang in some classes to fix deprecation warnings. Contributed by Takanobu Asanuma.",
        "commit_url": "https://github.com/apache/hadoop/commit/88625f5cd90766136a9ebd76a8d84b45a37e6c99",
        "buggy_code": "import org.apache.commons.lang3.StringEscapeUtils;",
        "fixed_code": "import org.apache.commons.text.StringEscapeUtils;",
        "patch": "@@ -27,7 +27,7 @@\n import java.util.EnumSet;\n import java.util.Collection;\n \n-import org.apache.commons.lang3.StringEscapeUtils;\n+import org.apache.commons.text.StringEscapeUtils;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.mapreduce.MRConfig;\n import org.apache.hadoop.mapreduce.v2.api.records.JobId;"
    },
    {
        "commit_id": "88625f5cd90766136a9ebd76a8d84b45a37e6c99",
        "commit_message": "HADOOP-15531. Use commons-text instead of commons-lang in some classes to fix deprecation warnings. Contributed by Takanobu Asanuma.",
        "commit_url": "https://github.com/apache/hadoop/commit/88625f5cd90766136a9ebd76a8d84b45a37e6c99",
        "buggy_code": "import org.apache.commons.lang3.StringEscapeUtils;",
        "fixed_code": "import org.apache.commons.text.StringEscapeUtils;",
        "patch": "@@ -24,7 +24,7 @@\n import static org.apache.hadoop.yarn.webapp.view.JQueryUI.C_PROGRESSBAR;\n import static org.apache.hadoop.yarn.webapp.view.JQueryUI.C_PROGRESSBAR_VALUE;\n \n-import org.apache.commons.lang3.StringEscapeUtils;\n+import org.apache.commons.text.StringEscapeUtils;\n import org.apache.hadoop.mapreduce.v2.api.records.TaskType;\n import org.apache.hadoop.mapreduce.v2.app.job.Task;\n import org.apache.hadoop.mapreduce.v2.app.webapp.dao.TaskInfo;"
    },
    {
        "commit_id": "88625f5cd90766136a9ebd76a8d84b45a37e6c99",
        "commit_message": "HADOOP-15531. Use commons-text instead of commons-lang in some classes to fix deprecation warnings. Contributed by Takanobu Asanuma.",
        "commit_url": "https://github.com/apache/hadoop/commit/88625f5cd90766136a9ebd76a8d84b45a37e6c99",
        "buggy_code": "import org.apache.commons.lang3.StringEscapeUtils;",
        "fixed_code": "import org.apache.commons.text.StringEscapeUtils;",
        "patch": "@@ -21,7 +21,7 @@\n import java.text.SimpleDateFormat;\n import java.util.Date;\n \n-import org.apache.commons.lang3.StringEscapeUtils;\n+import org.apache.commons.text.StringEscapeUtils;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.mapreduce.MRConfig;\n import org.apache.hadoop.mapreduce.v2.app.AppContext;"
    },
    {
        "commit_id": "88625f5cd90766136a9ebd76a8d84b45a37e6c99",
        "commit_message": "HADOOP-15531. Use commons-text instead of commons-lang in some classes to fix deprecation warnings. Contributed by Takanobu Asanuma.",
        "commit_url": "https://github.com/apache/hadoop/commit/88625f5cd90766136a9ebd76a8d84b45a37e6c99",
        "buggy_code": "import org.apache.commons.lang3.StringEscapeUtils;",
        "fixed_code": "import org.apache.commons.text.StringEscapeUtils;",
        "patch": "@@ -29,7 +29,7 @@\n \n import java.util.Collection;\n \n-import org.apache.commons.lang3.StringEscapeUtils;\n+import org.apache.commons.text.StringEscapeUtils;\n import org.apache.hadoop.mapreduce.v2.api.records.TaskId;\n import org.apache.hadoop.mapreduce.v2.api.records.TaskType;\n import org.apache.hadoop.mapreduce.v2.app.job.TaskAttempt;"
    },
    {
        "commit_id": "88625f5cd90766136a9ebd76a8d84b45a37e6c99",
        "commit_message": "HADOOP-15531. Use commons-text instead of commons-lang in some classes to fix deprecation warnings. Contributed by Takanobu Asanuma.",
        "commit_url": "https://github.com/apache/hadoop/commit/88625f5cd90766136a9ebd76a8d84b45a37e6c99",
        "buggy_code": "import org.apache.commons.lang3.StringEscapeUtils;",
        "fixed_code": "import org.apache.commons.text.StringEscapeUtils;",
        "patch": "@@ -26,7 +26,7 @@\n import java.util.List;\n import java.util.Set;\n \n-import org.apache.commons.lang3.StringEscapeUtils;\n+import org.apache.commons.text.StringEscapeUtils;\n import org.apache.hadoop.classification.InterfaceAudience.Private;\n \n @Private"
    },
    {
        "commit_id": "88625f5cd90766136a9ebd76a8d84b45a37e6c99",
        "commit_message": "HADOOP-15531. Use commons-text instead of commons-lang in some classes to fix deprecation warnings. Contributed by Takanobu Asanuma.",
        "commit_url": "https://github.com/apache/hadoop/commit/88625f5cd90766136a9ebd76a8d84b45a37e6c99",
        "buggy_code": "import static org.apache.commons.lang3.StringEscapeUtils.*;",
        "fixed_code": "import static org.apache.commons.text.StringEscapeUtils.*;",
        "patch": "@@ -28,7 +28,7 @@\n import static java.util.EnumSet.*;\n import java.util.Iterator;\n \n-import static org.apache.commons.lang3.StringEscapeUtils.*;\n+import static org.apache.commons.text.StringEscapeUtils.*;\n import static org.apache.hadoop.yarn.webapp.hamlet.HamletImpl.EOpt.*;\n \n import org.apache.hadoop.classification.InterfaceAudience;"
    },
    {
        "commit_id": "88625f5cd90766136a9ebd76a8d84b45a37e6c99",
        "commit_message": "HADOOP-15531. Use commons-text instead of commons-lang in some classes to fix deprecation warnings. Contributed by Takanobu Asanuma.",
        "commit_url": "https://github.com/apache/hadoop/commit/88625f5cd90766136a9ebd76a8d84b45a37e6c99",
        "buggy_code": "import static org.apache.commons.lang3.StringEscapeUtils.*;",
        "fixed_code": "import static org.apache.commons.text.StringEscapeUtils.*;",
        "patch": "@@ -28,7 +28,7 @@\n import static java.util.EnumSet.*;\n import java.util.Iterator;\n \n-import static org.apache.commons.lang3.StringEscapeUtils.*;\n+import static org.apache.commons.text.StringEscapeUtils.*;\n import static org.apache.hadoop.yarn.webapp.hamlet2.HamletImpl.EOpt.*;\n \n import org.apache.hadoop.classification.InterfaceAudience;"
    },
    {
        "commit_id": "88625f5cd90766136a9ebd76a8d84b45a37e6c99",
        "commit_message": "HADOOP-15531. Use commons-text instead of commons-lang in some classes to fix deprecation warnings. Contributed by Takanobu Asanuma.",
        "commit_url": "https://github.com/apache/hadoop/commit/88625f5cd90766136a9ebd76a8d84b45a37e6c99",
        "buggy_code": "import static org.apache.commons.lang3.StringEscapeUtils.escapeEcmaScript;",
        "fixed_code": "import static org.apache.commons.text.StringEscapeUtils.escapeEcmaScript;",
        "patch": "@@ -18,7 +18,7 @@\n \n package org.apache.hadoop.yarn.webapp.view;\n \n-import static org.apache.commons.lang3.StringEscapeUtils.escapeEcmaScript;\n+import static org.apache.commons.text.StringEscapeUtils.escapeEcmaScript;\n import static org.apache.hadoop.yarn.util.StringHelper.djoin;\n import static org.apache.hadoop.yarn.util.StringHelper.join;\n import static org.apache.hadoop.yarn.util.StringHelper.split;"
    },
    {
        "commit_id": "88625f5cd90766136a9ebd76a8d84b45a37e6c99",
        "commit_message": "HADOOP-15531. Use commons-text instead of commons-lang in some classes to fix deprecation warnings. Contributed by Takanobu Asanuma.",
        "commit_url": "https://github.com/apache/hadoop/commit/88625f5cd90766136a9ebd76a8d84b45a37e6c99",
        "buggy_code": "import org.apache.commons.lang3.StringEscapeUtils;",
        "fixed_code": "import org.apache.commons.text.StringEscapeUtils;",
        "patch": "@@ -20,7 +20,7 @@\n \n import java.io.PrintWriter;\n \n-import org.apache.commons.lang3.StringEscapeUtils;\n+import org.apache.commons.text.StringEscapeUtils;\n import org.apache.hadoop.classification.InterfaceAudience;\n import org.apache.hadoop.yarn.webapp.View;\n "
    },
    {
        "commit_id": "88625f5cd90766136a9ebd76a8d84b45a37e6c99",
        "commit_message": "HADOOP-15531. Use commons-text instead of commons-lang in some classes to fix deprecation warnings. Contributed by Takanobu Asanuma.",
        "commit_url": "https://github.com/apache/hadoop/commit/88625f5cd90766136a9ebd76a8d84b45a37e6c99",
        "buggy_code": "import org.apache.commons.lang3.StringEscapeUtils;",
        "fixed_code": "import org.apache.commons.text.StringEscapeUtils;",
        "patch": "@@ -25,7 +25,7 @@\n import java.util.Collection;\n import java.util.List;\n \n-import org.apache.commons.lang3.StringEscapeUtils;\n+import org.apache.commons.text.StringEscapeUtils;\n import org.apache.hadoop.security.UserGroupInformation;\n import org.apache.hadoop.yarn.api.ApplicationBaseProtocol;\n import org.apache.hadoop.yarn.api.protocolrecords.GetApplicationAttemptReportRequest;"
    },
    {
        "commit_id": "88625f5cd90766136a9ebd76a8d84b45a37e6c99",
        "commit_message": "HADOOP-15531. Use commons-text instead of commons-lang in some classes to fix deprecation warnings. Contributed by Takanobu Asanuma.",
        "commit_url": "https://github.com/apache/hadoop/commit/88625f5cd90766136a9ebd76a8d84b45a37e6c99",
        "buggy_code": "import org.apache.commons.lang3.StringEscapeUtils;",
        "fixed_code": "import org.apache.commons.text.StringEscapeUtils;",
        "patch": "@@ -28,7 +28,7 @@\n import java.util.List;\n import java.util.Map;\n \n-import org.apache.commons.lang3.StringEscapeUtils;\n+import org.apache.commons.text.StringEscapeUtils;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.CommonConfigurationKeys;\n import org.apache.hadoop.security.UserGroupInformation;"
    },
    {
        "commit_id": "88625f5cd90766136a9ebd76a8d84b45a37e6c99",
        "commit_message": "HADOOP-15531. Use commons-text instead of commons-lang in some classes to fix deprecation warnings. Contributed by Takanobu Asanuma.",
        "commit_url": "https://github.com/apache/hadoop/commit/88625f5cd90766136a9ebd76a8d84b45a37e6c99",
        "buggy_code": "import org.apache.commons.lang3.StringEscapeUtils;",
        "fixed_code": "import org.apache.commons.text.StringEscapeUtils;",
        "patch": "@@ -32,7 +32,7 @@\n import java.util.EnumSet;\n import java.util.List;\n \n-import org.apache.commons.lang3.StringEscapeUtils;\n+import org.apache.commons.text.StringEscapeUtils;\n import org.apache.commons.lang3.Range;\n import org.apache.hadoop.security.UserGroupInformation;\n import org.apache.hadoop.util.StringUtils;"
    },
    {
        "commit_id": "88625f5cd90766136a9ebd76a8d84b45a37e6c99",
        "commit_message": "HADOOP-15531. Use commons-text instead of commons-lang in some classes to fix deprecation warnings. Contributed by Takanobu Asanuma.",
        "commit_url": "https://github.com/apache/hadoop/commit/88625f5cd90766136a9ebd76a8d84b45a37e6c99",
        "buggy_code": "import org.apache.commons.lang3.StringEscapeUtils;",
        "fixed_code": "import org.apache.commons.text.StringEscapeUtils;",
        "patch": "@@ -29,7 +29,7 @@\n import java.util.concurrent.ConcurrentHashMap;\n import java.util.concurrent.ConcurrentMap;\n \n-import org.apache.commons.lang3.StringEscapeUtils;\n+import org.apache.commons.text.StringEscapeUtils;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.util.StringUtils;\n import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;"
    },
    {
        "commit_id": "88625f5cd90766136a9ebd76a8d84b45a37e6c99",
        "commit_message": "HADOOP-15531. Use commons-text instead of commons-lang in some classes to fix deprecation warnings. Contributed by Takanobu Asanuma.",
        "commit_url": "https://github.com/apache/hadoop/commit/88625f5cd90766136a9ebd76a8d84b45a37e6c99",
        "buggy_code": "import org.apache.commons.lang3.StringEscapeUtils;",
        "fixed_code": "import org.apache.commons.text.StringEscapeUtils;",
        "patch": "@@ -27,7 +27,7 @@\n import java.util.Collection;\n import java.util.List;\n \n-import org.apache.commons.lang3.StringEscapeUtils;\n+import org.apache.commons.text.StringEscapeUtils;\n import org.apache.commons.lang3.StringUtils;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.yarn.api.protocolrecords.GetApplicationAttemptReportRequest;"
    },
    {
        "commit_id": "88625f5cd90766136a9ebd76a8d84b45a37e6c99",
        "commit_message": "HADOOP-15531. Use commons-text instead of commons-lang in some classes to fix deprecation warnings. Contributed by Takanobu Asanuma.",
        "commit_url": "https://github.com/apache/hadoop/commit/88625f5cd90766136a9ebd76a8d84b45a37e6c99",
        "buggy_code": "import org.apache.commons.lang3.StringEscapeUtils;",
        "fixed_code": "import org.apache.commons.text.StringEscapeUtils;",
        "patch": "@@ -25,7 +25,7 @@\n import java.util.List;\n import java.util.Set;\n \n-import org.apache.commons.lang3.StringEscapeUtils;\n+import org.apache.commons.text.StringEscapeUtils;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.security.UserGroupInformation;\n import org.apache.hadoop.yarn.api.protocolrecords.GetApplicationAttemptsRequest;"
    },
    {
        "commit_id": "88625f5cd90766136a9ebd76a8d84b45a37e6c99",
        "commit_message": "HADOOP-15531. Use commons-text instead of commons-lang in some classes to fix deprecation warnings. Contributed by Takanobu Asanuma.",
        "commit_url": "https://github.com/apache/hadoop/commit/88625f5cd90766136a9ebd76a8d84b45a37e6c99",
        "buggy_code": "import org.apache.commons.lang3.StringEscapeUtils;",
        "fixed_code": "import org.apache.commons.text.StringEscapeUtils;",
        "patch": "@@ -26,7 +26,7 @@\n import java.util.List;\n import java.util.Set;\n \n-import org.apache.commons.lang3.StringEscapeUtils;\n+import org.apache.commons.text.StringEscapeUtils;\n import org.apache.hadoop.util.StringUtils;\n import org.apache.hadoop.yarn.api.protocolrecords.GetApplicationsRequest;\n import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;"
    },
    {
        "commit_id": "73746c5da76d5e39df131534a1ec35dfc5d2529b",
        "commit_message": "HDFS-13707. [PROVIDED Storage] Fix failing integration tests in ITestProvidedImplementation. Contributed by Virajith Jalaparti.",
        "commit_url": "https://github.com/apache/hadoop/commit/73746c5da76d5e39df131534a1ec35dfc5d2529b",
        "buggy_code": "conf.set(DFSConfigKeys.DFS_PROVIDED_ALIASMAP_TEXT_DELIMITER, \",\");",
        "fixed_code": "conf.set(DFSConfigKeys.DFS_PROVIDED_ALIASMAP_TEXT_DELIMITER, \"\\t\");",
        "patch": "@@ -132,7 +132,7 @@ public void setSeed() throws Exception {\n         nnDirPath.toString());\n     conf.set(DFSConfigKeys.DFS_PROVIDED_ALIASMAP_TEXT_READ_FILE,\n         new Path(nnDirPath, fileNameFromBlockPoolID(bpid)).toString());\n-    conf.set(DFSConfigKeys.DFS_PROVIDED_ALIASMAP_TEXT_DELIMITER, \",\");\n+    conf.set(DFSConfigKeys.DFS_PROVIDED_ALIASMAP_TEXT_DELIMITER, \"\\t\");\n \n     conf.set(MiniDFSCluster.HDFS_MINIDFS_BASEDIR_PROVIDED,\n         new File(providedPath.toUri()).toString());"
    },
    {
        "commit_id": "d3fa83a44b01c85f39bfb4deaf2972912ac61ca3",
        "commit_message": "HDFS-13705:The native ISA-L library loading failure should be made warning rather than an error message. Contributed by Shashikant Banerjee.",
        "commit_url": "https://github.com/apache/hadoop/commit/d3fa83a44b01c85f39bfb4deaf2972912ac61ca3",
        "buggy_code": "LOG.error(\"Loading ISA-L failed\", t);",
        "fixed_code": "LOG.warn(problem);",
        "patch": "@@ -46,7 +46,7 @@ public final class ErasureCodeNative {\n         loadLibrary();\n       } catch (Throwable t) {\n         problem = \"Loading ISA-L failed: \" + t.getMessage();\n-        LOG.error(\"Loading ISA-L failed\", t);\n+        LOG.warn(problem);\n       }\n       LOADING_FAILURE_REASON = problem;\n     }"
    },
    {
        "commit_id": "1e30547642c7c6c014745862dd06f90f091f90b6",
        "commit_message": "HDDS-170. Fix TestBlockDeletingService#testBlockDeletionTimeout. Contributed by Lokesh Jain.",
        "commit_url": "https://github.com/apache/hadoop/commit/1e30547642c7c6c014745862dd06f90f091f90b6",
        "buggy_code": "? taskResultFuture.get(serviceTimeout, TimeUnit.MILLISECONDS)",
        "fixed_code": "? taskResultFuture.get(serviceTimeout, unit)",
        "patch": "@@ -126,7 +126,7 @@ public synchronized void run() {\n         try {\n           // Collect task results\n           BackgroundTaskResult result = serviceTimeout > 0\n-              ? taskResultFuture.get(serviceTimeout, TimeUnit.MILLISECONDS)\n+              ? taskResultFuture.get(serviceTimeout, unit)\n               : taskResultFuture.get();\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"task execution result size {}\", result.getSize());"
    },
    {
        "commit_id": "1e30547642c7c6c014745862dd06f90f091f90b6",
        "commit_message": "HDDS-170. Fix TestBlockDeletingService#testBlockDeletionTimeout. Contributed by Lokesh Jain.",
        "commit_url": "https://github.com/apache/hadoop/commit/1e30547642c7c6c014745862dd06f90f091f90b6",
        "buggy_code": "svcInterval, serviceTimeout, ozoneConfig);",
        "fixed_code": "svcInterval, serviceTimeout, TimeUnit.MILLISECONDS, ozoneConfig);",
        "patch": "@@ -122,7 +122,7 @@ public OzoneContainer(\n         OZONE_BLOCK_DELETING_SERVICE_TIMEOUT,\n         OZONE_BLOCK_DELETING_SERVICE_TIMEOUT_DEFAULT, TimeUnit.MILLISECONDS);\n     this.blockDeletingService = new BlockDeletingService(manager,\n-        svcInterval, serviceTimeout, ozoneConfig);\n+        svcInterval, serviceTimeout, TimeUnit.MILLISECONDS, ozoneConfig);\n \n     this.dispatcher = new Dispatcher(manager, this.ozoneConfig);\n "
    },
    {
        "commit_id": "1e0d4b1c283fb98a95c60a1723f594befb3c18a9",
        "commit_message": "HDFS-13618. Fix TestDataNodeFaultInjector test failures on Windows. Contributed by Xiao Liang.",
        "commit_url": "https://github.com/apache/hadoop/commit/1e0d4b1c283fb98a95c60a1723f594befb3c18a9",
        "buggy_code": "PathUtils.getTestDir(getClass()).getAbsolutePath(),",
        "fixed_code": "PathUtils.getTestDir(getClass()).getPath(),",
        "patch": "@@ -118,7 +118,7 @@ private void verifyFaultInjectionDelayPipeline(\n       final MetricsDataNodeFaultInjector mdnFaultInjector) throws Exception {\n \n     final Path baseDir = new Path(\n-        PathUtils.getTestDir(getClass()).getAbsolutePath(),\n+        PathUtils.getTestDir(getClass()).getPath(),\n         GenericTestUtils.getMethodName());\n     final DataNodeFaultInjector oldDnInjector = DataNodeFaultInjector.get();\n     DataNodeFaultInjector.set(mdnFaultInjector);"
    },
    {
        "commit_id": "8d5509c68156faaa6641f4e747fc9ff80adccf88",
        "commit_message": "YARN-8292: Fix the dominant resource preemption cannot happen when some of the resource vector becomes negative. Contributed by Wangda Tan.",
        "commit_url": "https://github.com/apache/hadoop/commit/8d5509c68156faaa6641f4e747fc9ff80adccf88",
        "buggy_code": "totalPreemptedResourceAllowed);",
        "fixed_code": "totalPreemptedResourceAllowed, true);",
        "patch": "@@ -230,7 +230,7 @@ private void preemptFromLeastStarvedApp(LeafQueue leafQueue,\n       boolean ret = CapacitySchedulerPreemptionUtils\n           .tryPreemptContainerAndDeductResToObtain(rc, preemptionContext,\n               resToObtainByPartition, c, clusterResource, selectedCandidates,\n-              totalPreemptedResourceAllowed);\n+              totalPreemptedResourceAllowed, true);\n \n       // Subtract from respective user's resource usage once a container is\n       // selected for preemption."
    },
    {
        "commit_id": "7e26e1f2166d2238a63a4086061a21e60e253605",
        "commit_message": "HDDS-52. Fix TestSCMCli#testInfoContainer.\nContributed by Mukul Kumar Singh.",
        "commit_url": "https://github.com/apache/hadoop/commit/7e26e1f2166d2238a63a4086061a21e60e253605",
        "buggy_code": ".format(formatStrWithHash, container.getContainerID(), openStatus,",
        "fixed_code": ".format(formatStr, container.getContainerID(), openStatus,",
        "patch": "@@ -331,7 +331,7 @@ public void testInfoContainer() throws Exception {\n \n     openStatus = data.isOpen() ? \"OPEN\" : \"CLOSED\";\n     expected = String\n-        .format(formatStrWithHash, container.getContainerID(), openStatus,\n+        .format(formatStr, container.getContainerID(), openStatus,\n             data.getDBPath(), data.getContainerPath(), \"\",\n             datanodeDetails.getHostName(), datanodeDetails.getHostName());\n     assertEquals(expected, out.toString());"
    },
    {
        "commit_id": "17974ba3a6fa29c612a9fa147bebba83c0800c2a",
        "commit_message": "HDFS-13444. Ozone: Fix checkstyle issues in HDFS-7240. Contributed by Lokesh Jain.",
        "commit_url": "https://github.com/apache/hadoop/commit/17974ba3a6fa29c612a9fa147bebba83c0800c2a",
        "buggy_code": "public class HddsConfigKeys {",
        "fixed_code": "public final class HddsConfigKeys {",
        "patch": "@@ -1,6 +1,6 @@\n package org.apache.hadoop.hdds;\n \n-public class HddsConfigKeys {\n+public final class HddsConfigKeys {\n   private HddsConfigKeys() {\n   }\n }"
    },
    {
        "commit_id": "17974ba3a6fa29c612a9fa147bebba83c0800c2a",
        "commit_message": "HDFS-13444. Ozone: Fix checkstyle issues in HDFS-7240. Contributed by Lokesh Jain.",
        "commit_url": "https://github.com/apache/hadoop/commit/17974ba3a6fa29c612a9fa147bebba83c0800c2a",
        "buggy_code": "public class HddsUtils {",
        "fixed_code": "public final class HddsUtils {",
        "patch": "@@ -48,7 +48,7 @@\n /**\n  * HDDS specific stateless utility functions.\n  */\n-public class HddsUtils {\n+public final class HddsUtils {\n \n \n   private static final Logger LOG = LoggerFactory.getLogger(HddsUtils.class);"
    },
    {
        "commit_id": "17974ba3a6fa29c612a9fa147bebba83c0800c2a",
        "commit_message": "HDFS-13444. Ozone: Fix checkstyle issues in HDFS-7240. Contributed by Lokesh Jain.",
        "commit_url": "https://github.com/apache/hadoop/commit/17974ba3a6fa29c612a9fa147bebba83c0800c2a",
        "buggy_code": "public static void main(String args[]) {",
        "fixed_code": "public static void main(String[] args) {",
        "patch": "@@ -227,7 +227,7 @@ public static HddsDatanodeService createHddsDatanodeService(\n     return new HddsDatanodeService(conf);\n   }\n \n-  public static void main(String args[]) {\n+  public static void main(String[] args) {\n     try {\n       StringUtils.startupShutdownMessage(HddsDatanodeService.class, args, LOG);\n       HddsDatanodeService hddsDatanodeService ="
    },
    {
        "commit_id": "17974ba3a6fa29c612a9fa147bebba83c0800c2a",
        "commit_message": "HDFS-13444. Ozone: Fix checkstyle issues in HDFS-7240. Contributed by Lokesh Jain.",
        "commit_url": "https://github.com/apache/hadoop/commit/17974ba3a6fa29c612a9fa147bebba83c0800c2a",
        "buggy_code": "public class ContainerTestUtils {",
        "fixed_code": "public final class ContainerTestUtils {",
        "patch": "@@ -34,7 +34,7 @@\n /**\n  * Helper utility to test containers.\n  */\n-public class ContainerTestUtils {\n+public final class ContainerTestUtils {\n \n   private ContainerTestUtils() {\n   }"
    },
    {
        "commit_id": "17974ba3a6fa29c612a9fa147bebba83c0800c2a",
        "commit_message": "HDFS-13444. Ozone: Fix checkstyle issues in HDFS-7240. Contributed by Lokesh Jain.",
        "commit_url": "https://github.com/apache/hadoop/commit/17974ba3a6fa29c612a9fa147bebba83c0800c2a",
        "buggy_code": "public class ServerUtils {",
        "fixed_code": "public final class ServerUtils {",
        "patch": "@@ -32,7 +32,7 @@\n /**\n  * Generic utilities for all HDDS/Ozone servers.\n  */\n-public class ServerUtils {\n+public final class ServerUtils {\n \n   private static final Logger LOG = LoggerFactory.getLogger(\n       ServerUtils.class);"
    },
    {
        "commit_id": "17974ba3a6fa29c612a9fa147bebba83c0800c2a",
        "commit_message": "HDFS-13444. Ozone: Fix checkstyle issues in HDFS-7240. Contributed by Lokesh Jain.",
        "commit_url": "https://github.com/apache/hadoop/commit/17974ba3a6fa29c612a9fa147bebba83c0800c2a",
        "buggy_code": "public class TestUtils {",
        "fixed_code": "public final class TestUtils {",
        "patch": "@@ -27,7 +27,7 @@\n /**\n  * Stateless helper functions to handler scm/datanode connection.\n  */\n-public class TestUtils {\n+public final class TestUtils {\n \n   private TestUtils() {\n   }"
    },
    {
        "commit_id": "17974ba3a6fa29c612a9fa147bebba83c0800c2a",
        "commit_message": "HDFS-13444. Ozone: Fix checkstyle issues in HDFS-7240. Contributed by Lokesh Jain.",
        "commit_url": "https://github.com/apache/hadoop/commit/17974ba3a6fa29c612a9fa147bebba83c0800c2a",
        "buggy_code": "public void logOut(String msg, String ... variable) {",
        "fixed_code": "public void logOut(String msg, String... variable) {",
        "patch": "@@ -59,7 +59,7 @@ public void setErr(PrintStream err) {\n     this.err = err;\n   }\n \n-  public void logOut(String msg, String ... variable) {\n+  public void logOut(String msg, String... variable) {\n     this.out.println(String.format(msg, variable));\n   }\n "
    },
    {
        "commit_id": "17974ba3a6fa29c612a9fa147bebba83c0800c2a",
        "commit_message": "HDFS-13444. Ozone: Fix checkstyle issues in HDFS-7240. Contributed by Lokesh Jain.",
        "commit_url": "https://github.com/apache/hadoop/commit/17974ba3a6fa29c612a9fa147bebba83c0800c2a",
        "buggy_code": "public class KsmUtils {",
        "fixed_code": "public final class KsmUtils {",
        "patch": "@@ -34,7 +34,7 @@\n  * Stateless helper functions for the server and client side of KSM\n  * communication.\n  */\n-public class KsmUtils {\n+public final class KsmUtils {\n \n   private KsmUtils() {\n   }"
    },
    {
        "commit_id": "17974ba3a6fa29c612a9fa147bebba83c0800c2a",
        "commit_message": "HDFS-13444. Ozone: Fix checkstyle issues in HDFS-7240. Contributed by Lokesh Jain.",
        "commit_url": "https://github.com/apache/hadoop/commit/17974ba3a6fa29c612a9fa147bebba83c0800c2a",
        "buggy_code": "package org.apache.hadoop.hdds.scm.node;",
        "fixed_code": "package org.apache.hadoop.ozone.scm.node;",
        "patch": "@@ -14,7 +14,7 @@\n  * License for the specific language governing permissions and limitations under\n  * the License.\n  */\n-package org.apache.hadoop.hdds.scm.node;\n+package org.apache.hadoop.ozone.scm.node;\n \n import org.apache.hadoop.ozone.MiniOzoneCluster;\n import org.apache.hadoop.hdds.conf.OzoneConfiguration;"
    },
    {
        "commit_id": "025058f251a06a9fbe589ff1833d7de85ecd8b3e",
        "commit_message": "HDFS-13446. Ozone: Fix OzoneFileSystem contract test failures. Contributed by Mukul Kumar Singh.",
        "commit_url": "https://github.com/apache/hadoop/commit/025058f251a06a9fbe589ff1833d7de85ecd8b3e",
        "buggy_code": "org.apache.hadoop.hdfs.server.namenode.TestNameNodeHttpServer.class);",
        "fixed_code": "TestStorageContainerManagerHttpServer.class);",
        "patch": "@@ -76,7 +76,7 @@ public TestStorageContainerManagerHttpServer(Policy policy) {\n     conf = new Configuration();\n     keystoresDir = new File(BASEDIR).getAbsolutePath();\n     sslConfDir = KeyStoreTestUtil.getClasspathDir(\n-        org.apache.hadoop.hdfs.server.namenode.TestNameNodeHttpServer.class);\n+        TestStorageContainerManagerHttpServer.class);\n     KeyStoreTestUtil.setupSSLConfig(keystoresDir, sslConfDir, conf, false);\n     connectionFactory =\n         URLConnectionFactory.newDefaultURLConnectionFactory(conf);"
    },
    {
        "commit_id": "025058f251a06a9fbe589ff1833d7de85ecd8b3e",
        "commit_message": "HDFS-13446. Ozone: Fix OzoneFileSystem contract test failures. Contributed by Mukul Kumar Singh.",
        "commit_url": "https://github.com/apache/hadoop/commit/025058f251a06a9fbe589ff1833d7de85ecd8b3e",
        "buggy_code": "List<SCMCommand> list = commandMap.get(dd);",
        "fixed_code": "List<SCMCommand> list = commandMap.get(dd.getUuid());",
        "patch": "@@ -315,7 +315,7 @@ public void addDatanodeCommand(UUID dnId, SCMCommand command) {\n \n   // Returns the number of commands that is queued to this node manager.\n   public int getCommandCount(DatanodeDetails dd) {\n-    List<SCMCommand> list = commandMap.get(dd);\n+    List<SCMCommand> list = commandMap.get(dd.getUuid());\n     return (list == null) ? 0 : list.size();\n   }\n "
    },
    {
        "commit_id": "cd807c9890baf0778d782f8ed5f036b5f78b29d5",
        "commit_message": "HDFS-13133. Ozone: OzoneFileSystem: Calling delete with non-existing path shouldn't be logged on ERROR level. Contributed by Elek, Marton.",
        "commit_url": "https://github.com/apache/hadoop/commit/cd807c9890baf0778d782f8ed5f036b5f78b29d5",
        "buggy_code": "LOG.error(\"Couldn't delete {} - does not exist\", f);",
        "fixed_code": "LOG.debug(\"Couldn't delete {} - does not exist\", f);",
        "patch": "@@ -410,7 +410,7 @@ public boolean delete(Path f, boolean recursive) throws IOException {\n       DeleteIterator iterator = new DeleteIterator(f, recursive);\n       return iterator.iterate();\n     } catch (FileNotFoundException e) {\n-      LOG.error(\"Couldn't delete {} - does not exist\", f);\n+      LOG.debug(\"Couldn't delete {} - does not exist\", f);\n       return false;\n     }\n   }"
    },
    {
        "commit_id": "740a06cdd78cd1af98855b452e85c918e295a095",
        "commit_message": "HDFS-12719. Ozone: Fix checkstyle, javac, whitespace issues in HDFS-7240 branch. Contributed by Mukul Kumar Singh.",
        "commit_url": "https://github.com/apache/hadoop/commit/740a06cdd78cd1af98855b452e85c918e295a095",
        "buggy_code": "IOUtils.cleanup(null, storageContainerLocationClient, cluster);",
        "fixed_code": "IOUtils.cleanupWithLogger(null, storageContainerLocationClient, cluster);",
        "patch": "@@ -83,7 +83,7 @@ public static void shutdown() throws InterruptedException {\n     if (cluster != null) {\n       cluster.shutdown();\n     }\n-    IOUtils.cleanup(null, storageContainerLocationClient, cluster);\n+    IOUtils.cleanupWithLogger(null, storageContainerLocationClient, cluster);\n   }\n \n   /**"
    },
    {
        "commit_id": "740a06cdd78cd1af98855b452e85c918e295a095",
        "commit_message": "HDFS-12719. Ozone: Fix checkstyle, javac, whitespace issues in HDFS-7240 branch. Contributed by Mukul Kumar Singh.",
        "commit_url": "https://github.com/apache/hadoop/commit/740a06cdd78cd1af98855b452e85c918e295a095",
        "buggy_code": "IOUtils.cleanup(null, storageContainerLocationClient, cluster);",
        "fixed_code": "IOUtils.cleanupWithLogger(null, storageContainerLocationClient, cluster);",
        "patch": "@@ -90,7 +90,7 @@ public static void shutdown() throws InterruptedException {\n     if (cluster != null) {\n       cluster.shutdown();\n     }\n-    IOUtils.cleanup(null, storageContainerLocationClient, cluster);\n+    IOUtils.cleanupWithLogger(null, storageContainerLocationClient, cluster);\n   }\n \n   /**"
    },
    {
        "commit_id": "740a06cdd78cd1af98855b452e85c918e295a095",
        "commit_message": "HDFS-12719. Ozone: Fix checkstyle, javac, whitespace issues in HDFS-7240 branch. Contributed by Mukul Kumar Singh.",
        "commit_url": "https://github.com/apache/hadoop/commit/740a06cdd78cd1af98855b452e85c918e295a095",
        "buggy_code": "IOUtils.cleanup(null, storageContainerLocationClient, cluster);",
        "fixed_code": "IOUtils.cleanupWithLogger(null, storageContainerLocationClient, cluster);",
        "patch": "@@ -96,7 +96,7 @@ public static void shutdown() throws InterruptedException {\n     if (cluster != null) {\n       cluster.shutdown();\n     }\n-    IOUtils.cleanup(null, storageContainerLocationClient, cluster);\n+    IOUtils.cleanupWithLogger(null, storageContainerLocationClient, cluster);\n   }\n \n   /**"
    },
    {
        "commit_id": "740a06cdd78cd1af98855b452e85c918e295a095",
        "commit_message": "HDFS-12719. Ozone: Fix checkstyle, javac, whitespace issues in HDFS-7240 branch. Contributed by Mukul Kumar Singh.",
        "commit_url": "https://github.com/apache/hadoop/commit/740a06cdd78cd1af98855b452e85c918e295a095",
        "buggy_code": "public class Constants {",
        "fixed_code": "public final class Constants {",
        "patch": "@@ -21,7 +21,7 @@\n /**\n  * Constants for Ozone FileSystem implementation.\n  */\n-public class Constants {\n+public final class Constants {\n \n   public static final String OZONE_URI_SCHEME = \"o3\";\n "
    },
    {
        "commit_id": "740a06cdd78cd1af98855b452e85c918e295a095",
        "commit_message": "HDFS-12719. Ozone: Fix checkstyle, javac, whitespace issues in HDFS-7240 branch. Contributed by Mukul Kumar Singh.",
        "commit_url": "https://github.com/apache/hadoop/commit/740a06cdd78cd1af98855b452e85c918e295a095",
        "buggy_code": "public OzoneContract(Configuration conf) {",
        "fixed_code": "OzoneContract(Configuration conf) {",
        "patch": "@@ -48,7 +48,7 @@ class OzoneContract extends AbstractFSContract {\n   private static StorageHandler storageHandler;\n   private static final String CONTRACT_XML = \"contract/ozone.xml\";\n \n-  public OzoneContract(Configuration conf) {\n+  OzoneContract(Configuration conf) {\n     super(conf);\n     //insert the base features\n     addConfResource(CONTRACT_XML);"
    },
    {
        "commit_id": "d19b4c87633d39bc4939a826f2a45f41287ff1ca",
        "commit_message": "HDFS-12583. Ozone: Fix swallow exceptions which makes hard to debug failures. Contributed by Yiqun Lin.",
        "commit_url": "https://github.com/apache/hadoop/commit/d19b4c87633d39bc4939a826f2a45f41287ff1ca",
        "buggy_code": "throw new IOException(\"Unexpected OzoneException\", e);",
        "fixed_code": "throw new IOException(\"Unexpected OzoneException: \" + e.toString(), e);",
        "patch": "@@ -180,7 +180,7 @@ private synchronized void readChunkFromContainer(int readChunkOffset)\n       readChunkResponse = ContainerProtocolCalls.readChunk(xceiverClient,\n           chunks.get(readChunkOffset), key, traceID);\n     } catch (IOException e) {\n-      throw new IOException(\"Unexpected OzoneException\", e);\n+      throw new IOException(\"Unexpected OzoneException: \" + e.toString(), e);\n     }\n     chunkOffset = readChunkOffset;\n     ByteString byteString = readChunkResponse.getData();"
    },
    {
        "commit_id": "e3b51d90749b3e5e53ed7b3f75b7e4d2a20e6cca",
        "commit_message": "HDFS-12598. Ozone: Fix 3 node ratis replication in Ozone.  Contributed by Mukul Kumar Singh",
        "commit_url": "https://github.com/apache/hadoop/commit/e3b51d90749b3e5e53ed7b3f75b7e4d2a20e6cca",
        "buggy_code": "info.getState());",
        "fixed_code": "blockInfo.getState());",
        "patch": "@@ -274,7 +274,7 @@ public ContainerInfo allocateContainer(PipelineSelector selector, OzoneProtos\n     writeLock.lock();\n     try {\n       ContainerKey key = new ContainerKey(owner, type, replicationFactor,\n-          info.getState());\n+          blockInfo.getState());\n       PriorityQueue<BlockContainerInfo> queue = containers.get(key);\n       Preconditions.checkNotNull(queue);\n       queue.add(blockInfo);"
    },
    {
        "commit_id": "13fdb584906e0992e30f08c8a1df7a3c10f742bf",
        "commit_message": "HDFS-12554. Ozone: Fix TestDatanodeStateMachine#testDatanodeStateMachineWithInvalidConfiguration. Contributed by Ajay Kumar.",
        "commit_url": "https://github.com/apache/hadoop/commit/13fdb584906e0992e30f08c8a1df7a3c10f742bf",
        "buggy_code": "if (Strings.isNullOrEmpty(dataNodeIDPath)) {",
        "fixed_code": "if (dataNodeIDPath == null) {",
        "patch": "@@ -270,7 +270,7 @@ public static File getScmMetadirPath(Configuration conf) {\n    */\n   public static String getDatanodeIDPath(Configuration conf) {\n     String dataNodeIDPath = conf.get(ScmConfigKeys.OZONE_SCM_DATANODE_ID);\n-    if (Strings.isNullOrEmpty(dataNodeIDPath)) {\n+    if (dataNodeIDPath == null) {\n       String metaPath = conf.get(OzoneConfigKeys.OZONE_METADATA_DIRS);\n       if (Strings.isNullOrEmpty(metaPath)) {\n         // this means meta data is not found, in theory should not happen at"
    },
    {
        "commit_id": "eda559ffd2588a56f136c867b8ad3a6f1c56473c",
        "commit_message": "HDFS-12181. Ozone: Fix TestContainerReplicationManager by setting proper log level for LogCapturer. Contributed by Mukul Kumar Singh.",
        "commit_url": "https://github.com/apache/hadoop/commit/eda559ffd2588a56f136c867b8ad3a6f1c56473c",
        "buggy_code": "static final Logger LOG =",
        "fixed_code": "public static final Logger LOG =",
        "patch": "@@ -64,7 +64,7 @@\n  * computes the replication levels for each container.\n  */\n public class ContainerReplicationManager implements Closeable {\n-  static final Logger LOG =\n+  public static final Logger LOG =\n       LoggerFactory.getLogger(ContainerReplicationManager.class);\n \n   private final NodePoolManager poolManager;"
    },
    {
        "commit_id": "938728744c7e1af7ccf78dfc2d46a1d302cfacb5",
        "commit_message": "HDFS-11845. Ozone: Output error when DN handshakes with SCM. Contributed by Weiwei Yang",
        "commit_url": "https://github.com/apache/hadoop/commit/938728744c7e1af7ccf78dfc2d46a1d302cfacb5",
        "buggy_code": "100;",
        "fixed_code": "1000;",
        "patch": "@@ -122,7 +122,7 @@ public final class ScmConfigKeys {\n   public static final String OZONE_SCM_HEARTBEAT_RPC_TIMEOUT =\n       \"ozone.scm.heartbeat.rpc-timeout\";\n   public static final long OZONE_SCM_HEARTBEAT_RPC_TIMEOUT_DEFAULT =\n-      100;\n+      1000;\n \n   /**\n    * Defines how frequently we will log the missing of heartbeat to a specific"
    },
    {
        "commit_id": "e9d09c209ed015b53e0a654d5c643aca238e5330",
        "commit_message": "HDFS-11824. Ozone: Fix javac warnings. Contributed by Yiqun Lin.",
        "commit_url": "https://github.com/apache/hadoop/commit/e9d09c209ed015b53e0a654d5c643aca238e5330",
        "buggy_code": "return Optional.of(HostAndPort.fromString(value).getHostText());",
        "fixed_code": "return Optional.of(HostAndPort.fromString(value).getHost());",
        "patch": "@@ -281,7 +281,7 @@ public static Optional<String> getHostName(String value) {\n     if ((value == null) || value.isEmpty()) {\n       return Optional.absent();\n     }\n-    return Optional.of(HostAndPort.fromString(value).getHostText());\n+    return Optional.of(HostAndPort.fromString(value).getHost());\n   }\n \n   /**"
    },
    {
        "commit_id": "5984d40b86d297deb6b6f0971bdff1b8aab23d80",
        "commit_message": "HDFS-11666. Ozone: Fix compile error due to inconsistent package name. Contributed by Yiqun Lin.",
        "commit_url": "https://github.com/apache/hadoop/commit/5984d40b86d297deb6b6f0971bdff1b8aab23d80",
        "buggy_code": "import org.apache.hadoop.ozone.protocol.proto.StorageContainerDatanodeProtocolProtos.Type;;",
        "fixed_code": "import org.apache.hadoop.ozone.protocol.proto.StorageContainerDatanodeProtocolProtos.Type;",
        "patch": "@@ -18,7 +18,7 @@\n package org.apache.hadoop.ozone.container.common.statemachine.commandhandler;\n \n import com.google.common.base.Preconditions;\n-import org.apache.hadoop.ozone.protocol.proto.StorageContainerDatanodeProtocolProtos.Type;;\n+import org.apache.hadoop.ozone.protocol.proto.StorageContainerDatanodeProtocolProtos.Type;\n import org.apache.hadoop.ozone.container.common.statemachine.SCMConnectionManager;\n import org.apache.hadoop.ozone.container.common.statemachine.StateContext;\n import org.apache.hadoop.ozone.container.ozoneimpl.OzoneContainer;"
    },
    {
        "commit_id": "32cc2b8f1a3561fd922f6a7e662c1617a1871fb3",
        "commit_message": "HDFS-11509. Ozone: Fix TestEndpoint test regression. Contributed by Anu Engineer.",
        "commit_url": "https://github.com/apache/hadoop/commit/32cc2b8f1a3561fd922f6a7e662c1617a1871fb3",
        "buggy_code": "newBuilder().setState(noContainerReports).build();",
        "fixed_code": "newBuilder().setState(noContainerReports).setCount(0).build();",
        "patch": "@@ -92,7 +92,7 @@ public static void setUp() throws Exception {\n         scmServerImpl, serverAddress, 10);\n     testDir = PathUtils.getTestDir(TestEndPoint.class);\n     defaultReportState = StorageContainerDatanodeProtocolProtos.ReportState.\n-        newBuilder().setState(noContainerReports).build();\n+        newBuilder().setState(noContainerReports).setCount(0).build();\n   }\n \n   @Test"
    },
    {
        "commit_id": "964daed8532fa389c51ee3401da8d1a8ca04850f",
        "commit_message": "fix a build break due to merge",
        "commit_url": "https://github.com/apache/hadoop/commit/964daed8532fa389c51ee3401da8d1a8ca04850f",
        "buggy_code": "storageContainerList, context, r == (reports.length - 1));",
        "fixed_code": "storageContainerList, context);",
        "patch": "@@ -247,7 +247,7 @@ public DatanodeCommand blockReport(DatanodeRegistration registration,\n     for (int r = 0; r < reports.length; r++) {\n       final BlockListAsLongs storageContainerList = reports[r].getBlocks();\n       blockManager.processReport(registration, reports[r].getStorage(),\n-          storageContainerList, context, r == (reports.length - 1));\n+          storageContainerList, context);\n     }\n     return null;\n   }"
    },
    {
        "commit_id": "60fbef08ec38913d00e8786f3bd527bb61a779d8",
        "commit_message": "HDFS-9925. Ozone: Add Ozone Client lib for bucket handling. Contributed by Anu Engineer.\n\nFix build break",
        "commit_url": "https://github.com/apache/hadoop/commit/60fbef08ec38913d00e8786f3bd527bb61a779d8",
        "buggy_code": "return Collections.unmodifiableList(buckets);",
        "fixed_code": "return buckets;",
        "patch": "@@ -75,7 +75,7 @@ public static ListBuckets parse(String data) throws IOException {\n    * @return Bucket list\n    */\n   public List<BucketInfo> getBuckets() {\n-    return Collections.unmodifiableList(buckets);\n+    return buckets;\n   }\n \n   /**"
    },
    {
        "commit_id": "c4d3636c21acaeb2b7d56d19cd4996aa25151bd1",
        "commit_message": "HDFS-13435. RBF: Improve the error loggings for printing the stack trace.",
        "commit_url": "https://github.com/apache/hadoop/commit/c4d3636c21acaeb2b7d56d19cd4996aa25151bd1",
        "buggy_code": "LOG.info(\"Cannot get the live nodes: {}\", e.getMessage());",
        "fixed_code": "LOG.error(\"Cannot get the live nodes: {}\", e.getMessage());",
        "patch": "@@ -456,7 +456,7 @@ public String getNodeUsage() {\n         dev = (float) Math.sqrt(dev / usages.length);\n       }\n     } catch (IOException e) {\n-      LOG.info(\"Cannot get the live nodes: {}\", e.getMessage());\n+      LOG.error(\"Cannot get the live nodes: {}\", e.getMessage());\n     }\n \n     final Map<String, Object> innerInfo = new HashMap<>();"
    },
    {
        "commit_id": "c4d3636c21acaeb2b7d56d19cd4996aa25151bd1",
        "commit_message": "HDFS-13435. RBF: Improve the error loggings for printing the stack trace.",
        "commit_url": "https://github.com/apache/hadoop/commit/c4d3636c21acaeb2b7d56d19cd4996aa25151bd1",
        "buggy_code": "LOG.info(\"Failed to register State Store bean {}\", e.getMessage());",
        "fixed_code": "LOG.error(\"Failed to register State Store bean {}\", e.getMessage());",
        "patch": "@@ -183,7 +183,7 @@ protected void serviceInit(Configuration config) throws Exception {\n     } catch (NotCompliantMBeanException e) {\n       throw new RuntimeException(\"Bad StateStoreMBean setup\", e);\n     } catch (MetricsException e) {\n-      LOG.info(\"Failed to register State Store bean {}\", e.getMessage());\n+      LOG.error(\"Failed to register State Store bean {}\", e.getMessage());\n     }\n \n     super.serviceInit(this.conf);"
    },
    {
        "commit_id": "c4d3636c21acaeb2b7d56d19cd4996aa25151bd1",
        "commit_message": "HDFS-13435. RBF: Improve the error loggings for printing the stack trace.",
        "commit_url": "https://github.com/apache/hadoop/commit/c4d3636c21acaeb2b7d56d19cd4996aa25151bd1",
        "buggy_code": "LOG.error(\"Cannot close the writer for {}\", recordPathTemp);",
        "fixed_code": "LOG.error(\"Cannot close the writer for {}\", recordPathTemp, e);",
        "patch": "@@ -361,7 +361,7 @@ public <T extends BaseRecord> boolean putAll(\n           try {\n             writer.close();\n           } catch (IOException e) {\n-            LOG.error(\"Cannot close the writer for {}\", recordPathTemp);\n+            LOG.error(\"Cannot close the writer for {}\", recordPathTemp, e);\n           }\n         }\n       }"
    },
    {
        "commit_id": "0c93d43f3d624a4fd17b3b050443d9e7e20d4f0a",
        "commit_message": "HDFS-13045. RBF: Improve error message returned from subcluster. Contributed by Inigo Goiri.",
        "commit_url": "https://github.com/apache/hadoop/commit/0c93d43f3d624a4fd17b3b050443d9e7e20d4f0a",
        "buggy_code": "RemoteLocation location = new RemoteLocation(nsId, path);",
        "fixed_code": "RemoteLocation location = new RemoteLocation(nsId, path, src);",
        "patch": "@@ -134,7 +134,7 @@ public static MountTable newInstance(final String src,\n     for (Entry<String, String> entry : destinations.entrySet()) {\n       String nsId = entry.getKey();\n       String path = normalizeFileSystemPath(entry.getValue());\n-      RemoteLocation location = new RemoteLocation(nsId, path);\n+      RemoteLocation location = new RemoteLocation(nsId, path, src);\n       locations.add(location);\n     }\n "
    },
    {
        "commit_id": "0c93d43f3d624a4fd17b3b050443d9e7e20d4f0a",
        "commit_message": "HDFS-13045. RBF: Improve error message returned from subcluster. Contributed by Inigo Goiri.",
        "commit_url": "https://github.com/apache/hadoop/commit/0c93d43f3d624a4fd17b3b050443d9e7e20d4f0a",
        "buggy_code": "RemoteLocation loc = new RemoteLocation(nsId, path);",
        "fixed_code": "RemoteLocation loc = new RemoteLocation(nsId, path, getSourcePath());",
        "patch": "@@ -102,7 +102,7 @@ public List<RemoteLocation> getDestinations() {\n     for (RemoteLocationProto dest : destList) {\n       String nsId = dest.getNameserviceId();\n       String path = dest.getPath();\n-      RemoteLocation loc = new RemoteLocation(nsId, path);\n+      RemoteLocation loc = new RemoteLocation(nsId, path, getSourcePath());\n       ret.add(loc);\n     }\n     return ret;"
    },
    {
        "commit_id": "3a0f4bc0d513f9ffb4cf571c1ac85395e116fdcc",
        "commit_message": "HDFS-13261. Fix incorrect null value check. Contributed by Jianfei Jiang.",
        "commit_url": "https://github.com/apache/hadoop/commit/3a0f4bc0d513f9ffb4cf571c1ac85395e116fdcc",
        "buggy_code": "Preconditions.checkNotNull(name == null);",
        "fixed_code": "Preconditions.checkNotNull(name, \"zone name cannot be null\");",
        "patch": "@@ -223,7 +223,7 @@ public void reset() {\n    * a listReencryptionStatus call, for the crypto admin to consume.\n    */\n   public void setZoneName(final String name) {\n-    Preconditions.checkNotNull(name == null);\n+    Preconditions.checkNotNull(name, \"zone name cannot be null\");\n     zoneName = name;\n   }\n "
    },
    {
        "commit_id": "3a0f4bc0d513f9ffb4cf571c1ac85395e116fdcc",
        "commit_message": "HDFS-13261. Fix incorrect null value check. Contributed by Jianfei Jiang.",
        "commit_url": "https://github.com/apache/hadoop/commit/3a0f4bc0d513f9ffb4cf571c1ac85395e116fdcc",
        "buggy_code": "Preconditions.checkNotNull(commands != null);",
        "fixed_code": "Preconditions.checkNotNull(commands, \"commands cannot be null.\");",
        "patch": "@@ -145,7 +145,7 @@ static class HelpCommand implements Command {\n     private final Command[] commands;\n \n     public HelpCommand(Command[] commands) {\n-      Preconditions.checkNotNull(commands != null);\n+      Preconditions.checkNotNull(commands, \"commands cannot be null.\");\n       this.commands = commands;\n     }\n "
    },
    {
        "commit_id": "f9dd5b61f4ed0288cc01cb1a676df8c9cd69cdd9",
        "commit_message": "YARN-7811.  Fixed a bug in user defined docker network settings.  (Contributed by Billie Rinaldi)",
        "commit_url": "https://github.com/apache/hadoop/commit/f9dd5b61f4ed0288cc01cb1a676df8c9cd69cdd9",
        "buggy_code": ".getProperty(DOCKER_NETWORK, DEFAULT_DOCKER_NETWORK));",
        "fixed_code": ".getProperty(DOCKER_NETWORK));",
        "patch": "@@ -37,7 +37,7 @@ public void processArtifact(AbstractLauncher launcher,\n     launcher.setYarnDockerMode(true);\n     launcher.setDockerImage(compInstance.getCompSpec().getArtifact().getId());\n     launcher.setDockerNetwork(compInstance.getCompSpec().getConfiguration()\n-        .getProperty(DOCKER_NETWORK, DEFAULT_DOCKER_NETWORK));\n+        .getProperty(DOCKER_NETWORK));\n     String domain = compInstance.getComponent().getScheduler().getConfig()\n         .get(RegistryConstants.KEY_DNS_DOMAIN);\n     String hostname;"
    },
    {
        "commit_id": "d95c13774e1bd5b3cc61bf4da8bae4a93ed0040c",
        "commit_message": "HDFS-12963. Error log level in ShortCircuitRegistry#removeShm. Contributed by hu xiaodong.",
        "commit_url": "https://github.com/apache/hadoop/commit/d95c13774e1bd5b3cc61bf4da8bae4a93ed0040c",
        "buggy_code": "LOG.debug(\"removing shm \" + shm);",
        "fixed_code": "LOG.trace(\"removing shm \" + shm);",
        "patch": "@@ -114,7 +114,7 @@ String getClientName() {\n \n   public synchronized void removeShm(ShortCircuitShm shm) {\n     if (LOG.isTraceEnabled()) {\n-      LOG.debug(\"removing shm \" + shm);\n+      LOG.trace(\"removing shm \" + shm);\n     }\n     // Stop tracking the shmId.\n     RegisteredShm removedShm = segments.remove(shm.getShmId());"
    },
    {
        "commit_id": "37f4696a9cc9284b242215f56a10990e1028d40c",
        "commit_message": "YARN-7740. Fix logging for destroy yarn service cli when app does not exist and some minor bugs. Contributed by Jian He",
        "commit_url": "https://github.com/apache/hadoop/commit/37f4696a9cc9284b242215f56a10990e1028d40c",
        "buggy_code": ".put(SERVICE_ZK_PATH, ServiceRegistryUtils.mkClusterPath(user, app.getName()));",
        "fixed_code": ".put(SERVICE_ZK_PATH, ServiceRegistryUtils.mkServiceHomePath(user, app.getName()));",
        "patch": "@@ -399,7 +399,7 @@ private void initGlobalTokensForSubstitute(ServiceContext context) {\n       LOG.error(\"Failed to get user.\", e);\n     }\n     globalTokens\n-        .put(SERVICE_ZK_PATH, ServiceRegistryUtils.mkClusterPath(user, app.getName()));\n+        .put(SERVICE_ZK_PATH, ServiceRegistryUtils.mkServiceHomePath(user, app.getName()));\n \n     globalTokens.put(ServiceApiConstants.USER, user);\n     String dnsDomain = getConfig().getTrimmed(KEY_DNS_DOMAIN);"
    },
    {
        "commit_id": "a55884c68eb175f1c9f61771386c086bf1ee65a9",
        "commit_message": "YARN-7542. Fix issue that causes some Running Opportunistic Containers to be recovered as PAUSED. (Sampada Dehankar via asuresh)",
        "commit_url": "https://github.com/apache/hadoop/commit/a55884c68eb175f1c9f61771386c086bf1ee65a9",
        "buggy_code": "ContainerEventType.RECOVER_PAUSED_CONTAINER));",
        "fixed_code": "ContainerEventType.CONTAINER_LAUNCHED));",
        "patch": "@@ -72,7 +72,7 @@ public Integer call() {\n     String containerIdStr = containerId.toString();\n \n     dispatcher.getEventHandler().handle(new ContainerEvent(containerId,\n-        ContainerEventType.RECOVER_PAUSED_CONTAINER));\n+        ContainerEventType.CONTAINER_LAUNCHED));\n \n     boolean notInterrupted = true;\n     try {"
    },
    {
        "commit_id": "94a2ac6b719913aa698b66bf40b7ebbe6fa606da",
        "commit_message": "YARN-7466.  addendum patch for failing unit test.  (Contributed by Chandni Singh)",
        "commit_url": "https://github.com/apache/hadoop/commit/94a2ac6b719913aa698b66bf40b7ebbe6fa606da",
        "buggy_code": "labelExpression, 0L);",
        "fixed_code": "labelExpression, -1);",
        "patch": "@@ -177,7 +177,7 @@ public AllocateResponse allocate(\n       List<ContainerId> releases, String labelExpression) throws Exception {\n     List<ResourceRequest> reqs =\n         createReq(new String[] { host }, memory, priority, numContainers,\n-            labelExpression, 0L);\n+            labelExpression, -1);\n     return allocate(reqs, releases);\n   }\n   "
    },
    {
        "commit_id": "80c3fec3a13c41051daaae42e5c9a9fedf5c7ee7",
        "commit_message": "HDFS-12912. [READ] Fix configuration and implementation of LevelDB-based alias maps",
        "commit_url": "https://github.com/apache/hadoop/commit/80c3fec3a13c41051daaae42e5c9a9fedf5c7ee7",
        "buggy_code": ".isThrownBy(() -> InMemoryAliasMap.init(conf)).withMessage(",
        "fixed_code": ".isThrownBy(() -> InMemoryAliasMap.init(conf, \"bpid\")).withMessage(",
        "patch": "@@ -39,7 +39,7 @@ public void testInit() {\n         nonExistingDirectory);\n \n     assertThatExceptionOfType(IOException.class)\n-        .isThrownBy(() -> InMemoryAliasMap.init(conf)).withMessage(\n+        .isThrownBy(() -> InMemoryAliasMap.init(conf, \"bpid\")).withMessage(\n             InMemoryAliasMap.createPathErrorMessage(nonExistingDirectory));\n   }\n }\n\\ No newline at end of file"
    },
    {
        "commit_id": "80c3fec3a13c41051daaae42e5c9a9fedf5c7ee7",
        "commit_message": "HDFS-12912. [READ] Fix configuration and implementation of LevelDB-based alias maps",
        "commit_url": "https://github.com/apache/hadoop/commit/80c3fec3a13c41051daaae42e5c9a9fedf5c7ee7",
        "buggy_code": "config -> aliasMapMock, bpid);",
        "fixed_code": "(config, blockPoolID) -> aliasMapMock, bpid);",
        "patch": "@@ -54,7 +54,7 @@ public void setUp() throws IOException {\n     aliasMapMock = mock(InMemoryAliasMap.class);\n     when(aliasMapMock.getBlockPoolId()).thenReturn(bpid);\n     levelDBAliasMapServer = new InMemoryLevelDBAliasMapServer(\n-        config -> aliasMapMock, bpid);\n+        (config, blockPoolID) -> aliasMapMock, bpid);\n     conf = new Configuration();\n     int port = 9877;\n "
    },
    {
        "commit_id": "ce04340ec73617daff74378056a95c5d0cc0a790",
        "commit_message": "HADOOP-15104. AliyunOSS: change the default value of max error retry. Contributed by Jinhu Wu",
        "commit_url": "https://github.com/apache/hadoop/commit/ce04340ec73617daff74378056a95c5d0cc0a790",
        "buggy_code": "public static final int MAX_ERROR_RETRIES_DEFAULT = 20;",
        "fixed_code": "public static final int MAX_ERROR_RETRIES_DEFAULT = 10;",
        "patch": "@@ -66,7 +66,7 @@ private Constants() {\n \n   // Number of times we should retry errors\n   public static final String MAX_ERROR_RETRIES_KEY = \"fs.oss.attempts.maximum\";\n-  public static final int MAX_ERROR_RETRIES_DEFAULT = 20;\n+  public static final int MAX_ERROR_RETRIES_DEFAULT = 10;\n \n   // Time until we give up trying to establish a connection to oss\n   public static final String ESTABLISH_TIMEOUT_KEY ="
    },
    {
        "commit_id": "b7b8cd53242da8d47ba4a6d99d906bdb2a1a3494",
        "commit_message": "YARN-7538. Fix performance regression introduced by Capacity Scheduler absolute min/max resource refactoring. (Sunil G via wangda)\n\nChange-Id: Ic9bd7e599c56970fe01cb0e1bba6df7d1f77eb29",
        "commit_url": "https://github.com/apache/hadoop/commit/b7b8cd53242da8d47ba4a6d99d906bdb2a1a3494",
        "buggy_code": "assertEquals(\"P2 Used Resource should be 7 GB\", 7 * GB,",
        "fixed_code": "assertEquals(\"P2 Used Resource should be 8 GB\", 8 * GB,",
        "patch": "@@ -4307,7 +4307,7 @@ public void testCSReservationWithRootUnblocked() throws Exception {\n           null, null, NULL_UPDATE_REQUESTS);\n       CapacityScheduler.schedule(cs);\n     }\n-    assertEquals(\"P2 Used Resource should be 7 GB\", 7 * GB,\n+    assertEquals(\"P2 Used Resource should be 8 GB\", 8 * GB,\n         cs.getQueue(\"p2\").getUsedResources().getMemorySize());\n \n     //Free a container from X1"
    },
    {
        "commit_id": "ce05c6e9811bca0bdc01152c2a82508a639480f5",
        "commit_message": "YARN-6545. Followup fix for YARN-6405. Contributed by Jian He",
        "commit_url": "https://github.com/apache/hadoop/commit/ce05c6e9811bca0bdc01152c2a82508a639480f5",
        "buggy_code": "HADOOP_XML(\"hadoop-xml\"),",
        "fixed_code": "HADOOP_XML(\"hadoop_xml\"),",
        "patch": "@@ -25,7 +25,7 @@ public enum ConfigFormat {\n   JSON(\"json\"),\n   PROPERTIES(\"properties\"),\n   XML(\"xml\"),\n-  HADOOP_XML(\"hadoop-xml\"),\n+  HADOOP_XML(\"hadoop_xml\"),\n   ENV(\"env\"),\n   TEMPLATE(\"template\"),\n   YAML(\"yaml\"),"
    },
    {
        "commit_id": "ce05c6e9811bca0bdc01152c2a82508a639480f5",
        "commit_message": "YARN-6545. Followup fix for YARN-6405. Contributed by Jian He",
        "commit_url": "https://github.com/apache/hadoop/commit/ce05c6e9811bca0bdc01152c2a82508a639480f5",
        "buggy_code": "public static final List<String> ZK_USERS_PATH_LIST = new ArrayList<String>();",
        "fixed_code": "private static final List<String> ZK_USERS_PATH_LIST = new ArrayList<String>();",
        "patch": "@@ -52,7 +52,7 @@ public class ZKIntegration implements Watcher, Closeable {\n   public static final String SVC_SLIDER = \"/\" + ZK_SERVICES + \"/\" + ZK_SLIDER;\n   public static final String SVC_SLIDER_USERS = SVC_SLIDER + \"/\" + ZK_USERS;\n \n-  public static final List<String> ZK_USERS_PATH_LIST = new ArrayList<String>();\n+  private static final List<String> ZK_USERS_PATH_LIST = new ArrayList<String>();\n   static {\n     ZK_USERS_PATH_LIST.add(ZK_SERVICES);\n     ZK_USERS_PATH_LIST.add(ZK_SLIDER);"
    },
    {
        "commit_id": "74fff4086e8cb540d991b9362ff4c2404348bbdf",
        "commit_message": "YARN-6014. Followup fix for slider core module findbugs. Contributed by Jian He",
        "commit_url": "https://github.com/apache/hadoop/commit/74fff4086e8cb540d991b9362ff4c2404348bbdf",
        "buggy_code": "String verb;",
        "fixed_code": "public String verb;",
        "patch": "@@ -24,6 +24,6 @@\n @JsonIgnoreProperties(ignoreUnknown = true)\n @JsonSerialize(include = JsonSerialize.Inclusion.NON_NULL)\n public class StopResponse {\n-  String verb;\n+  public String verb;\n   public String text;\n }"
    },
    {
        "commit_id": "db96e8aa2173c6aab607ea838ab906280ee74b1b",
        "commit_message": "YARN-5967. Fix slider core module findbugs warnings. Contributed by Jian He",
        "commit_url": "https://github.com/apache/hadoop/commit/db96e8aa2173c6aab607ea838ab906280ee74b1b",
        "buggy_code": "import static org.apache.slider.api.proto.RestTypeMarshalling.*;",
        "fixed_code": "import static org.apache.slider.api.types.RestTypeMarshalling.*;",
        "patch": "@@ -51,7 +51,7 @@\n import java.util.List;\n import java.util.Map;\n \n-import static org.apache.slider.api.proto.RestTypeMarshalling.*;\n+import static org.apache.slider.api.types.RestTypeMarshalling.*;\n \n /**\n  * Cluster operations at a slightly higher level than the RPC code"
    },
    {
        "commit_id": "db96e8aa2173c6aab607ea838ab906280ee74b1b",
        "commit_message": "YARN-5967. Fix slider core module findbugs warnings. Contributed by Jian He",
        "commit_url": "https://github.com/apache/hadoop/commit/db96e8aa2173c6aab607ea838ab906280ee74b1b",
        "buggy_code": "String s = new String(data, 0, data.length);",
        "fixed_code": "String s = new String(data, 0, data.length, \"UTF-8\");",
        "patch": "@@ -194,7 +194,7 @@ public Document parseConfiguration(FileSystem fs,\n     byte[] data = loadBytes(fs, path);\n     //this is here to track down a parse issue\n     //related to configurations\n-    String s = new String(data, 0, data.length);\n+    String s = new String(data, 0, data.length, \"UTF-8\");\n     log.debug(\"XML resource {} is \\\"{}\\\"\", path, s);\n /* JDK7\n     try (ByteArrayInputStream in = new ByteArrayInputStream(data)) {"
    },
    {
        "commit_id": "db96e8aa2173c6aab607ea838ab906280ee74b1b",
        "commit_message": "YARN-5967. Fix slider core module findbugs warnings. Contributed by Jian He",
        "commit_url": "https://github.com/apache/hadoop/commit/db96e8aa2173c6aab607ea838ab906280ee74b1b",
        "buggy_code": "Integer.valueOf(SliderKeys.PASS_LEN));",
        "fixed_code": "Integer.parseInt(SliderKeys.PASS_LEN));",
        "patch": "@@ -163,7 +163,7 @@ public void resolve() throws BadConfigException {\n   public String getPassphrase() {\n     if (passphrase == null) {\n       passphrase = RandomStringUtils.randomAlphanumeric(\n-          Integer.valueOf(SliderKeys.PASS_LEN));\n+          Integer.parseInt(SliderKeys.PASS_LEN));\n     }\n \n     return passphrase;"
    },
    {
        "commit_id": "db96e8aa2173c6aab607ea838ab906280ee74b1b",
        "commit_message": "YARN-5967. Fix slider core module findbugs warnings. Contributed by Jian He",
        "commit_url": "https://github.com/apache/hadoop/commit/db96e8aa2173c6aab607ea838ab906280ee74b1b",
        "buggy_code": "import static org.apache.slider.api.proto.RestTypeMarshalling.marshall;",
        "fixed_code": "import static org.apache.slider.api.types.RestTypeMarshalling.marshall;",
        "patch": "@@ -59,7 +59,7 @@\n import java.util.Map;\n import java.util.concurrent.TimeUnit;\n \n-import static org.apache.slider.api.proto.RestTypeMarshalling.marshall;\n+import static org.apache.slider.api.types.RestTypeMarshalling.marshall;\n import static org.apache.slider.server.appmaster.web.rest.RestPaths.*;\n \n /**"
    },
    {
        "commit_id": "db96e8aa2173c6aab607ea838ab906280ee74b1b",
        "commit_message": "YARN-5967. Fix slider core module findbugs warnings. Contributed by Jian He",
        "commit_url": "https://github.com/apache/hadoop/commit/db96e8aa2173c6aab607ea838ab906280ee74b1b",
        "buggy_code": "sb.append(String.format(\"\\n  [%02d]  \", entry.rolePriority));",
        "fixed_code": "sb.append(String.format(\"%n  [%02d]  \", entry.rolePriority));",
        "patch": "@@ -253,7 +253,7 @@ public String toFullString() {\n       new StringBuilder(toString());\n     sb.append(\"{ \");\n     for (NodeEntry entry : nodeEntries) {\n-      sb.append(String.format(\"\\n  [%02d]  \", entry.rolePriority));\n+      sb.append(String.format(\"%n  [%02d]  \", entry.rolePriority));\n         sb.append(entry.toString());\n     }\n     sb.append(\"} \");"
    },
    {
        "commit_id": "db96e8aa2173c6aab607ea838ab906280ee74b1b",
        "commit_message": "YARN-5967. Fix slider core module findbugs warnings. Contributed by Jian He",
        "commit_url": "https://github.com/apache/hadoop/commit/db96e8aa2173c6aab607ea838ab906280ee74b1b",
        "buggy_code": "static class newerThan implements Comparator<Container>, Serializable {",
        "fixed_code": "static class newerThan implements Comparator<Container> {",
        "patch": "@@ -232,7 +232,7 @@ private OutstandingRequest removeOpenRequest(Container container) {\n    * the most recent one is picked first. This operation <i>does not</i>\n    * change the role history, though it queries it.\n    */\n-  static class newerThan implements Comparator<Container>, Serializable {\n+  static class newerThan implements Comparator<Container> {\n     private RoleHistory rh;\n     \n     public newerThan(RoleHistory rh) {"
    },
    {
        "commit_id": "db96e8aa2173c6aab607ea838ab906280ee74b1b",
        "commit_message": "YARN-5967. Fix slider core module findbugs warnings. Contributed by Jian He",
        "commit_url": "https://github.com/apache/hadoop/commit/db96e8aa2173c6aab607ea838ab906280ee74b1b",
        "buggy_code": "SliderUtils.write(testWriteFile, \"test\".getBytes(\"UTF-8\"), true);",
        "fixed_code": "SliderUtils.write(testWriteFile, \"test\".getBytes(\"UTF-8\"));",
        "patch": "@@ -153,7 +153,7 @@ public void testIsHdp() {\n   @Test\n   public void testWrite() throws IOException {\n     File testWriteFile = folder.newFile(\"testWrite\");\n-    SliderUtils.write(testWriteFile, \"test\".getBytes(\"UTF-8\"), true);\n+    SliderUtils.write(testWriteFile, \"test\".getBytes(\"UTF-8\"));\n     Assert.assertTrue(FileUtils.readFileToString(testWriteFile, \"UTF-8\").equals(\"test\"));\n   }\n }"
    },
    {
        "commit_id": "3741e5518f0750c3567323edda6df8c3102a0293",
        "commit_message": "YARN-5796. Convert enums values in service code to upper case and special handling of an error. Contributed by Gour Saha",
        "commit_url": "https://github.com/apache/hadoop/commit/3741e5518f0750c3567323edda6df8c3102a0293",
        "buggy_code": "DOCKER(\"docker\"), TARBALL(\"tarball\"), APPLICATION(\"application\");",
        "fixed_code": "DOCKER(\"DOCKER\"), TARBALL(\"TARBALL\"), APPLICATION(\"APPLICATION\");",
        "patch": "@@ -40,7 +40,7 @@ public class Artifact implements Serializable {\n   private String id = null;\n \n   public enum TypeEnum {\n-    DOCKER(\"docker\"), TARBALL(\"tarball\"), APPLICATION(\"application\");\n+    DOCKER(\"DOCKER\"), TARBALL(\"TARBALL\"), APPLICATION(\"APPLICATION\");\n \n     private String value;\n "
    },
    {
        "commit_id": "3741e5518f0750c3567323edda6df8c3102a0293",
        "commit_message": "YARN-5796. Convert enums values in service code to upper case and special handling of an error. Contributed by Gour Saha",
        "commit_url": "https://github.com/apache/hadoop/commit/3741e5518f0750c3567323edda6df8c3102a0293",
        "buggy_code": "HTTP(\"http\");",
        "fixed_code": "HTTP(\"HTTP\");",
        "patch": "@@ -39,7 +39,7 @@ public class ReadinessCheck implements Serializable {\n   private static final long serialVersionUID = -3836839816887186801L;\n \n   public enum TypeEnum {\n-    HTTP(\"http\");\n+    HTTP(\"HTTP\");\n \n     private String value;\n "
    },
    {
        "commit_id": "7da243ebe05be27ade86feac83a6243eebba619c",
        "commit_message": "YARN-5729. Bug fixes for the service Rest API. Contributed by Gour Saha",
        "commit_url": "https://github.com/apache/hadoop/commit/7da243ebe05be27ade86feac83a6243eebba619c",
        "buggy_code": "public class Resource extends BaseResource {",
        "fixed_code": "public class Resource extends BaseResource implements Cloneable {",
        "patch": "@@ -35,7 +35,7 @@\n \n @ApiModel(description = \"Resource determines the amount of resources (vcores, memory, network, etc.) usable by a container. This field determines the resource to be applied for all the containers of a component or application. The resource specified at the app (or global) level can be overriden at the component level. Only one of profile OR cpu & memory are exepected. It raises a validation exception otherwise.\")\n @javax.annotation.Generated(value = \"class io.swagger.codegen.languages.JavaClientCodegen\", date = \"2016-06-02T08:15:05.615-07:00\")\n-public class Resource extends BaseResource {\n+public class Resource extends BaseResource implements Cloneable {\n   private static final long serialVersionUID = -6431667797380250037L;\n \n   private String profile = null;"
    },
    {
        "commit_id": "4d8abd84f40a6124e7b05b34c14b4035309623ef",
        "commit_message": "YARN-7024: Fix issues on recovery in LevelDB store. Contributed by Jonathan Hung",
        "commit_url": "https://github.com/apache/hadoop/commit/4d8abd84f40a6124e7b05b34c14b4035309623ef",
        "buggy_code": "return pendingMutations;",
        "fixed_code": "return new LinkedList<>(pendingMutations);",
        "patch": "@@ -79,7 +79,7 @@ public synchronized Configuration retrieve() {\n \n   @Override\n   public synchronized List<LogMutation> getPendingMutations() {\n-    return pendingMutations;\n+    return new LinkedList<>(pendingMutations);\n   }\n \n   @Override"
    },
    {
        "commit_id": "592bf2d550a07ea5c5df3ba0ab2952c34d941b4b",
        "commit_message": "YARN-7279. Fix typo in helper message of ContainerLauncher. Contributed by Elek, Marton.",
        "commit_url": "https://github.com/apache/hadoop/commit/592bf2d550a07ea5c5df3ba0ab2952c34d941b4b",
        "buggy_code": ".append(\"  <name>mapreduce.reduce.e nv</name>\\n\")",
        "fixed_code": ".append(\"  <name>mapreduce.reduce.env</name>\\n\")",
        "patch": "@@ -642,7 +642,7 @@ private String analysesErrorMsgOfContainerExitWithFailure(String errorMsg) {\n           .append(\"  <value>HADOOP_MAPRED_HOME=${full path of your hadoop \"\n               + \"distribution directory}</value>\\n\")\n           .append(\"</property>\\n<property>\\n\")\n-          .append(\"  <name>mapreduce.reduce.e nv</name>\\n\")\n+          .append(\"  <name>mapreduce.reduce.env</name>\\n\")\n           .append(\"  <value>HADOOP_MAPRED_HOME=${full path of your hadoop \"\n               + \"distribution directory}</value>\\n\")\n           .append(\"</property>\\n\");"
    },
    {
        "commit_id": "e490602e9b306d5b8a543b93fb15a7395bb9a03d",
        "commit_message": "YARN-7039. Fix javac and javadoc errors in YARN-3926 branch. (Sunil G via wangda)\n\nChange-Id: I442bf6d838b3aba83f1f6779cf9dcf8596a2102d",
        "commit_url": "https://github.com/apache/hadoop/commit/e490602e9b306d5b8a543b93fb15a7395bb9a03d",
        "buggy_code": "int mem = NodeManagerHardwareUtils.getContainerMemoryMB(null, conf);",
        "fixed_code": "long mem = NodeManagerHardwareUtils.getContainerMemoryMB(null, conf);",
        "patch": "@@ -172,7 +172,7 @@ public void testGetContainerMemoryMB() throws Exception {\n     YarnConfiguration conf = new YarnConfiguration();\n     conf.setBoolean(YarnConfiguration.NM_ENABLE_HARDWARE_CAPABILITY_DETECTION,\n         true);\n-    int mem = NodeManagerHardwareUtils.getContainerMemoryMB(null, conf);\n+    long mem = NodeManagerHardwareUtils.getContainerMemoryMB(null, conf);\n     Assert.assertEquals(YarnConfiguration.DEFAULT_NM_PMEM_MB, mem);\n \n     mem = NodeManagerHardwareUtils.getContainerMemoryMB(plugin, conf);"
    },
    {
        "commit_id": "661f5eb0c6791148f2d15d5730635ccb668601e3",
        "commit_message": "YARN-7128. The error message in TimelineSchemaCreator is not enough to find out the error. (Jinjiang Ling via Haibo Chen)",
        "commit_url": "https://github.com/apache/hadoop/commit/661f5eb0c6791148f2d15d5730635ccb668601e3",
        "buggy_code": "LOG.error(\"Error in creating hbase tables: \" + e.getMessage());",
        "fixed_code": "LOG.error(\"Error in creating hbase tables: \", e);",
        "patch": "@@ -276,7 +276,7 @@ private static void createAllSchemas(Configuration hbaseConf,\n       createAllTables(hbaseConf, skipExisting);\n       LOG.info(\"Successfully created HBase schema. \");\n     } catch (IOException e) {\n-      LOG.error(\"Error in creating hbase tables: \" + e.getMessage());\n+      LOG.error(\"Error in creating hbase tables: \", e);\n       exceptions.add(e);\n     }\n "
    },
    {
        "commit_id": "5dba54596a1587e0ba5f9f02f40483e597b0df64",
        "commit_message": "HDFS-12388. A bad error message in DFSStripedOutputStream. Contributed by Huafeng Wang",
        "commit_url": "https://github.com/apache/hadoop/commit/5dba54596a1587e0ba5f9f02f40483e597b0df64",
        "buggy_code": "+ failCount + \" > the number of data blocks = \"",
        "fixed_code": "+ failCount + \" > the number of parity blocks = \"",
        "patch": "@@ -390,7 +390,7 @@ private Set<StripedDataStreamer> checkStreamers() throws IOException {\n     }\n     if (failCount > (numAllBlocks - numDataBlocks)) {\n       throw new IOException(\"Failed: the number of failed blocks = \"\n-          + failCount + \" > the number of data blocks = \"\n+          + failCount + \" > the number of parity blocks = \"\n           + (numAllBlocks - numDataBlocks));\n     }\n     return newFailed;"
    },
    {
        "commit_id": "7a82d7bcea8124e1b65c275fac15bf2047d17471",
        "commit_message": "YARN-6979. [Addendum patch] Fixed classname and added javadocs. (Kartheek Muthyala via asuresh)",
        "commit_url": "https://github.com/apache/hadoop/commit/7a82d7bcea8124e1b65c275fac15bf2047d17471",
        "buggy_code": "DECREASE_CONTAINERS_RESOURCE,",
        "fixed_code": "UPDATE_CONTAINERS,",
        "patch": "@@ -21,6 +21,6 @@\n public enum ContainerManagerEventType {\n   FINISH_APPS,\n   FINISH_CONTAINERS,\n-  DECREASE_CONTAINERS_RESOURCE,\n+  UPDATE_CONTAINERS,\n   SIGNAL_CONTAINERS\n }"
    },
    {
        "commit_id": "1a18d5e514d13aa3a88e9b6089394a27296d6bc3",
        "commit_message": "YARN-6515. Fix warnings from Spotbugs in hadoop-yarn-server-nodemanager. Contributed by Naganarasimha G R.",
        "commit_url": "https://github.com/apache/hadoop/commit/1a18d5e514d13aa3a88e9b6089394a27296d6bc3",
        "buggy_code": "protected final static Map<ContainerId, ContainerMetrics>",
        "fixed_code": "private final static Map<ContainerId, ContainerMetrics>",
        "patch": "@@ -130,7 +130,7 @@ public class ContainerMetrics implements MetricsSource {\n   /**\n    * Simple metrics cache to help prevent re-registrations.\n    */\n-  protected final static Map<ContainerId, ContainerMetrics>\n+  private final static Map<ContainerId, ContainerMetrics>\n       usageMetrics = new HashMap<>();\n   // Create a timer to unregister container metrics,\n   // whose associated thread run as a daemon."
    },
    {
        "commit_id": "46b7054fa7eae9129c21c9f3dc70acff46bfdc41",
        "commit_message": "YARN-6951. Fix debug log when Resource Handler chain is enabled. Contributed by Yang Wang.",
        "commit_url": "https://github.com/apache/hadoop/commit/46b7054fa7eae9129c21c9f3dc70acff46bfdc41",
        "buggy_code": "== null));",
        "fixed_code": "!= null));",
        "patch": "@@ -307,7 +307,7 @@ public void init() throws IOException {\n           .getConfiguredResourceHandlerChain(conf);\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Resource handler chain enabled = \" + (resourceHandlerChain\n-            == null));\n+            != null));\n       }\n       if (resourceHandlerChain != null) {\n         LOG.debug(\"Bootstrapping resource handler chain\");"
    },
    {
        "commit_id": "f9139ac8f60184a82a8bb315237bea04bdb98ec8",
        "commit_message": "YARN-6872. [Addendum patch] Ensure apps could run given NodeLabels are disabled post RM switchover/restart. Contributed by Sunil G",
        "commit_url": "https://github.com/apache/hadoop/commit/f9139ac8f60184a82a8bb315237bea04bdb98ec8",
        "buggy_code": "appSchedulingInfo.recoverContainer(rmContainer);",
        "fixed_code": "appSchedulingInfo.recoverContainer(rmContainer, node.getPartition());",
        "patch": "@@ -1103,7 +1103,7 @@ public void recoverContainer(SchedulerNode node,\n     try {\n       writeLock.lock();\n       // recover app scheduling info\n-      appSchedulingInfo.recoverContainer(rmContainer);\n+      appSchedulingInfo.recoverContainer(rmContainer, node.getPartition());\n \n       if (rmContainer.getState().equals(RMContainerState.COMPLETED)) {\n         return;"
    },
    {
        "commit_id": "6436768baf1b2ac05f6786edcd76fd3a66c03eaa",
        "commit_message": "HDFS-12089. Fix ambiguous NN retry log message in WebHDFS. Contributed by Eric Badger",
        "commit_url": "https://github.com/apache/hadoop/commit/6436768baf1b2ac05f6786edcd76fd3a66c03eaa",
        "buggy_code": "LOG.info(\"Retrying connect to namenode: {}. Already tried {}\"",
        "fixed_code": "LOG.info(\"Retrying connect to namenode: {}. Already retried {}\"",
        "patch": "@@ -792,7 +792,7 @@ private void shouldRetry(final IOException ioe, final int retry\n               a.action == RetryPolicy.RetryAction.RetryDecision.FAILOVER_AND_RETRY;\n \n           if (isRetry || isFailoverAndRetry) {\n-            LOG.info(\"Retrying connect to namenode: {}. Already tried {}\"\n+            LOG.info(\"Retrying connect to namenode: {}. Already retried {}\"\n                     + \" time(s); retry policy is {}, delay {}ms.\",\n                 nnAddr, retry, retryPolicy, a.delayMillis);\n "
    },
    {
        "commit_id": "092ebdf885468a2bf79cbfb168286b7cddc4a0db",
        "commit_message": "HADOOP-12940. Fix warnings from Spotbugs in hadoop-common.",
        "commit_url": "https://github.com/apache/hadoop/commit/092ebdf885468a2bf79cbfb168286b7cddc4a0db",
        "buggy_code": "lazyPersist ? 1 : getDefaultReplication(item.path),",
        "fixed_code": "(short) 1,",
        "patch": "@@ -501,7 +501,7 @@ FSDataOutputStream create(PathData item, boolean lazyPersist,\n                         createFlags,\n                         getConf().getInt(IO_FILE_BUFFER_SIZE_KEY,\n                             IO_FILE_BUFFER_SIZE_DEFAULT),\n-                        lazyPersist ? 1 : getDefaultReplication(item.path),\n+                        (short) 1,\n                         getDefaultBlockSize(),\n                         null,\n                         null);"
    },
    {
        "commit_id": "092ebdf885468a2bf79cbfb168286b7cddc4a0db",
        "commit_message": "HADOOP-12940. Fix warnings from Spotbugs in hadoop-common.",
        "commit_url": "https://github.com/apache/hadoop/commit/092ebdf885468a2bf79cbfb168286b7cddc4a0db",
        "buggy_code": "return (major << 16 + minor);",
        "fixed_code": "return (major << 16) + minor;",
        "patch": "@@ -395,7 +395,7 @@ public boolean equals(Object other) {\n \n     @Override\n     public int hashCode() {\n-      return (major << 16 + minor);\n+      return (major << 16) + minor;\n     }\n   }\n "
    },
    {
        "commit_id": "c4b5c32669423b9a792f33f9f8333d95528f2515",
        "commit_message": "HADOOP-13854. KMS should log error details in KMSExceptionsProvider.",
        "commit_url": "https://github.com/apache/hadoop/commit/c4b5c32669423b9a792f33f9f8333d95528f2515",
        "buggy_code": "private static final Logger LOG = LoggerFactory.getLogger(KMS.class);",
        "fixed_code": "static final Logger LOG = LoggerFactory.getLogger(KMS.class);",
        "patch": "@@ -70,7 +70,7 @@ public enum KMSOp {\n   private KeyProviderCryptoExtension provider;\n   private KMSAudit kmsAudit;\n \n-  private static final Logger LOG = LoggerFactory.getLogger(KMS.class);\n+  static final Logger LOG = LoggerFactory.getLogger(KMS.class);\n \n   public KMS() throws Exception {\n     provider = KMSWebApp.getKeyProvider();"
    },
    {
        "commit_id": "0dcf843c008f2b9cece8c0a0cef78140398ac464",
        "commit_message": "HDFS-11893. Fix TestDFSShell.testMoveWithTargetPortEmpty failure. Contributed by Brahma Reddy Battula.",
        "commit_url": "https://github.com/apache/hadoop/commit/0dcf843c008f2b9cece8c0a0cef78140398ac464",
        "buggy_code": "argv[2] = \"hdfs://localhost/testfile2\";",
        "fixed_code": "argv[2] = \"hdfs://\" + srcFs.getUri().getHost() + \"/testfile2\";",
        "patch": "@@ -789,7 +789,7 @@ public void testMoveWithTargetPortEmpty() throws Exception {\n       argv = new String[3];\n       argv[0] = \"-mv\";\n       argv[1] = srcFs.getUri() + \"/testfile\";\n-      argv[2] = \"hdfs://localhost/testfile2\";\n+      argv[2] = \"hdfs://\" + srcFs.getUri().getHost() + \"/testfile2\";\n       int ret = ToolRunner.run(shell, argv);\n       assertEquals(\"mv should have succeeded\", 0, ret);\n     } finally {"
    },
    {
        "commit_id": "2ba9903932e3c99afb0e6abb7cd3c5d29a554bd9",
        "commit_message": "HADOOP-14400. Fix warnings from spotbugs in hadoop-tools. Contributed by Weiwei Yang.",
        "commit_url": "https://github.com/apache/hadoop/commit/2ba9903932e3c99afb0e6abb7cd3c5d29a554bd9",
        "buggy_code": "return heapSpace.size();",
        "fixed_code": "return getHeapSpaceSize();",
        "patch": "@@ -58,7 +58,7 @@ int getNumCalls() {\n     \n     // Get the total number of 1mb objects stored within\n     long getHeapUsageInMB() {\n-      return heapSpace.size();\n+      return getHeapSpaceSize();\n     }\n     \n     @Override"
    },
    {
        "commit_id": "b062b323b7f48343c379520f5380e1a63dbcc7a4",
        "commit_message": "HADOOP-14281. Fix TestKafkaMetrics#testPutMetrics. Contributed by Alison Yu.",
        "commit_url": "https://github.com/apache/hadoop/commit/b062b323b7f48343c379520f5380e1a63dbcc7a4",
        "buggy_code": "SimpleDateFormat timeFormat = new SimpleDateFormat(\"hh:mm:ss\");",
        "fixed_code": "SimpleDateFormat timeFormat = new SimpleDateFormat(\"HH:mm:ss\");",
        "patch": "@@ -155,7 +155,7 @@ StringBuilder recordToJson(MetricsRecord record) {\n     Date currDate = new Date(timestamp);\n     SimpleDateFormat dateFormat = new SimpleDateFormat(\"yyyy-MM-dd\");\n     String date = dateFormat.format(currDate);\n-    SimpleDateFormat timeFormat = new SimpleDateFormat(\"hh:mm:ss\");\n+    SimpleDateFormat timeFormat = new SimpleDateFormat(\"HH:mm:ss\");\n     String time = timeFormat.format(currDate);\n     String hostname = new String(\"null\");\n     try {"
    },
    {
        "commit_id": "30fc5801966feb7f9bdd7d79db75acc595102913",
        "commit_message": "YARN-6519. Fix warnings from Spotbugs in hadoop-yarn-server-resourcemanager. Contributed by Weiwei Yang.",
        "commit_url": "https://github.com/apache/hadoop/commit/30fc5801966feb7f9bdd7d79db75acc595102913",
        "buggy_code": "protected final static List<Container> EMPTY_CONTAINER_LIST =",
        "fixed_code": "private final static List<Container> EMPTY_CONTAINER_LIST =",
        "patch": "@@ -393,7 +393,7 @@ public boolean hasApplicationMasterRegistered(\n     return hasApplicationMasterRegistered;\n   }\n \n-  protected final static List<Container> EMPTY_CONTAINER_LIST =\n+  private final static List<Container> EMPTY_CONTAINER_LIST =\n       new ArrayList<Container>();\n   protected static final Allocation EMPTY_ALLOCATION = new Allocation(\n       EMPTY_CONTAINER_LIST, Resources.createResource(0), null, null, null);"
    },
    {
        "commit_id": "30fc5801966feb7f9bdd7d79db75acc595102913",
        "commit_message": "YARN-6519. Fix warnings from Spotbugs in hadoop-yarn-server-resourcemanager. Contributed by Weiwei Yang.",
        "commit_url": "https://github.com/apache/hadoop/commit/30fc5801966feb7f9bdd7d79db75acc595102913",
        "buggy_code": "localityStatistics[containerType.index][requestType.index]++;",
        "fixed_code": "localityStatistics[containerType.getIndex()][requestType.getIndex()]++;",
        "patch": "@@ -152,7 +152,7 @@ public void updateAggregatePreemptedAppResourceUsage(\n \n   public void incNumAllocatedContainers(NodeType containerType,\n       NodeType requestType) {\n-    localityStatistics[containerType.index][requestType.index]++;\n+    localityStatistics[containerType.getIndex()][requestType.getIndex()]++;\n     totalAllocatedContainers++;\n   }\n "
    },
    {
        "commit_id": "30fc5801966feb7f9bdd7d79db75acc595102913",
        "commit_message": "YARN-6519. Fix warnings from Spotbugs in hadoop-yarn-server-resourcemanager. Contributed by Weiwei Yang.",
        "commit_url": "https://github.com/apache/hadoop/commit/30fc5801966feb7f9bdd7d79db75acc595102913",
        "buggy_code": "protected final static List<Container> EMPTY_CONTAINER_LIST =",
        "fixed_code": "private final static List<Container> EMPTY_CONTAINER_LIST =",
        "patch": "@@ -132,7 +132,7 @@ public abstract class AbstractYarnScheduler\n   protected int nmExpireInterval;\n   protected long nmHeartbeatInterval;\n \n-  protected final static List<Container> EMPTY_CONTAINER_LIST =\n+  private final static List<Container> EMPTY_CONTAINER_LIST =\n       new ArrayList<Container>();\n   protected static final Allocation EMPTY_ALLOCATION = new Allocation(\n     EMPTY_CONTAINER_LIST, Resources.createResource(0), null, null, null);"
    },
    {
        "commit_id": "3ed3062fe3979ff55a411b730a8eee2b2c96d6b3",
        "commit_message": "MAPREDUCE-6881. Fix warnings from Spotbugs in hadoop-mapreduce. Contributed by Weiwei Yang.",
        "commit_url": "https://github.com/apache/hadoop/commit/3ed3062fe3979ff55a411b730a8eee2b2c96d6b3",
        "buggy_code": "return Long.valueOf(this.jvmId).compareTo(that.jvmId);",
        "fixed_code": "return Long.compare(this.jvmId, that.jvmId);",
        "patch": "@@ -98,7 +98,7 @@ public int compareTo(JVMId that) {\n     int jobComp = this.jobId.compareTo(that.jobId);\n     if(jobComp == 0) {\n       if(this.isMap == that.isMap) {\n-        return Long.valueOf(this.jvmId).compareTo(that.jvmId);\n+        return Long.compare(this.jvmId, that.jvmId);\n       } else {\n         return this.isMap ? -1 : 1;\n       }"
    },
    {
        "commit_id": "4f3ca0396a810f54f7fd0489a224c1bb13143aa4",
        "commit_message": "YARN-6510. Fix profs stat file warning caused by process names that includes parenthesis. (Wilfred Spiegelenburg via Haibo Chen)",
        "commit_url": "https://github.com/apache/hadoop/commit/4f3ca0396a810f54f7fd0489a224c1bb13143aa4",
        "buggy_code": "\"^([\\\\d-]+)\\\\s\\\\(([^)]+)\\\\)\\\\s[^\\\\s]\\\\s([\\\\d-]+)\\\\s([\\\\d-]+)\\\\s\" +",
        "fixed_code": "\"^([\\\\d-]+)\\\\s\\\\((.*)\\\\)\\\\s[^\\\\s]\\\\s([\\\\d-]+)\\\\s([\\\\d-]+)\\\\s\" +",
        "patch": "@@ -58,7 +58,7 @@ public class ProcfsBasedProcessTree extends ResourceCalculatorProcessTree {\n   private static final String PROCFS = \"/proc/\";\n \n   private static final Pattern PROCFS_STAT_FILE_FORMAT = Pattern.compile(\n-      \"^([\\\\d-]+)\\\\s\\\\(([^)]+)\\\\)\\\\s[^\\\\s]\\\\s([\\\\d-]+)\\\\s([\\\\d-]+)\\\\s\" +\n+      \"^([\\\\d-]+)\\\\s\\\\((.*)\\\\)\\\\s[^\\\\s]\\\\s([\\\\d-]+)\\\\s([\\\\d-]+)\\\\s\" +\n       \"([\\\\d-]+)\\\\s([\\\\d-]+\\\\s){7}(\\\\d+)\\\\s(\\\\d+)\\\\s([\\\\d-]+\\\\s){7}(\\\\d+)\\\\s\" +\n       \"(\\\\d+)(\\\\s[\\\\d-]+){15}\");\n "
    },
    {
        "commit_id": "2fd568fdd416a8ea3a3fa8efe237a40a42fb7e03",
        "commit_message": "HDFS-11637. Fix javac warning caused by the deprecated key used in TestDFSClientRetries#testFailuresArePerOperation. Contributed by Yiqun Lin.",
        "commit_url": "https://github.com/apache/hadoop/commit/2fd568fdd416a8ea3a3fa8efe237a40a42fb7e03",
        "buggy_code": "conf.setInt(DFSConfigKeys.DFS_CLIENT_RETRY_WINDOW_BASE, 10);",
        "fixed_code": "conf.setInt(HdfsClientConfigKeys.Retry.WINDOW_BASE_KEY, 10);",
        "patch": "@@ -370,7 +370,7 @@ public void testLeaseRenewSocketTimeout() throws Exception\n     String file1 = \"/testFile1\";\n     String file2 = \"/testFile2\";\n     // Set short retry timeouts so this test runs faster\n-    conf.setInt(DFSConfigKeys.DFS_CLIENT_RETRY_WINDOW_BASE, 10);\n+    conf.setInt(HdfsClientConfigKeys.Retry.WINDOW_BASE_KEY, 10);\n     conf.setInt(DFS_CLIENT_SOCKET_TIMEOUT_KEY, 2 * 1000);\n     MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).build();\n     try {"
    },
    {
        "commit_id": "46d37a65cf09c2714b4c0c4ec0399031d60027a5",
        "commit_message": "HDFS-10506. Addendum patch to include missing changes. Contributed by Akira Ajisaka.",
        "commit_url": "https://github.com/apache/hadoop/commit/46d37a65cf09c2714b4c0c4ec0399031d60027a5",
        "buggy_code": "Long genstamp = block.removeChildLong(INODE_SECTION_GEMSTAMP);",
        "fixed_code": "Long genstamp = block.removeChildLong(INODE_SECTION_GENSTAMP);",
        "patch": "@@ -669,7 +669,7 @@ private HdfsProtos.BlockProto.Builder createBlockBuilder(Node block)\n       throw new IOException(\"<block> found without <id>\");\n     }\n     blockBld.setBlockId(id);\n-    Long genstamp = block.removeChildLong(INODE_SECTION_GEMSTAMP);\n+    Long genstamp = block.removeChildLong(INODE_SECTION_GENSTAMP);\n     if (genstamp == null) {\n       throw new IOException(\"<block> found without <genstamp>\");\n     }"
    },
    {
        "commit_id": "6c399a88e9b5ef8f822a9bd469dbf9fdb3141e38",
        "commit_message": "HADOOP-14059. typo in s3a rename(self, subdir) error message. Contributed by Steve Loughran.",
        "commit_url": "https://github.com/apache/hadoop/commit/6c399a88e9b5ef8f822a9bd469dbf9fdb3141e38",
        "buggy_code": "\"cannot rename a directory to a subdirectory o fitself \");",
        "fixed_code": "\"cannot rename a directory to a subdirectory of itself \");",
        "patch": "@@ -813,7 +813,7 @@ private boolean innerRename(Path src, Path dst)\n       //Verify dest is not a child of the source directory\n       if (dstKey.startsWith(srcKey)) {\n         throw new RenameFailedException(srcKey, dstKey,\n-            \"cannot rename a directory to a subdirectory o fitself \");\n+            \"cannot rename a directory to a subdirectory of itself \");\n       }\n \n       List<DeleteObjectsRequest.KeyVersion> keysToDelete = new ArrayList<>();"
    },
    {
        "commit_id": "04a5f5a6dc88769cca8b1a15057a0756712b5013",
        "commit_message": "HADOOP-14156. Fix grammar error in ConfTest.java.\n\nThis closes #187\n\nSigned-off-by: Akira Ajisaka <aajisaka@apache.org>",
        "commit_url": "https://github.com/apache/hadoop/commit/04a5f5a6dc88769cca8b1a15057a0756712b5013",
        "buggy_code": "terminate(1, HADOOP_CONF_DIR + \" does not defined\");",
        "fixed_code": "terminate(1, HADOOP_CONF_DIR + \" is not defined\");",
        "patch": "@@ -269,7 +269,7 @@ public static void main(String[] args) throws IOException {\n     } else {\n       String confDirName = System.getenv(HADOOP_CONF_DIR);\n       if (confDirName == null) {\n-        terminate(1, HADOOP_CONF_DIR + \" does not defined\");\n+        terminate(1, HADOOP_CONF_DIR + \" is not defined\");\n       }\n       File confDir = new File(confDirName);\n       if (!confDir.isDirectory()) {"
    },
    {
        "commit_id": "d150f061f4ebde923fda28ea898a9606b8789758",
        "commit_message": "HDFS-11438. Fix typo in error message of StoragePolicyAdmin tool. Contributed by Alison Yu.",
        "commit_url": "https://github.com/apache/hadoop/commit/d150f061f4ebde923fda28ea898a9606b8789758",
        "buggy_code": "+ \"the storage policy will be unsetd.\\nUsage: \" + getLongUsage());",
        "fixed_code": "+ \"the storage policy will be unset.\\nUsage: \" + getLongUsage());",
        "patch": "@@ -259,7 +259,7 @@ public int run(Configuration conf, List<String> args) throws IOException {\n       final String path = StringUtils.popOptionWithArgument(\"-path\", args);\n       if (path == null) {\n         System.err.println(\"Please specify the path from which \"\n-            + \"the storage policy will be unsetd.\\nUsage: \" + getLongUsage());\n+            + \"the storage policy will be unset.\\nUsage: \" + getLongUsage());\n         return 1;\n       }\n "
    },
    {
        "commit_id": "0013090fb4340eadf147054e65a73de20a62c1c1",
        "commit_message": "HADOOP-14102. Relax error message assertion in S3A test ITestS3AEncryptionSSEC. Contributed by Mingliang Liu",
        "commit_url": "https://github.com/apache/hadoop/commit/0013090fb4340eadf147054e65a73de20a62c1c1",
        "buggy_code": "\"Forbidden (Service: Amazon S3; Status Code: 403;\", () -> {",
        "fixed_code": "\"Service: Amazon S3; Status Code: 403;\", () -> {",
        "patch": "@@ -58,7 +58,7 @@ public void testCreateFileAndReadWithDifferentEncryptionKey() throws\n     Exception {\n     final Path[] path = new Path[1];\n     intercept(java.nio.file.AccessDeniedException.class,\n-        \"Forbidden (Service: Amazon S3; Status Code: 403;\", () -> {\n+        \"Service: Amazon S3; Status Code: 403;\", () -> {\n \n         int len = 2048;\n         skipIfEncryptionTestsDisabled(getConfiguration());"
    },
    {
        "commit_id": "b9f8491252f5a23a91a1d695d748556a0fd803ae",
        "commit_message": "HADOOP-14058. Fix NativeS3FileSystemContractBaseTest#testDirWithDifferentMarkersWorks. Contributed by Yiqun Lin.",
        "commit_url": "https://github.com/apache/hadoop/commit/b9f8491252f5a23a91a1d695d748556a0fd803ae",
        "buggy_code": "for (int i = 0; i < 3; i++) {",
        "fixed_code": "for (int i = 0; i <= 3; i++) {",
        "patch": "@@ -85,7 +85,7 @@ private void createTestFiles(String base) throws IOException {\n \n   public void testDirWithDifferentMarkersWorks() throws Exception {\n \n-    for (int i = 0; i < 3; i++) {\n+    for (int i = 0; i <= 3; i++) {\n       String base = \"test/hadoop\" + i;\n       Path path = path(\"/\" + base);\n "
    },
    {
        "commit_id": "4d1f3d9020b8a8bf1d2a81e4d6ad20418ed9bcc2",
        "commit_message": "YARN-6016. Fix minor bugs in handling of local AMRMToken in AMRMProxy. (Botong Huang via Subru).",
        "commit_url": "https://github.com/apache/hadoop/commit/4d1f3d9020b8a8bf1d2a81e4d6ad20418ed9bcc2",
        "buggy_code": "keyId = this.amrmToken.decodeIdentifier().getKeyId();",
        "fixed_code": "keyId = this.localToken.decodeIdentifier().getKeyId();",
        "patch": "@@ -115,7 +115,7 @@ public synchronized int getLocalAMRMTokenKeyId() {\n           throw new YarnRuntimeException(\"Missing AMRM token for \"\n               + this.applicationAttemptId);\n         }\n-        keyId = this.amrmToken.decodeIdentifier().getKeyId();\n+        keyId = this.localToken.decodeIdentifier().getKeyId();\n         this.localTokenKeyId = keyId;\n       } catch (IOException e) {\n         throw new YarnRuntimeException(\"AMRM token decode error for \""
    },
    {
        "commit_id": "ed09c1418da07a54bb1c5875b31bac47088db56e",
        "commit_message": "HADOOP-13928. TestAdlFileContextMainOperationsLive.testGetFileContext1 runtime error. (John Zhuge via lei)",
        "commit_url": "https://github.com/apache/hadoop/commit/ed09c1418da07a54bb1c5875b31bac47088db56e",
        "buggy_code": "return DELEGATE_TO_FS_DEFAULT_PORT;",
        "fixed_code": "return getDefaultPortIfDefined(fsImpl);",
        "patch": "@@ -160,7 +160,7 @@ public Path getHomeDirectory() {\n \n   @Override\n   public int getUriDefaultPort() {\n-    return DELEGATE_TO_FS_DEFAULT_PORT;\n+    return getDefaultPortIfDefined(fsImpl);\n   }\n \n   @Override"
    },
    {
        "commit_id": "4db119b7b55213929e5b86f2abb0ed84a21719b5",
        "commit_message": "YARN-6079. Fix simple spelling errors in yarn test code. Contributed by vijay.",
        "commit_url": "https://github.com/apache/hadoop/commit/4db119b7b55213929e5b86f2abb0ed84a21719b5",
        "buggy_code": "assertEquals(\"Expected no macthing requests.\", matches.size(), 0);",
        "fixed_code": "assertEquals(\"Expected no matching requests.\", matches.size(), 0);",
        "patch": "@@ -247,7 +247,7 @@ public void testAMRMClientNoMatchingRequests()\n     Resource testCapability1 = Resource.newInstance(1024,  2);\n     List<? extends Collection<ContainerRequest>> matches =\n         amClient.getMatchingRequests(priority, node, testCapability1);\n-    assertEquals(\"Expected no macthing requests.\", matches.size(), 0);\n+    assertEquals(\"Expected no matching requests.\", matches.size(), 0);\n   }\n   \n   @Test (timeout=60000)"
    },
    {
        "commit_id": "4db119b7b55213929e5b86f2abb0ed84a21719b5",
        "commit_message": "YARN-6079. Fix simple spelling errors in yarn test code. Contributed by vijay.",
        "commit_url": "https://github.com/apache/hadoop/commit/4db119b7b55213929e5b86f2abb0ed84a21719b5",
        "buggy_code": "LOG.info(String.format(\"Exclude protential property: %s\\n\", gsp.propertyName));",
        "fixed_code": "LOG.info(String.format(\"Exclude potential property: %s\\n\", gsp.propertyName));",
        "patch": "@@ -222,7 +222,7 @@ private <R> Map<String, GetSetPair> getGetSetPairs(Class<R> recordClass)\n       GetSetPair gsp = cur.getValue();\n       if ((gsp.getMethod == null) ||\n           (gsp.setMethod == null)) {\n-        LOG.info(String.format(\"Exclude protential property: %s\\n\", gsp.propertyName));\n+        LOG.info(String.format(\"Exclude potential property: %s\\n\", gsp.propertyName));\n         itr.remove();\n       } else {\n         LOG.info(String.format(\"New property: %s type: %s\", gsp.toString(), gsp.type));"
    },
    {
        "commit_id": "4db119b7b55213929e5b86f2abb0ed84a21719b5",
        "commit_message": "YARN-6079. Fix simple spelling errors in yarn test code. Contributed by vijay.",
        "commit_url": "https://github.com/apache/hadoop/commit/4db119b7b55213929e5b86f2abb0ed84a21719b5",
        "buggy_code": "Assert.assertTrue(\"invalid label charactor should not add to repo\", caught);",
        "fixed_code": "Assert.assertTrue(\"invalid label character should not add to repo\", caught);",
        "patch": "@@ -155,7 +155,7 @@ public void testAddInvalidlabel() throws IOException {\n     } catch (IOException e) {\n       caught = true;\n     }\n-    Assert.assertTrue(\"invalid label charactor should not add to repo\", caught);\n+    Assert.assertTrue(\"invalid label character should not add to repo\", caught);\n \n     caught = false;\n     try {"
    },
    {
        "commit_id": "4db119b7b55213929e5b86f2abb0ed84a21719b5",
        "commit_message": "YARN-6079. Fix simple spelling errors in yarn test code. Contributed by vijay.",
        "commit_url": "https://github.com/apache/hadoop/commit/4db119b7b55213929e5b86f2abb0ed84a21719b5",
        "buggy_code": "Assert.assertTrue(\"Node script time out message not propogated\",",
        "fixed_code": "Assert.assertTrue(\"Node script time out message not propagated\",",
        "patch": "@@ -163,7 +163,7 @@ public void testNodeHealthService() throws Exception {\n     LOG.info(\"Checking Healthy--->timeout\");\n     Assert.assertFalse(\"Node health status reported healthy even after timeout\",\n         healthStatus.getIsNodeHealthy());\n-    Assert.assertTrue(\"Node script time out message not propogated\",\n+    Assert.assertTrue(\"Node script time out message not propagated\",\n         healthStatus.getHealthReport().equals(\n             NodeHealthScriptRunner.NODE_HEALTH_SCRIPT_TIMED_OUT_MSG\n             + NodeHealthCheckerService.SEPARATOR"
    },
    {
        "commit_id": "4db119b7b55213929e5b86f2abb0ed84a21719b5",
        "commit_message": "YARN-6079. Fix simple spelling errors in yarn test code. Contributed by vijay.",
        "commit_url": "https://github.com/apache/hadoop/commit/4db119b7b55213929e5b86f2abb0ed84a21719b5",
        "buggy_code": "throw new Exception(\"Unexpected resource recevied.\");",
        "fixed_code": "throw new Exception(\"Unexpected resource received.\");",
        "patch": "@@ -2102,7 +2102,7 @@ rls.new LocalizerRunner(new LocalizerContext(user, container1\n             Assert.assertEquals(userCachePath, destinationDirectory.getParent()\n               .toUri().toString());\n           } else {\n-            throw new Exception(\"Unexpected resource recevied.\");\n+            throw new Exception(\"Unexpected resource received.\");\n           }\n         }\n       }"
    },
    {
        "commit_id": "4db119b7b55213929e5b86f2abb0ed84a21719b5",
        "commit_message": "YARN-6079. Fix simple spelling errors in yarn test code. Contributed by vijay.",
        "commit_url": "https://github.com/apache/hadoop/commit/4db119b7b55213929e5b86f2abb0ed84a21719b5",
        "buggy_code": "Assert.assertEquals(\"Unhelathy Nodes\", initialUnHealthy,",
        "fixed_code": "Assert.assertEquals(\"Unhealthy Nodes\", initialUnHealthy,",
        "patch": "@@ -917,7 +917,7 @@ public void testResourceUpdateOnRebootedNode() {\n \n     Assert.assertEquals(NodeState.REBOOTED, node.getState());\n     Assert.assertEquals(\"Active Nodes\", initialActive, cm.getNumActiveNMs());\n-    Assert.assertEquals(\"Unhelathy Nodes\", initialUnHealthy,\n+    Assert.assertEquals(\"Unhealthy Nodes\", initialUnHealthy,\n         cm.getUnhealthyNMs());\n     Assert.assertEquals(\"Decommissioning Nodes\", initialDecommissioning,\n         cm.getNumDecommissioningNMs());"
    },
    {
        "commit_id": "4db119b7b55213929e5b86f2abb0ed84a21719b5",
        "commit_message": "YARN-6079. Fix simple spelling errors in yarn test code. Contributed by vijay.",
        "commit_url": "https://github.com/apache/hadoop/commit/4db119b7b55213929e5b86f2abb0ed84a21719b5",
        "buggy_code": "\"Exepected AbsoluteUsedCapacity > 0.95, got: \"",
        "fixed_code": "\"Expected AbsoluteUsedCapacity > 0.95, got: \"",
        "patch": "@@ -827,7 +827,7 @@ public void testDRFUserLimits() throws Exception {\n     assertTrue(\"Verify user_1 got resources \", queueUser1.getUsed()\n         .getMemorySize() > 0);\n     assertTrue(\n-        \"Exepected AbsoluteUsedCapacity > 0.95, got: \"\n+        \"Expected AbsoluteUsedCapacity > 0.95, got: \"\n             + b.getAbsoluteUsedCapacity(), b.getAbsoluteUsedCapacity() > 0.95);\n \n     // Verify consumedRatio is based on dominant resources"
    },
    {
        "commit_id": "4db119b7b55213929e5b86f2abb0ed84a21719b5",
        "commit_message": "YARN-6079. Fix simple spelling errors in yarn test code. Contributed by vijay.",
        "commit_url": "https://github.com/apache/hadoop/commit/4db119b7b55213929e5b86f2abb0ed84a21719b5",
        "buggy_code": "Assert.fail(\"Exception is not expteced.\");",
        "fixed_code": "Assert.fail(\"Exception is not expected.\");",
        "patch": "@@ -268,7 +268,7 @@ public static ClientRMService mockClientRMService(RMContext rmContext) {\n       when(clientRMService.getApplications(any(GetApplicationsRequest.class)))\n           .thenReturn(response);\n     } catch (YarnException e) {\n-      Assert.fail(\"Exception is not expteced.\");\n+      Assert.fail(\"Exception is not expected.\");\n     }\n     return clientRMService;\n   }"
    },
    {
        "commit_id": "4db119b7b55213929e5b86f2abb0ed84a21719b5",
        "commit_message": "YARN-6079. Fix simple spelling errors in yarn test code. Contributed by vijay.",
        "commit_url": "https://github.com/apache/hadoop/commit/4db119b7b55213929e5b86f2abb0ed84a21719b5",
        "buggy_code": "fail(\"resourceInfo object shouldnt be available for finished apps\");",
        "fixed_code": "fail(\"resourceInfo object shouldn't be available for finished apps\");",
        "patch": "@@ -156,7 +156,7 @@ public void testAppsFinished() throws JSONException, Exception {\n     assertEquals(\"incorrect number of elements\", 1, apps.length());\n     try {\n       apps.getJSONArray(\"app\").getJSONObject(0).getJSONObject(\"resourceInfo\");\n-      fail(\"resourceInfo object shouldnt be available for finished apps\");\n+      fail(\"resourceInfo object shouldn't be available for finished apps\");\n     } catch (Exception e) {\n       assertTrue(\"resourceInfo shouldn't be available for finished apps\",\n           true);"
    },
    {
        "commit_id": "9262797e86453fc04b7ca3710b73b21fcdf9e6b4",
        "commit_message": "YARN-5257. Fix unreleased resources and null dereferences (yufeigu via rkanter)",
        "commit_url": "https://github.com/apache/hadoop/commit/9262797e86453fc04b7ca3710b73b21fcdf9e6b4",
        "buggy_code": "} else if (type.equals(DOMAIN_DATA_TYPE)) {",
        "fixed_code": "} else if (type.equals(DOMAIN_DATA_TYPE) && domains != null) {",
        "patch": "@@ -805,7 +805,7 @@ private static void putTimelineDataInJSONFile(String path, String type) {\n                 error.getErrorCode());\n           }\n         }\n-      } else if (type.equals(DOMAIN_DATA_TYPE)) {\n+      } else if (type.equals(DOMAIN_DATA_TYPE) && domains != null) {\n         boolean hasError = false;\n         for (TimelineDomain domain : domains.getDomains()) {\n           try {"
    },
    {
        "commit_id": "9262797e86453fc04b7ca3710b73b21fcdf9e6b4",
        "commit_message": "YARN-5257. Fix unreleased resources and null dereferences (yufeigu via rkanter)",
        "commit_url": "https://github.com/apache/hadoop/commit/9262797e86453fc04b7ca3710b73b21fcdf9e6b4",
        "buggy_code": "new FileOutputStream(filepath), Charset.forName(\"UTF-8\"));) {",
        "fixed_code": "new FileOutputStream(filepath), Charset.forName(\"UTF-8\"))) {",
        "patch": "@@ -189,7 +189,7 @@ public String generateGraphViz() {\n \n   public void save(String filepath) throws IOException {\n     try (OutputStreamWriter fout = new OutputStreamWriter(\n-        new FileOutputStream(filepath), Charset.forName(\"UTF-8\"));) {\n+        new FileOutputStream(filepath), Charset.forName(\"UTF-8\"))) {\n       fout.write(generateGraphViz());\n     }\n   }"
    },
    {
        "commit_id": "9262797e86453fc04b7ca3710b73b21fcdf9e6b4",
        "commit_message": "YARN-5257. Fix unreleased resources and null dereferences (yufeigu via rkanter)",
        "commit_url": "https://github.com/apache/hadoop/commit/9262797e86453fc04b7ca3710b73b21fcdf9e6b4",
        "buggy_code": "if (oldState != newState) {",
        "fixed_code": "if (newState != null && oldState != newState) {",
        "patch": "@@ -603,7 +603,7 @@ public void handle(ApplicationEvent event) {\n       } catch (InvalidStateTransitionException e) {\n         LOG.warn(\"Can't handle this event at current state\", e);\n       }\n-      if (oldState != newState) {\n+      if (newState != null && oldState != newState) {\n         LOG.info(\"Application \" + applicationID + \" transitioned from \"\n             + oldState + \" to \" + newState);\n       }"
    },
    {
        "commit_id": "9262797e86453fc04b7ca3710b73b21fcdf9e6b4",
        "commit_message": "YARN-5257. Fix unreleased resources and null dereferences (yufeigu via rkanter)",
        "commit_url": "https://github.com/apache/hadoop/commit/9262797e86453fc04b7ca3710b73b21fcdf9e6b4",
        "buggy_code": "if (oldState != newState) {",
        "fixed_code": "if (newState != null && oldState != newState) {",
        "patch": "@@ -1680,7 +1680,7 @@ public void handle(ContainerEvent event) {\n             + oldState + \"], eventType: [\" + event.getType() + \"],\" +\n             \" container: [\" + containerID + \"]\", e);\n       }\n-      if (oldState != newState) {\n+      if (newState != null && oldState != newState) {\n         LOG.info(\"Container \" + containerID + \" transitioned from \"\n             + oldState\n             + \" to \" + newState);"
    },
    {
        "commit_id": "9262797e86453fc04b7ca3710b73b21fcdf9e6b4",
        "commit_message": "YARN-5257. Fix unreleased resources and null dereferences (yufeigu via rkanter)",
        "commit_url": "https://github.com/apache/hadoop/commit/9262797e86453fc04b7ca3710b73b21fcdf9e6b4",
        "buggy_code": "if (oldState != newState) {",
        "fixed_code": "if (newState != null && oldState != newState) {",
        "patch": "@@ -200,7 +200,7 @@ public void handle(ResourceEvent event) {\n       } catch (InvalidStateTransitionException e) {\n         LOG.warn(\"Can't handle this event at current state\", e);\n       }\n-      if (oldState != newState) {\n+      if (newState != null && oldState != newState) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Resource \" + resourcePath + (localPath != null ?\n               \"(->\" + localPath + \")\": \"\") + \" transitioned from \" + oldState"
    },
    {
        "commit_id": "e216e8e2334519b7c833d99586218e99a39265f3",
        "commit_message": "HADOOP-13932. Fix indefinite article in comments (Contributed by LiXin Ge via Daniel Templeton)",
        "commit_url": "https://github.com/apache/hadoop/commit/e216e8e2334519b7c833d99586218e99a39265f3",
        "buggy_code": "LOG.info(\"Exception while executing a FS operation.\", e);",
        "fixed_code": "LOG.info(\"Exception while executing an FS operation.\", e);",
        "patch": "@@ -740,7 +740,7 @@ T runWithRetries() throws Exception {\n         try {\n           return run();\n         } catch (IOException e) {\n-          LOG.info(\"Exception while executing a FS operation.\", e);\n+          LOG.info(\"Exception while executing an FS operation.\", e);\n           if (++retry > fsNumRetries) {\n             LOG.info(\"Maxed out FS retries. Giving up!\");\n             throw e;"
    },
    {
        "commit_id": "8f218ea2849477bdcb4918586aaefed0cd118341",
        "commit_message": "HDFS-11250. Fix a typo in ReplicaUnderRecovery#setRecoveryID. Contributed by Yiqun Lin.",
        "commit_url": "https://github.com/apache/hadoop/commit/8f218ea2849477bdcb4918586aaefed0cd118341",
        "buggy_code": "throw new IllegalArgumentException(\"The new rcovery id: \" + recoveryId",
        "fixed_code": "throw new IllegalArgumentException(\"The new recovery id: \" + recoveryId",
        "patch": "@@ -65,7 +65,7 @@ public void setRecoveryID(long recoveryId) {\n     if (recoveryId > this.recoveryId) {\n       this.recoveryId = recoveryId;\n     } else {\n-      throw new IllegalArgumentException(\"The new rcovery id: \" + recoveryId\n+      throw new IllegalArgumentException(\"The new recovery id: \" + recoveryId\n           + \" must be greater than the current one: \" + this.recoveryId);\n     }\n   }"
    },
    {
        "commit_id": "ada876cd1d22b61f237603cf339bbed65285dab8",
        "commit_message": "Revert YARN-4126. RM should not issue delegation tokens in unsecure mode.",
        "commit_url": "https://github.com/apache/hadoop/commit/ada876cd1d22b61f237603cf339bbed65285dab8",
        "buggy_code": "return false;",
        "fixed_code": "return true;",
        "patch": "@@ -1238,7 +1238,7 @@ private boolean isAllowedDelegationTokenOp() throws IOException {\n           .contains(UserGroupInformation.getCurrentUser()\n                   .getRealAuthenticationMethod());\n     } else {\n-      return false;\n+      return true;\n     }\n   }\n "
    },
    {
        "commit_id": "b0b033ea2e462356b8bbcf7790953ac09c712430",
        "commit_message": "MAPREDUCE-6821. Fix javac warning related to the deprecated APIs after upgrading Jackson. Contributed by Yiqin Lin.",
        "commit_url": "https://github.com/apache/hadoop/commit/b0b033ea2e462356b8bbcf7790953ac09c712430",
        "buggy_code": "new ObjectMapper().reader(Map.class);",
        "fixed_code": "new ObjectMapper().readerFor(Map.class);",
        "patch": "@@ -72,7 +72,7 @@\n class JobSubmitter {\n   protected static final Log LOG = LogFactory.getLog(JobSubmitter.class);\n   private static final ObjectReader READER =\n-      new ObjectMapper().reader(Map.class);\n+      new ObjectMapper().readerFor(Map.class);\n   private static final String SHUFFLE_KEYGEN_ALGORITHM = \"HmacSHA1\";\n   private static final int SHUFFLE_KEY_LENGTH = 64;\n   private FileSystem jtFs;"
    },
    {
        "commit_id": "2d4731c067ff64cd88f496eac8faaf302faa2ccc",
        "commit_message": "HDFS-11233. Fix javac warnings related to the deprecated APIs after upgrading Jackson. Contributed by Yiqun Lin.",
        "commit_url": "https://github.com/apache/hadoop/commit/2d4731c067ff64cd88f496eac8faaf302faa2ccc",
        "buggy_code": "new ObjectMapper().reader(DiskBalancerWorkItem.class);",
        "fixed_code": "new ObjectMapper().readerFor(DiskBalancerWorkItem.class);",
        "patch": "@@ -37,7 +37,7 @@\n public class DiskBalancerWorkItem {\n   private static final ObjectMapper MAPPER = new ObjectMapper();\n   private static final ObjectReader READER =\n-      new ObjectMapper().reader(DiskBalancerWorkItem.class);\n+      new ObjectMapper().readerFor(DiskBalancerWorkItem.class);\n \n   private  long startTime;\n   private long secondsElapsed;"
    },
    {
        "commit_id": "2d4731c067ff64cd88f496eac8faaf302faa2ccc",
        "commit_message": "HDFS-11233. Fix javac warnings related to the deprecated APIs after upgrading Jackson. Contributed by Yiqun Lin.",
        "commit_url": "https://github.com/apache/hadoop/commit/2d4731c067ff64cd88f496eac8faaf302faa2ccc",
        "buggy_code": "new ObjectMapper().reader(DatanodeAdminProperties.class);",
        "fixed_code": "new ObjectMapper().readerFor(DatanodeAdminProperties.class);",
        "patch": "@@ -48,7 +48,7 @@\n @InterfaceStability.Unstable\n public final class CombinedHostsFileReader {\n   private static final ObjectReader READER =\n-      new ObjectMapper().reader(DatanodeAdminProperties.class);\n+      new ObjectMapper().readerFor(DatanodeAdminProperties.class);\n   private static final JsonFactory JSON_FACTORY = new JsonFactory();\n \n   private CombinedHostsFileReader() {"
    },
    {
        "commit_id": "2d4731c067ff64cd88f496eac8faaf302faa2ccc",
        "commit_message": "HDFS-11233. Fix javac warnings related to the deprecated APIs after upgrading Jackson. Contributed by Yiqun Lin.",
        "commit_url": "https://github.com/apache/hadoop/commit/2d4731c067ff64cd88f496eac8faaf302faa2ccc",
        "buggy_code": "ObjectReader reader = new ObjectMapper().reader(List.class);",
        "fixed_code": "ObjectReader reader = new ObjectMapper().readerFor(List.class);",
        "patch": "@@ -526,7 +526,7 @@ static List<String> toXAttrNames(final Map<?, ?> json)\n     }\n \n     final String namesInJson = (String) json.get(\"XAttrNames\");\n-    ObjectReader reader = new ObjectMapper().reader(List.class);\n+    ObjectReader reader = new ObjectMapper().readerFor(List.class);\n     final List<Object> xattrs = reader.readValue(namesInJson);\n     final List<String> names =\n         Lists.newArrayListWithCapacity(json.keySet().size());"
    },
    {
        "commit_id": "2d4731c067ff64cd88f496eac8faaf302faa2ccc",
        "commit_message": "HDFS-11233. Fix javac warnings related to the deprecated APIs after upgrading Jackson. Contributed by Yiqun Lin.",
        "commit_url": "https://github.com/apache/hadoop/commit/2d4731c067ff64cd88f496eac8faaf302faa2ccc",
        "buggy_code": "new ObjectMapper().reader(Map.class);",
        "fixed_code": "new ObjectMapper().readerFor(Map.class);",
        "patch": "@@ -152,7 +152,7 @@ public class WebHdfsFileSystem extends FileSystem\n   private String restCsrfCustomHeader;\n   private Set<String> restCsrfMethodsToIgnore;\n   private static final ObjectReader READER =\n-      new ObjectMapper().reader(Map.class);\n+      new ObjectMapper().readerFor(Map.class);\n \n   private DFSOpsCountStatistics storageStatistics;\n "
    },
    {
        "commit_id": "2d4731c067ff64cd88f496eac8faaf302faa2ccc",
        "commit_message": "HDFS-11233. Fix javac warnings related to the deprecated APIs after upgrading Jackson. Contributed by Yiqun Lin.",
        "commit_url": "https://github.com/apache/hadoop/commit/2d4731c067ff64cd88f496eac8faaf302faa2ccc",
        "buggy_code": "new ObjectMapper().reader(Map.class);",
        "fixed_code": "new ObjectMapper().readerFor(Map.class);",
        "patch": "@@ -56,7 +56,7 @@\n public class ConfRefreshTokenBasedAccessTokenProvider\n     extends AccessTokenProvider {\n   private static final ObjectReader READER =\n-      new ObjectMapper().reader(Map.class);\n+      new ObjectMapper().readerFor(Map.class);\n \n   public static final String OAUTH_REFRESH_TOKEN_KEY\n       = \"dfs.webhdfs.oauth2.refresh.token\";"
    },
    {
        "commit_id": "2d4731c067ff64cd88f496eac8faaf302faa2ccc",
        "commit_message": "HDFS-11233. Fix javac warnings related to the deprecated APIs after upgrading Jackson. Contributed by Yiqun Lin.",
        "commit_url": "https://github.com/apache/hadoop/commit/2d4731c067ff64cd88f496eac8faaf302faa2ccc",
        "buggy_code": "new ObjectMapper().reader(Map.class);",
        "fixed_code": "new ObjectMapper().readerFor(Map.class);",
        "patch": "@@ -56,7 +56,7 @@\n public abstract class CredentialBasedAccessTokenProvider\n     extends AccessTokenProvider {\n   private static final ObjectReader READER =\n-      new ObjectMapper().reader(Map.class);\n+      new ObjectMapper().readerFor(Map.class);\n \n   public static final String OAUTH_CREDENTIAL_KEY\n       = \"dfs.webhdfs.oauth2.credential\";"
    },
    {
        "commit_id": "2d4731c067ff64cd88f496eac8faaf302faa2ccc",
        "commit_message": "HDFS-11233. Fix javac warnings related to the deprecated APIs after upgrading Jackson. Contributed by Yiqun Lin.",
        "commit_url": "https://github.com/apache/hadoop/commit/2d4731c067ff64cd88f496eac8faaf302faa2ccc",
        "buggy_code": "new ObjectMapper().reader(BlockIteratorState.class);",
        "fixed_code": "new ObjectMapper().readerFor(BlockIteratorState.class);",
        "patch": "@@ -106,7 +106,7 @@ public class FsVolumeImpl implements FsVolumeSpi {\n   private static final ObjectWriter WRITER =\n       new ObjectMapper().writerWithDefaultPrettyPrinter();\n   private static final ObjectReader READER =\n-      new ObjectMapper().reader(BlockIteratorState.class);\n+      new ObjectMapper().readerFor(BlockIteratorState.class);\n \n   private final FsDatasetImpl dataset;\n   private final String storageID;"
    },
    {
        "commit_id": "2d4731c067ff64cd88f496eac8faaf302faa2ccc",
        "commit_message": "HDFS-11233. Fix javac warnings related to the deprecated APIs after upgrading Jackson. Contributed by Yiqun Lin.",
        "commit_url": "https://github.com/apache/hadoop/commit/2d4731c067ff64cd88f496eac8faaf302faa2ccc",
        "buggy_code": "new ObjectMapper().reader(HashMap.class);",
        "fixed_code": "new ObjectMapper().readerFor(HashMap.class);",
        "patch": "@@ -77,7 +77,7 @@\n  */\n public abstract class Command extends Configured implements Closeable {\n   private static final ObjectReader READER =\n-      new ObjectMapper().reader(HashMap.class);\n+      new ObjectMapper().readerFor(HashMap.class);\n   static final Logger LOG = LoggerFactory.getLogger(Command.class);\n   private Map<String, String> validArgs = new HashMap<>();\n   private URI clusterURI;"
    },
    {
        "commit_id": "2d4731c067ff64cd88f496eac8faaf302faa2ccc",
        "commit_message": "HDFS-11233. Fix javac warnings related to the deprecated APIs after upgrading Jackson. Contributed by Yiqun Lin.",
        "commit_url": "https://github.com/apache/hadoop/commit/2d4731c067ff64cd88f496eac8faaf302faa2ccc",
        "buggy_code": "new ObjectMapper().reader(DiskBalancerCluster.class);",
        "fixed_code": "new ObjectMapper().readerFor(DiskBalancerCluster.class);",
        "patch": "@@ -38,7 +38,7 @@ public class JsonNodeConnector implements ClusterConnector {\n   private static final Logger LOG =\n       LoggerFactory.getLogger(JsonNodeConnector.class);\n   private static final ObjectReader READER =\n-      new ObjectMapper().reader(DiskBalancerCluster.class);\n+      new ObjectMapper().readerFor(DiskBalancerCluster.class);\n   private final URL clusterURI;\n \n   /**"
    },
    {
        "commit_id": "2d4731c067ff64cd88f496eac8faaf302faa2ccc",
        "commit_message": "HDFS-11233. Fix javac warnings related to the deprecated APIs after upgrading Jackson. Contributed by Yiqun Lin.",
        "commit_url": "https://github.com/apache/hadoop/commit/2d4731c067ff64cd88f496eac8faaf302faa2ccc",
        "buggy_code": "new ObjectMapper().reader(DiskBalancerCluster.class);",
        "fixed_code": "new ObjectMapper().readerFor(DiskBalancerCluster.class);",
        "patch": "@@ -73,7 +73,7 @@ public class DiskBalancerCluster {\n   private static final Logger LOG =\n       LoggerFactory.getLogger(DiskBalancerCluster.class);\n   private static final ObjectReader READER =\n-      new ObjectMapper().reader(DiskBalancerCluster.class);\n+      new ObjectMapper().readerFor(DiskBalancerCluster.class);\n   private final Set<String> exclusionList;\n   private final Set<String> inclusionList;\n   private ClusterConnector clusterConnector;"
    },
    {
        "commit_id": "2d4731c067ff64cd88f496eac8faaf302faa2ccc",
        "commit_message": "HDFS-11233. Fix javac warnings related to the deprecated APIs after upgrading Jackson. Contributed by Yiqun Lin.",
        "commit_url": "https://github.com/apache/hadoop/commit/2d4731c067ff64cd88f496eac8faaf302faa2ccc",
        "buggy_code": "new ObjectMapper().reader(DiskBalancerVolume.class);",
        "fixed_code": "new ObjectMapper().readerFor(DiskBalancerVolume.class);",
        "patch": "@@ -33,7 +33,7 @@\n @JsonIgnoreProperties(ignoreUnknown = true)\n public class DiskBalancerVolume {\n   private static final ObjectReader READER =\n-      new ObjectMapper().reader(DiskBalancerVolume.class);\n+      new ObjectMapper().readerFor(DiskBalancerVolume.class);\n \n   private String path;\n   private long capacity;"
    },
    {
        "commit_id": "2d4731c067ff64cd88f496eac8faaf302faa2ccc",
        "commit_message": "HDFS-11233. Fix javac warnings related to the deprecated APIs after upgrading Jackson. Contributed by Yiqun Lin.",
        "commit_url": "https://github.com/apache/hadoop/commit/2d4731c067ff64cd88f496eac8faaf302faa2ccc",
        "buggy_code": "return mapper.writerWithType(planType)",
        "fixed_code": "return mapper.writerFor(planType)",
        "patch": "@@ -166,7 +166,7 @@ public static NodePlan parseJson(String json) throws IOException {\n   public String toJson() throws IOException {\n     ObjectMapper mapper = new ObjectMapper();\n     JavaType planType = mapper.constructType(NodePlan.class);\n-    return mapper.writerWithType(planType)\n+    return mapper.writerFor(planType)\n         .writeValueAsString(this);\n   }\n "
    },
    {
        "commit_id": "43ebff2e354142bddcb42755766a965ae8a503a6",
        "commit_message": "YARN-5559. Analyse 2.8.0/3.0.0 jdiff reports and fix any issues. Contributed by  Akira Ajisaka & Wangda Tan",
        "commit_url": "https://github.com/apache/hadoop/commit/43ebff2e354142bddcb42755766a965ae8a503a6",
        "buggy_code": "GetClusterNodeLabelsRequest.newInstance()).getNodeLabels();",
        "fixed_code": "GetClusterNodeLabelsRequest.newInstance()).getNodeLabelList();",
        "patch": "@@ -899,7 +899,7 @@ public Map<NodeLabel, Set<NodeId>> getLabelsToNodes(Set<String> labels)\n   @Override\n   public List<NodeLabel> getClusterNodeLabels() throws YarnException, IOException {\n     return rmClient.getClusterNodeLabels(\n-        GetClusterNodeLabelsRequest.newInstance()).getNodeLabels();\n+        GetClusterNodeLabelsRequest.newInstance()).getNodeLabelList();\n   }\n \n   @Override"
    },
    {
        "commit_id": "0cfd7ad21f4457513ed3416e5d77f3123bfe9da0",
        "commit_message": "MAPREDUCE-6815. Fix flaky TestKill.testKillTask(). Contributed by Haibo Chen",
        "commit_url": "https://github.com/apache/hadoop/commit/0cfd7ad21f4457513ed3416e5d77f3123bfe9da0",
        "buggy_code": "app.waitForState(job, JobState.RUNNING);",
        "fixed_code": "app.waitForInternalState((JobImpl) job, JobStateInternal.RUNNING);",
        "patch": "@@ -105,7 +105,7 @@ public void testKillTask() throws Exception {\n     Job job = app.submit(new Configuration());\n     \n     //wait and vailidate for Job to become RUNNING\n-    app.waitForState(job, JobState.RUNNING);\n+    app.waitForInternalState((JobImpl) job, JobStateInternal.RUNNING);\n     Map<TaskId,Task> tasks = job.getTasks();\n     Assert.assertEquals(\"No of tasks is not correct\", 2, \n         tasks.size());"
    },
    {
        "commit_id": "aa3cab1eb29c56368d15882d7260a994e615e8d8",
        "commit_message": "YARN-5799. Fix Opportunistic Allocation to set the correct value of Node Http Address. (asuresh)",
        "commit_url": "https://github.com/apache/hadoop/commit/aa3cab1eb29c56368d15882d7260a994e615e8d8",
        "buggy_code": "context.getContainerTokenSecretManager(), webServer.getPort()));",
        "fixed_code": "context.getContainerTokenSecretManager()));",
        "patch": "@@ -374,7 +374,7 @@ protected void serviceInit(Configuration conf) throws Exception {\n \n     ((NMContext) context).setQueueableContainerAllocator(\n         new OpportunisticContainerAllocator(\n-            context.getContainerTokenSecretManager(), webServer.getPort()));\n+            context.getContainerTokenSecretManager()));\n \n     dispatcher.register(ContainerManagerEventType.class, containerManager);\n     dispatcher.register(NodeManagerEventType.class, this);"
    },
    {
        "commit_id": "f63cd78f6008bf7cfc9ee74217ed6f3d4f5bec5c",
        "commit_message": "HDFS-10730. Fix some failed tests due to BindException. Contributed by Yiqun Lin",
        "commit_url": "https://github.com/apache/hadoop/commit/f63cd78f6008bf7cfc9ee74217ed6f3d4f5bec5c",
        "buggy_code": "cluster.restartDataNode(dnp, true);",
        "fixed_code": "cluster.restartDataNode(dnp);",
        "patch": "@@ -252,7 +252,7 @@ public void run() {\n     Thread.sleep(3000); // grace period to trigger decommissioning call\n     // start datanode so that decommissioning live node will be finished\n     for (DataNodeProperties dnp : stoppedDns) {\n-      cluster.restartDataNode(dnp, true);\n+      cluster.restartDataNode(dnp);\n       LOG.info(\"Restarts stopped datanode:{} to trigger block reconstruction\",\n           dnp.datanode);\n     }"
    },
    {
        "commit_id": "f63cd78f6008bf7cfc9ee74217ed6f3d4f5bec5c",
        "commit_message": "HDFS-10730. Fix some failed tests due to BindException. Contributed by Yiqun Lin",
        "commit_url": "https://github.com/apache/hadoop/commit/f63cd78f6008bf7cfc9ee74217ed6f3d4f5bec5c",
        "buggy_code": "cluster.restartDataNode(dnIdxToDie, true);",
        "fixed_code": "cluster.restartDataNode(dnIdxToDie);",
        "patch": "@@ -479,7 +479,7 @@ private FileChecksum getFileChecksum(String filePath, int range,\n     }\n \n     if (dnIdxToDie != -1) {\n-      cluster.restartDataNode(dnIdxToDie, true);\n+      cluster.restartDataNode(dnIdxToDie);\n     }\n \n     return fc;"
    },
    {
        "commit_id": "efdf810cf9f72d78e97e860576c64a382ece437c",
        "commit_message": "HADOOP-7352. FileSystem#listStatus should throw IOE upon access error. Contributed by John Zhuge.",
        "commit_url": "https://github.com/apache/hadoop/commit/efdf810cf9f72d78e97e860576c64a382ece437c",
        "buggy_code": "return null;",
        "fixed_code": "return new FileStatus[0];",
        "patch": "@@ -196,7 +196,7 @@ public Path getWorkingDirectory() {\n \n     @Override\n     public FileStatus[] listStatus(Path f) throws IOException {\n-      return null;\n+      return new FileStatus[0];\n     }\n \n     @Override"
    },
    {
        "commit_id": "efdf810cf9f72d78e97e860576c64a382ece437c",
        "commit_message": "HADOOP-7352. FileSystem#listStatus should throw IOE upon access error. Contributed by John Zhuge.",
        "commit_url": "https://github.com/apache/hadoop/commit/efdf810cf9f72d78e97e860576c64a382ece437c",
        "buggy_code": "return null;",
        "fixed_code": "return new FileStatus[0];",
        "patch": "@@ -226,7 +226,7 @@ public Path getWorkingDirectory() {\n \n     @Override\n     public FileStatus[] listStatus(Path f) throws IOException {\n-      return null;\n+      return new FileStatus[0];\n     }\n \n     @Override"
    },
    {
        "commit_id": "2ab1ef15c5e0b05fed5106d6bbecb3ead2b25f9a",
        "commit_message": "HDFS-10908. Improve StripedBlockReader#createBlockReader error logging. Contributed by Manoj Govindassamy.",
        "commit_url": "https://github.com/apache/hadoop/commit/2ab1ef15c5e0b05fed5106d6bbecb3ead2b25f9a",
        "buggy_code": "LOG.debug(\"Exception while creating remote block reader, datanode {}\",",
        "fixed_code": "LOG.info(\"Exception while creating remote block reader, datanode {}\",",
        "patch": "@@ -122,7 +122,7 @@ private BlockReader createBlockReader(long offsetInBlock) {\n           \"\", newConnectedPeer(block, dnAddr, blockToken, source), source,\n           null, stripedReader.getCachingStrategy(), datanode.getTracer(), -1);\n     } catch (IOException e) {\n-      LOG.debug(\"Exception while creating remote block reader, datanode {}\",\n+      LOG.info(\"Exception while creating remote block reader, datanode {}\",\n           source, e);\n       return null;\n     }"
    },
    {
        "commit_id": "82c55dcbc8e3d5314aae9f8f600c660759213e45",
        "commit_message": "HADOOP-13640. Fix findbugs warning in VersionInfoMojo.java. Contributed by Yuanbo Liu.",
        "commit_url": "https://github.com/apache/hadoop/commit/82c55dcbc8e3d5314aae9f8f600c660759213e45",
        "buggy_code": "index = path.indexOf(\"/\", branchIndex);",
        "fixed_code": "index = path.indexOf('/', branchIndex);",
        "patch": "@@ -160,7 +160,7 @@ private String[] getSvnUriInfo(String str) {\n       if (index > -1) {\n         res[0] = path.substring(0, index - 1);\n         int branchIndex = index + \"branches\".length() + 1;\n-        index = path.indexOf(\"/\", branchIndex);\n+        index = path.indexOf('/', branchIndex);\n         if (index > -1) {\n           res[1] = path.substring(branchIndex, index);\n         } else {"
    },
    {
        "commit_id": "8ae4729107d33c6001cf1fdc8837afb71ea6c0d3",
        "commit_message": "HDFS-10828. Fix usage of FsDatasetImpl object lock in ReplicaMap. (Arpit Agarwal)",
        "commit_url": "https://github.com/apache/hadoop/commit/8ae4729107d33c6001cf1fdc8837afb71ea6c0d3",
        "buggy_code": "ReplicaMap replicaMap = new ReplicaMap(dataset);",
        "fixed_code": "ReplicaMap replicaMap = new ReplicaMap(dataset.datasetLock);",
        "patch": "@@ -407,7 +407,7 @@ public void changeStoredGenerationStamp(\n   @Override\n   public Iterator<Replica> getStoredReplicas(String bpid) throws IOException {\n     // Reload replicas from the disk.\n-    ReplicaMap replicaMap = new ReplicaMap(dataset);\n+    ReplicaMap replicaMap = new ReplicaMap(dataset.datasetLock);\n     try (FsVolumeReferences refs = dataset.getFsVolumeReferences()) {\n       for (FsVolumeSpi vol : refs) {\n         FsVolumeImpl volume = (FsVolumeImpl) vol;"
    },
    {
        "commit_id": "e5ef51e717647328db9f2b80f21fe44b99079d08",
        "commit_message": "HADOOP-13643. Math error in AbstractContractDistCpTest. Contributed by Aaron Fabbri.",
        "commit_url": "https://github.com/apache/hadoop/commit/e5ef51e717647328db9f2b80f21fe44b99079d08",
        "buggy_code": "int fileSizeMb = fileSizeKb * 1024;",
        "fixed_code": "int fileSizeMb = fileSizeKb / 1024;",
        "patch": "@@ -160,7 +160,7 @@ private void largeFiles(FileSystem srcFS, Path srcDir, FileSystem dstFS,\n     Path inputFile3 = new Path(inputDir, \"file3\");\n     mkdirs(srcFS, inputDir);\n     int fileSizeKb = conf.getInt(\"scale.test.distcp.file.size.kb\", 10 * 1024);\n-    int fileSizeMb = fileSizeKb * 1024;\n+    int fileSizeMb = fileSizeKb / 1024;\n     getLog().info(\"{} with file size {}\", testName.getMethodName(), fileSizeMb);\n     byte[] data1 = dataset((fileSizeMb + 1) * 1024 * 1024, 33, 43);\n     createFile(srcFS, inputFile1, true, data1);"
    },
    {
        "commit_id": "e80386d69d5fb6a08aa3366e42d2518747af569f",
        "commit_message": "HADOOP-13601. Fix a log message typo in AbstractDelegationTokenSecretManager. Contributed by Mehran Hassani.",
        "commit_url": "https://github.com/apache/hadoop/commit/e80386d69d5fb6a08aa3366e42d2518747af569f",
        "buggy_code": "LOG.info(\"Token cancelation requested for identifier: \"+id);",
        "fixed_code": "LOG.info(\"Token cancellation requested for identifier: \" + id);",
        "patch": "@@ -528,7 +528,7 @@ public synchronized TokenIdent cancelToken(Token<TokenIdent> token,\n     DataInputStream in = new DataInputStream(buf);\n     TokenIdent id = createIdentifier();\n     id.readFields(in);\n-    LOG.info(\"Token cancelation requested for identifier: \"+id);\n+    LOG.info(\"Token cancellation requested for identifier: \" + id);\n     \n     if (id.getUser() == null) {\n       throw new InvalidToken(\"Token with no owner\");"
    },
    {
        "commit_id": "27c3b86252386c9c064a6420b3c650644cbb9ef3",
        "commit_message": "YARN-5564. Fix typo in RM_SCHEDULER_RESERVATION_THRESHOLD_INCREMENT_MULTIPLE. Contributed by  Ray Chiang",
        "commit_url": "https://github.com/apache/hadoop/commit/27c3b86252386c9c064a6420b3c650644cbb9ef3",
        "buggy_code": "RM_SCHEDULER_RESERVATION_THRESHOLD_INCERMENT_MULTIPLE,",
        "fixed_code": "RM_SCHEDULER_RESERVATION_THRESHOLD_INCREMENT_MULTIPLE,",
        "patch": "@@ -1455,7 +1455,7 @@ public void testReservationThresholdGatesReservations() throws Exception {\n     // Set threshold to 2 * 1024 ==> 2048 MB & 2 * 1 ==> 2 vcores (test will\n     // use vcores)\n     conf.setFloat(FairSchedulerConfiguration.\n-            RM_SCHEDULER_RESERVATION_THRESHOLD_INCERMENT_MULTIPLE,\n+            RM_SCHEDULER_RESERVATION_THRESHOLD_INCREMENT_MULTIPLE,\n         2f);\n     scheduler.init(conf);\n     scheduler.start();"
    },
    {
        "commit_id": "27c3b86252386c9c064a6420b3c650644cbb9ef3",
        "commit_message": "YARN-5564. Fix typo in RM_SCHEDULER_RESERVATION_THRESHOLD_INCREMENT_MULTIPLE. Contributed by  Ray Chiang",
        "commit_url": "https://github.com/apache/hadoop/commit/27c3b86252386c9c064a6420b3c650644cbb9ef3",
        "buggy_code": ".RM_SCHEDULER_RESERVATION_THRESHOLD_INCERMENT_MULTIPLE,",
        "fixed_code": ".RM_SCHEDULER_RESERVATION_THRESHOLD_INCREMENT_MULTIPLE,",
        "patch": "@@ -135,7 +135,7 @@ private void startResourceManagerWithRealFairScheduler() {\n     conf.setFloat(FairSchedulerConfiguration.PREEMPTION_THRESHOLD, 0f);\n     conf.setFloat(\n             FairSchedulerConfiguration\n-                    .RM_SCHEDULER_RESERVATION_THRESHOLD_INCERMENT_MULTIPLE,\n+                    .RM_SCHEDULER_RESERVATION_THRESHOLD_INCREMENT_MULTIPLE,\n             TEST_RESERVATION_THRESHOLD);\n \n     resourceManager = new MockRM(conf);"
    },
    {
        "commit_id": "f4a21d3abaa7c5a9f0a0d8417e81f7eaf3d1b29a",
        "commit_message": "HDFS-10795. Fix an error in ReaderStrategy#ByteBufferStrategy. Contributed by Sammi Chen",
        "commit_url": "https://github.com/apache/hadoop/commit/f4a21d3abaa7c5a9f0a0d8417e81f7eaf3d1b29a",
        "buggy_code": "int nRead = blockReader.read(readBuf.slice());",
        "fixed_code": "int nRead = blockReader.read(tmpBuf);",
        "patch": "@@ -181,7 +181,7 @@ public int readFromBlock(BlockReader blockReader,\n                            int length) throws IOException {\n     ByteBuffer tmpBuf = readBuf.duplicate();\n     tmpBuf.limit(tmpBuf.position() + length);\n-    int nRead = blockReader.read(readBuf.slice());\n+    int nRead = blockReader.read(tmpBuf);\n     // Only when data are read, update the position\n     if (nRead > 0) {\n       readBuf.position(readBuf.position() + nRead);"
    },
    {
        "commit_id": "1360bd2d545134b582e70f2add33a105710dc80b",
        "commit_message": "MAPREDUCE-6764. Teragen LOG initialization bug. Contributed by Yufei Gu.",
        "commit_url": "https://github.com/apache/hadoop/commit/1360bd2d545134b582e70f2add33a105710dc80b",
        "buggy_code": "private static final Log LOG = LogFactory.getLog(TeraSort.class);",
        "fixed_code": "private static final Log LOG = LogFactory.getLog(TeraGen.class);",
        "patch": "@@ -66,7 +66,7 @@\n  * <b>bin/hadoop jar hadoop-*-examples.jar teragen 10000000000 in-dir</b>\n  */\n public class TeraGen extends Configured implements Tool {\n-  private static final Log LOG = LogFactory.getLog(TeraSort.class);\n+  private static final Log LOG = LogFactory.getLog(TeraGen.class);\n \n   public static enum Counters {CHECKSUM}\n "
    },
    {
        "commit_id": "9f473cf903e586c556154abd56b3a3d820c6b028",
        "commit_message": "HDFS-10655. Fix path related byte array conversion bugs. (daryn)",
        "commit_url": "https://github.com/apache/hadoop/commit/9f473cf903e586c556154abd56b3a3d820c6b028",
        "buggy_code": "\"/user/testHome/really_big_name_0003_fail\", \"/user/testHome/\",",
        "fixed_code": "\"/user/testHome/really_big_name_0003_fail\", \"/user/testHome\",",
        "patch": "@@ -191,7 +191,7 @@ public void testParentDirectoryNameIsCorrect() throws Exception {\n       \"/user/testHome/FileNameLength\", PathComponentTooLongException.class);\n \n     renameCheckParentDirectory(\"/user/testHome/FileNameLength\",\n-      \"/user/testHome/really_big_name_0003_fail\", \"/user/testHome/\",\n+      \"/user/testHome/really_big_name_0003_fail\", \"/user/testHome\",\n       PathComponentTooLongException.class);\n \n   }"
    },
    {
        "commit_id": "bf6f4a3b980a07d0b268eeb984a649a362877734",
        "commit_message": "YARN-4366. Fix Lint Warnings in YARN Common (templedf via rkanter)",
        "commit_url": "https://github.com/apache/hadoop/commit/bf6f4a3b980a07d0b268eeb984a649a362877734",
        "buggy_code": "Method method = cls.getMethod(action, null);",
        "fixed_code": "Method method = cls.getMethod(action);",
        "patch": "@@ -97,7 +97,7 @@ private Dest addController(WebApp.HTTP httpMethod, String path,\n       // Note: this does not distinguish methods with the same signature\n       // but different return types.\n       // TODO: We may want to deal with methods that take parameters in the future\n-      Method method = cls.getMethod(action, null);\n+      Method method = cls.getMethod(action);\n       Dest dest = routes.get(path);\n       if (dest == null) {\n         method.setAccessible(true); // avoid any runtime checks"
    },
    {
        "commit_id": "1ff6833bbacf5c4eeaff5e70553ac083a691bb21",
        "commit_message": "YARN-5243. fix several rebase and other miscellaneous issues before merge. (Sangjin Lee via Varun Saxena)",
        "commit_url": "https://github.com/apache/hadoop/commit/1ff6833bbacf5c4eeaff5e70553ac083a691bb21",
        "buggy_code": "DSEvent.DS_APP_ATTEMPT_END, domainId, appSubmitterUgi);",
        "fixed_code": "DSEvent.DS_APP_ATTEMPT_START, domainId, appSubmitterUgi);",
        "patch": "@@ -631,7 +631,7 @@ public void run() throws YarnException, IOException, InterruptedException {\n             DSEvent.DS_APP_ATTEMPT_START);\n       } else {\n         publishApplicationAttemptEvent(timelineClient, appAttemptID.toString(),\n-          DSEvent.DS_APP_ATTEMPT_END, domainId, appSubmitterUgi);\n+            DSEvent.DS_APP_ATTEMPT_START, domainId, appSubmitterUgi);\n       }\n     }\n "
    },
    {
        "commit_id": "960af7d4717b8a8949d0b2e43949e7daab45aa88",
        "commit_message": "YARN-4409. Fix javadoc and checkstyle issues in timelineservice code (Varun Saxena via sjlee)",
        "commit_url": "https://github.com/apache/hadoop/commit/960af7d4717b8a8949d0b2e43949e7daab45aa88",
        "buggy_code": "protected Map<ApplicationId, String> registeredCollectors;",
        "fixed_code": "private Map<ApplicationId, String> registeredCollectors;",
        "patch": "@@ -483,7 +483,7 @@ public static class NMContext implements Context {\n     protected final ConcurrentMap<ContainerId, Container> containers =\n         new ConcurrentSkipListMap<ContainerId, Container>();\n \n-    protected Map<ApplicationId, String> registeredCollectors;\n+    private Map<ApplicationId, String> registeredCollectors;\n \n     protected final ConcurrentMap<ContainerId,\n         org.apache.hadoop.yarn.api.records.Container> increasedContainers ="
    },
    {
        "commit_id": "960af7d4717b8a8949d0b2e43949e7daab45aa88",
        "commit_message": "YARN-4409. Fix javadoc and checkstyle issues in timelineservice code (Varun Saxena via sjlee)",
        "commit_url": "https://github.com/apache/hadoop/commit/960af7d4717b8a8949d0b2e43949e7daab45aa88",
        "buggy_code": "public byte[] getBytes();",
        "fixed_code": "byte[] getBytes();",
        "patch": "@@ -29,6 +29,6 @@ public interface ColumnFamily<T> {\n    *\n    * @return a clone of the byte representation of the column family.\n    */\n-  public byte[] getBytes();\n+  byte[] getBytes();\n \n }\n\\ No newline at end of file"
    },
    {
        "commit_id": "960af7d4717b8a8949d0b2e43949e7daab45aa88",
        "commit_message": "YARN-4409. Fix javadoc and checkstyle issues in timelineservice code (Varun Saxena via sjlee)",
        "commit_url": "https://github.com/apache/hadoop/commit/960af7d4717b8a8949d0b2e43949e7daab45aa88",
        "buggy_code": "return table.getResultScanner(hbaseConf, conn, scan);",
        "fixed_code": "return getTable().getResultScanner(hbaseConf, conn, scan);",
        "patch": "@@ -111,7 +111,7 @@ protected ResultScanner getResults(Configuration hbaseConf,\n     // the scanner may still return more than the limit; therefore we need to\n     // read the right number as we iterate\n     scan.setFilter(new PageFilter(getFilters().getLimit()));\n-    return table.getResultScanner(hbaseConf, conn, scan);\n+    return getTable().getResultScanner(hbaseConf, conn, scan);\n   }\n \n   @Override"
    },
    {
        "commit_id": "22e7ae57715cedb1dcba736e357e8daaf5133e5c",
        "commit_message": "YARN-3792. Test case failures in TestDistributedShell and some issue fixes related to ATSV2 (Naganarasimha G R via sjlee)\n\n(cherry picked from commit 84f37f1c7eefec6d139cbf091c50d6c06f734323)",
        "commit_url": "https://github.com/apache/hadoop/commit/22e7ae57715cedb1dcba736e357e8daaf5133e5c",
        "buggy_code": "flowRunId = Long.valueOf(cliParser.getOptionValue(\"flow_run_id\"));",
        "fixed_code": "flowRunId = Long.parseLong(cliParser.getOptionValue(\"flow_run_id\"));",
        "patch": "@@ -497,7 +497,7 @@ public boolean init(String[] args) throws ParseException {\n     }\n     if (cliParser.hasOption(\"flow_run_id\")) {\n       try {\n-        flowRunId = Long.valueOf(cliParser.getOptionValue(\"flow_run_id\"));\n+        flowRunId = Long.parseLong(cliParser.getOptionValue(\"flow_run_id\"));\n       } catch (NumberFormatException e) {\n         throw new IllegalArgumentException(\n             \"Flow run is not a valid long value\", e);"
    },
    {
        "commit_id": "22e7ae57715cedb1dcba736e357e8daaf5133e5c",
        "commit_message": "YARN-3792. Test case failures in TestDistributedShell and some issue fixes related to ATSV2 (Naganarasimha G R via sjlee)\n\n(cherry picked from commit 84f37f1c7eefec6d139cbf091c50d6c06f734323)",
        "commit_url": "https://github.com/apache/hadoop/commit/22e7ae57715cedb1dcba736e357e8daaf5133e5c",
        "buggy_code": "switch (parts[0]) {",
        "fixed_code": "switch (parts[0].toUpperCase()) {",
        "patch": "@@ -56,7 +56,7 @@ public void postPut(ApplicationId appId, TimelineCollector collector) {\n       if (parts.length != 2 || parts[1].isEmpty()) {\n         continue;\n       }\n-      switch (parts[0]) {\n+      switch (parts[0].toUpperCase()) {\n         case TimelineUtils.FLOW_NAME_TAG_PREFIX:\n           collector.getTimelineEntityContext().setFlowName(parts[1]);\n           break;"
    },
    {
        "commit_id": "22e7ae57715cedb1dcba736e357e8daaf5133e5c",
        "commit_message": "YARN-3792. Test case failures in TestDistributedShell and some issue fixes related to ATSV2 (Naganarasimha G R via sjlee)\n\n(cherry picked from commit 84f37f1c7eefec6d139cbf091c50d6c06f734323)",
        "commit_url": "https://github.com/apache/hadoop/commit/22e7ae57715cedb1dcba736e357e8daaf5133e5c",
        "buggy_code": "LOG.info(\"the collector service for \" + appId + \" was removed\");",
        "fixed_code": "LOG.info(\"The collector service for \" + appId + \" was removed\");",
        "patch": "@@ -128,7 +128,7 @@ public boolean remove(ApplicationId appId) {\n       postRemove(appId, collector);\n       // stop the service to do clean up\n       collector.stop();\n-      LOG.info(\"the collector service for \" + appId + \" was removed\");\n+      LOG.info(\"The collector service for \" + appId + \" was removed\");\n     }\n     return collector != null;\n   }"
    },
    {
        "commit_id": "5b8e1c26d702e42b606265860c5e475970876aa5",
        "commit_message": "HDFS-10541. Diskbalancer: When no actions in plan, error message says \"Plan was generated more than 24 hours ago\". Contributed by Anu Engineer.",
        "commit_url": "https://github.com/apache/hadoop/commit/5b8e1c26d702e42b606265860c5e475970876aa5",
        "buggy_code": "if (plan != null) {",
        "fixed_code": "if (plan != null && plan.getVolumeSetPlans().size() > 0) {",
        "patch": "@@ -140,7 +140,7 @@ public void execute(CommandLine cmd) throws Exception {\n           .getBytes(StandardCharsets.UTF_8));\n     }\n \n-    if (plan != null) {\n+    if (plan != null && plan.getVolumeSetPlans().size() > 0) {\n       LOG.info(\"Writing plan to : {}\", getOutputPath());\n       try (FSDataOutputStream planStream = create(String.format(\n           DiskBalancer.PLAN_TEMPLATE,"
    },
    {
        "commit_id": "cb68e5b3bdb0079af867a9e49559827ecee03010",
        "commit_message": "HDFS-10540. Diskbalancer: The CLI error message for disk balancer is not enabled is not clear. Contributed by Anu Engineer.",
        "commit_url": "https://github.com/apache/hadoop/commit/cb68e5b3bdb0079af867a9e49559827ecee03010",
        "buggy_code": "LOG.error(\"Another Diskbalancer instance is running ? - Target \" +",
        "fixed_code": "LOG.debug(\"Another Diskbalancer instance is running ? - Target \" +",
        "patch": "@@ -171,7 +171,7 @@ protected void setOutputPath(String path) throws IOException {\n       diskBalancerLogs = new Path(path);\n     }\n     if (fs.exists(diskBalancerLogs)) {\n-      LOG.error(\"Another Diskbalancer instance is running ? - Target \" +\n+      LOG.debug(\"Another Diskbalancer instance is running ? - Target \" +\n           \"Directory already exists. {}\", diskBalancerLogs);\n       throw new IOException(\"Another DiskBalancer files already exist at the \" +\n           \"target location. \" + diskBalancerLogs.toString());"
    },
    {
        "commit_id": "709a814fe0153e86a37806796ea27c8252d9c6d1",
        "commit_message": "HDFS-10516. Fix bug when warming up EDEK cache of more than one encryption zone. Contributed by Xiao Chen.",
        "commit_url": "https://github.com/apache/hadoop/commit/709a814fe0153e86a37806796ea27c8252d9c6d1",
        "buggy_code": "ret[index] = entry.getValue().getKeyName();",
        "fixed_code": "ret[index++] = entry.getValue().getKeyName();",
        "patch": "@@ -416,7 +416,7 @@ String[] getKeyNames() {\n     int index = 0;\n     for (Map.Entry<Long, EncryptionZoneInt> entry : encryptionZones\n         .entrySet()) {\n-      ret[index] = entry.getValue().getKeyName();\n+      ret[index++] = entry.getValue().getKeyName();\n     }\n     return ret;\n   }"
    },
    {
        "commit_id": "db54670e83a84c1d7deff2c225725687cf9e5f14",
        "commit_message": "YARN-5165. Fix NoOvercommitPolicy to take advantage of RLE representation of plan. (Carlo Curino via asuresh)",
        "commit_url": "https://github.com/apache/hadoop/commit/db54670e83a84c1d7deff2c225725687cf9e5f14",
        "buggy_code": "Resource clusterCapacity = Resource.newInstance(100 * 1024, 10);",
        "fixed_code": "Resource clusterCapacity = Resource.newInstance(100 * 1024, 100);",
        "patch": "@@ -53,7 +53,7 @@ public class TestSimpleCapacityReplanner {\n   @Test\n   public void testReplanningPlanCapacityLoss() throws PlanningException {\n \n-    Resource clusterCapacity = Resource.newInstance(100 * 1024, 10);\n+    Resource clusterCapacity = Resource.newInstance(100 * 1024, 100);\n     Resource minAlloc = Resource.newInstance(1024, 1);\n     Resource maxAlloc = Resource.newInstance(1024 * 8, 8);\n "
    },
    {
        "commit_id": "5b41b288d01b0124664ddf6a3d6b545058bcfe6f",
        "commit_message": "YARN-5162. Fix Exceptions thrown during in registerAM call when Distributed Scheduling is Enabled (Hitesh Sharma via asuresh)",
        "commit_url": "https://github.com/apache/hadoop/commit/5b41b288d01b0124664ddf6a3d6b545058bcfe6f",
        "buggy_code": "if (!protocol.equals(ApplicationMasterProtocolPB.class)) {",
        "fixed_code": "if (!ApplicationMasterProtocolPB.class.isAssignableFrom(protocol)) {",
        "patch": "@@ -41,7 +41,7 @@ public KerberosInfo getKerberosInfo(Class<?> protocol, Configuration conf) {\n \n   @Override\n   public TokenInfo getTokenInfo(Class<?> protocol, Configuration conf) {\n-    if (!protocol.equals(ApplicationMasterProtocolPB.class)) {\n+    if (!ApplicationMasterProtocolPB.class.isAssignableFrom(protocol)) {\n       return null;\n     }\n     return new TokenInfo() {"
    },
    {
        "commit_id": "1597630681c784a3d59f5605b87e96197b8139d7",
        "commit_message": "YARN-5110. Fix OpportunisticContainerAllocator to insert complete HostAddress in issued ContainerTokenIds. (Konstantinos Karanasos via asuresh)",
        "commit_url": "https://github.com/apache/hadoop/commit/1597630681c784a3d59f5605b87e96197b8139d7",
        "buggy_code": "public abstract void setWaitQueueLength(int queueWaitTime);",
        "fixed_code": "public abstract void setWaitQueueLength(int waitQueueLength);",
        "patch": "@@ -41,5 +41,5 @@ public static QueuedContainersStatus newInstance() {\n \n   public abstract int getWaitQueueLength();\n \n-  public abstract void setWaitQueueLength(int queueWaitTime);\n+  public abstract void setWaitQueueLength(int waitQueueLength);\n }"
    },
    {
        "commit_id": "ccc93e78127c14bfd84179395b055c4061ea436a",
        "commit_message": "YARN-5075. Fix findbugs warnings in hadoop-yarn-common module. (asuresh)",
        "commit_url": "https://github.com/apache/hadoop/commit/ccc93e78127c14bfd84179395b055c4061ea436a",
        "buggy_code": "public QueuedContainersStatus getQueuedContainersStatus();",
        "fixed_code": "QueuedContainersStatus getQueuedContainersStatus();",
        "patch": "@@ -170,7 +170,7 @@ public void updateNodeHeartbeatResponseForContainersDecreasing(\n   \n   public List<Container> pullNewlyIncreasedContainers();\n \n-  public QueuedContainersStatus getQueuedContainersStatus();\n+  QueuedContainersStatus getQueuedContainersStatus();\n \n   long getUntrackedTimeStamp();\n "
    },
    {
        "commit_id": "27c4e90efce04e1b1302f668b5eb22412e00d033",
        "commit_message": "HADOOP-13028 add low level counter metrics for S3A; use in read performance tests. contributed by: stevel\npatch includes\nHADOOP-12844 Recover when S3A fails on IOException in read()\nHADOOP-13058 S3A FS fails during init against a read-only FS if multipart purge\nHADOOP-13047 S3a Forward seek in stream length to be configurable",
        "commit_url": "https://github.com/apache/hadoop/commit/27c4e90efce04e1b1302f668b5eb22412e00d033",
        "buggy_code": "MutableCounterLong(MetricsInfo info, long initValue) {",
        "fixed_code": "public MutableCounterLong(MetricsInfo info, long initValue) {",
        "patch": "@@ -34,7 +34,7 @@ public class MutableCounterLong extends MutableCounter {\n \n   private AtomicLong value = new AtomicLong();\n \n-  MutableCounterLong(MetricsInfo info, long initValue) {\n+  public MutableCounterLong(MetricsInfo info, long initValue) {\n     super(info);\n     this.value.set(initValue);\n   }"
    },
    {
        "commit_id": "b9e5a32fa14b727b44118ec7f43fb95de05a7c2c",
        "commit_message": "HDFS-10372. Fix for failing TestFsDatasetImpl#testCleanShutdownOfVolume. Contributed by Rushabh Shah.",
        "commit_url": "https://github.com/apache/hadoop/commit/b9e5a32fa14b727b44118ec7f43fb95de05a7c2c",
        "buggy_code": "GenericTestUtils.assertExceptionContains(info.toString(), ioe);",
        "fixed_code": "GenericTestUtils.assertExceptionContains(info.getXferAddr(), ioe);",
        "patch": "@@ -683,7 +683,7 @@ public void testCleanShutdownOfVolume() throws Exception {\n         Assert.fail(\"This is not a valid code path. \"\n             + \"out.close should have thrown an exception.\");\n       } catch (IOException ioe) {\n-        GenericTestUtils.assertExceptionContains(info.toString(), ioe);\n+        GenericTestUtils.assertExceptionContains(info.getXferAddr(), ioe);\n       }\n       finalizedDir.setWritable(true);\n       finalizedDir.setExecutable(true);"
    },
    {
        "commit_id": "ef0870ad038a1b72ea14ce550e34139b81eb901b",
        "commit_message": "HADOOP-12378. Fix findbugs warnings in hadoop-tools module. Contributed by Akira AJISAKA.",
        "commit_url": "https://github.com/apache/hadoop/commit/ef0870ad038a1b72ea14ce550e34139b81eb901b",
        "buggy_code": "protected final char flag = 'e';",
        "fixed_code": "protected static final char flag = 'e';",
        "patch": "@@ -19,6 +19,6 @@\n package org.apache.hadoop.ant.condition;\n \n public class DfsExists extends DfsBaseConditional {\n-  protected final char flag = 'e';\n+  protected static final char flag = 'e';\n   protected char getFlag() { return flag; }\n }"
    },
    {
        "commit_id": "ef0870ad038a1b72ea14ce550e34139b81eb901b",
        "commit_message": "HADOOP-12378. Fix findbugs warnings in hadoop-tools module. Contributed by Akira AJISAKA.",
        "commit_url": "https://github.com/apache/hadoop/commit/ef0870ad038a1b72ea14ce550e34139b81eb901b",
        "buggy_code": "protected final char flag = 'd';",
        "fixed_code": "protected static final char flag = 'd';",
        "patch": "@@ -19,6 +19,6 @@\n package org.apache.hadoop.ant.condition;\n \n public class DfsIsDir extends DfsBaseConditional {\n-  protected final char flag = 'd';\n+  protected static final char flag = 'd';\n   protected char getFlag() { return flag; }\n }"
    },
    {
        "commit_id": "ef0870ad038a1b72ea14ce550e34139b81eb901b",
        "commit_message": "HADOOP-12378. Fix findbugs warnings in hadoop-tools module. Contributed by Akira AJISAKA.",
        "commit_url": "https://github.com/apache/hadoop/commit/ef0870ad038a1b72ea14ce550e34139b81eb901b",
        "buggy_code": "protected final char flag = 'z';",
        "fixed_code": "protected static final char flag = 'z';",
        "patch": "@@ -19,6 +19,6 @@\n package org.apache.hadoop.ant.condition;\n \n public class DfsZeroLen extends DfsBaseConditional {\n-  protected final char flag = 'z';\n+  protected static final char flag = 'z';\n   protected char getFlag() { return flag; }\n }"
    },
    {
        "commit_id": "f16722d2ef31338a57a13e2c8d18c1c62d58bbaf",
        "commit_message": "YARN-4956. findbug issue on LevelDBCacheTimelineStore. (Zhiyuan Yang via gtcarrera9)",
        "commit_url": "https://github.com/apache/hadoop/commit/f16722d2ef31338a57a13e2c8d18c1c62d58bbaf",
        "buggy_code": "protected void serviceInit(Configuration conf) throws Exception {",
        "fixed_code": "protected synchronized void serviceInit(Configuration conf) throws Exception {",
        "patch": "@@ -78,7 +78,7 @@ public LevelDBCacheTimelineStore(String id) {\n   }\n \n   @Override\n-  protected void serviceInit(Configuration conf) throws Exception {\n+  protected synchronized void serviceInit(Configuration conf) throws Exception {\n     configuration = conf;\n     Options options = new Options();\n     options.createIfMissing(true);"
    },
    {
        "commit_id": "809226752dd109e16956038017dece16ada6ee0f",
        "commit_message": "HDFS-10286. Fix TestDFSAdmin#testNameNodeGetReconfigurableProperties. Contributed by Xiaobing Zhou.",
        "commit_url": "https://github.com/apache/hadoop/commit/809226752dd109e16956038017dece16ada6ee0f",
        "buggy_code": "assertEquals(4, outs.size());",
        "fixed_code": "assertEquals(5, outs.size());",
        "patch": "@@ -234,7 +234,7 @@ public void testNameNodeGetReconfigurableProperties() throws IOException {\n     final List<String> outs = Lists.newArrayList();\n     final List<String> errs = Lists.newArrayList();\n     getReconfigurableProperties(\"namenode\", address, outs, errs);\n-    assertEquals(4, outs.size());\n+    assertEquals(5, outs.size());\n     assertEquals(DFS_HEARTBEAT_INTERVAL_KEY, outs.get(1));\n     assertEquals(DFS_NAMENODE_HEARTBEAT_RECHECK_INTERVAL_KEY, outs.get(2));\n     assertEquals(errs.size(), 0);"
    },
    {
        "commit_id": "843ee8d59d8bacbca0d87ccf0790772e39d16138",
        "commit_message": "HADOOP-12994. Specify PositionedReadable, add contract tests, fix problems. Contributed by Steve Loughran.",
        "commit_url": "https://github.com/apache/hadoop/commit/843ee8d59d8bacbca0d87ccf0790772e39d16138",
        "buggy_code": "createFile(getFileSystem(), srcFile, false, block);",
        "fixed_code": "createFile(getFileSystem(), srcFile, true, block);",
        "patch": "@@ -53,7 +53,7 @@ public void setup() throws Exception {\n     target = new Path(testPath, \"target\");\n \n     byte[] block = dataset(TEST_FILE_LEN, 0, 255);\n-    createFile(getFileSystem(), srcFile, false, block);\n+    createFile(getFileSystem(), srcFile, true, block);\n     touch(getFileSystem(), zeroByteFile);\n   }\n "
    },
    {
        "commit_id": "54b2e78fd28c9def42bec7f0418833bad352686c",
        "commit_message": "HDFS-10253. Fix TestRefreshCallQueue failure (Contributed by Xiaoyu Yao)",
        "commit_url": "https://github.com/apache/hadoop/commit/54b2e78fd28c9def42bec7f0418833bad352686c",
        "buggy_code": "public MockCallQueue(int cap, String ns, Configuration conf) {",
        "fixed_code": "public MockCallQueue(int levels, int cap, String ns, Configuration conf) {",
        "patch": "@@ -92,7 +92,7 @@ public void tearDown() throws Exception {\n \n   @SuppressWarnings(\"serial\")\n   public static class MockCallQueue<E> extends LinkedBlockingQueue<E> {\n-    public MockCallQueue(int cap, String ns, Configuration conf) {\n+    public MockCallQueue(int levels, int cap, String ns, Configuration conf) {\n       super(cap);\n       mockQueueConstructions++;\n     }"
    },
    {
        "commit_id": "bffaa38a91b26920206a3350bf1cd60ec950aa74",
        "commit_message": "HDFS-9777. Fix typos in DFSAdmin command line and documentation.(Wei-Chiu Chuang via umamahesh)",
        "commit_url": "https://github.com/apache/hadoop/commit/bffaa38a91b26920206a3350bf1cd60ec950aa74",
        "buggy_code": "throw new IllegalArgumentException(\"Failed to covert \\\"\" + argv[1]",
        "fixed_code": "throw new IllegalArgumentException(\"Failed to convert \\\"\" + argv[1]",
        "patch": "@@ -372,7 +372,7 @@ static int run(DistributedFileSystem dfs, String[] argv, int idx) throws IOExcep\n       final RollingUpgradeAction action = RollingUpgradeAction.fromString(\n           argv.length >= 2? argv[1]: \"\");\n       if (action == null) {\n-        throw new IllegalArgumentException(\"Failed to covert \\\"\" + argv[1]\n+        throw new IllegalArgumentException(\"Failed to convert \\\"\" + argv[1]\n             +\"\\\" to \" + RollingUpgradeAction.class.getSimpleName());\n       }\n "
    },
    {
        "commit_id": "5d00067ca7f1460bbda1330e7329205a7b1ce019",
        "commit_message": "HADOOP-12771. Fix typo in JvmPauseMonitor#getNumGcWarnThreadholdExceeded. Contributed by Xiaobing Zhou.",
        "commit_url": "https://github.com/apache/hadoop/commit/5d00067ca7f1460bbda1330e7329205a7b1ce019",
        "buggy_code": "pauseMonitor.getNumGcWarnThreadholdExceeded());",
        "fixed_code": "pauseMonitor.getNumGcWarnThresholdExceeded());",
        "patch": "@@ -140,7 +140,7 @@ private void getGcUsage(MetricsRecordBuilder rb) {\n     \n     if (pauseMonitor != null) {\n       rb.addCounter(GcNumWarnThresholdExceeded,\n-          pauseMonitor.getNumGcWarnThreadholdExceeded());\n+          pauseMonitor.getNumGcWarnThresholdExceeded());\n       rb.addCounter(GcNumInfoThresholdExceeded,\n           pauseMonitor.getNumGcInfoThresholdExceeded());\n       rb.addCounter(GcTotalExtraSleepTime,"
    },
    {
        "commit_id": "5d00067ca7f1460bbda1330e7329205a7b1ce019",
        "commit_message": "HADOOP-12771. Fix typo in JvmPauseMonitor#getNumGcWarnThreadholdExceeded. Contributed by Xiaobing Zhou.",
        "commit_url": "https://github.com/apache/hadoop/commit/5d00067ca7f1460bbda1330e7329205a7b1ce019",
        "buggy_code": "public long getNumGcWarnThreadholdExceeded() {",
        "fixed_code": "public long getNumGcWarnThresholdExceeded() {",
        "patch": "@@ -106,7 +106,7 @@ public boolean isStarted() {\n     return monitorThread != null;\n   }\n \n-  public long getNumGcWarnThreadholdExceeded() {\n+  public long getNumGcWarnThresholdExceeded() {\n     return numGcWarnThresholdExceeded;\n   }\n   "
    },
    {
        "commit_id": "8171874dd198a6d10f48211f311595b222e6b930",
        "commit_message": "HADOOP-12755. Fix typo in defaultFS warning message.",
        "commit_url": "https://github.com/apache/hadoop/commit/8171874dd198a6d10f48211f311595b222e6b930",
        "buggy_code": "\"Warning: fs.defaultFs is not set when running \\\"%s\\\" command.%n\",",
        "fixed_code": "\"Warning: fs.defaultFS is not set when running \\\"%s\\\" command.%n\",",
        "patch": "@@ -112,7 +112,7 @@ protected void processRawArguments(LinkedList<String> args)\n           defaultFs == null || defaultFs.equals(FS_DEFAULT_NAME_DEFAULT);\n       if (missingDefaultFs) {\n         err.printf(\n-            \"Warning: fs.defaultFs is not set when running \\\"%s\\\" command.%n\",\n+            \"Warning: fs.defaultFS is not set when running \\\"%s\\\" command.%n\",\n             getCommandName());\n       }\n     }"
    },
    {
        "commit_id": "8171874dd198a6d10f48211f311595b222e6b930",
        "commit_message": "HADOOP-12755. Fix typo in defaultFS warning message.",
        "commit_url": "https://github.com/apache/hadoop/commit/8171874dd198a6d10f48211f311595b222e6b930",
        "buggy_code": "\"Warning: fs.defaultFs is not set when running \\\"ls\\\" command.\"));",
        "fixed_code": "\"Warning: fs.defaultFS is not set when running \\\"ls\\\" command.\"));",
        "patch": "@@ -1062,7 +1062,7 @@ private static void displayWarningOnLocalFileSystem(boolean shouldDisplay)\n     ls.err = err;\n     ls.run(\"file:///.\");\n     assertEquals(shouldDisplay, buf.toString().contains(\n-        \"Warning: fs.defaultFs is not set when running \\\"ls\\\" command.\"));\n+        \"Warning: fs.defaultFS is not set when running \\\"ls\\\" command.\"));\n   }\n \n   @Test"
    },
    {
        "commit_id": "5ff5f67332b527acaca7a69ac421930a02ca55b3",
        "commit_message": "YARN-4557. Fix improper Queues sorting in PartitionedQueueComparator when accessible-node-labels=*. (Naganarasimha G R via wangda)",
        "commit_url": "https://github.com/apache/hadoop/commit/5ff5f67332b527acaca7a69ac421930a02ca55b3",
        "buggy_code": "private static final Comparator COMPARATOR =",
        "fixed_code": "private static final Comparator<Priority> COMPARATOR =",
        "patch": "@@ -57,7 +57,7 @@\n public class AppSchedulingInfo {\n   \n   private static final Log LOG = LogFactory.getLog(AppSchedulingInfo.class);\n-  private static final Comparator COMPARATOR =\n+  private static final Comparator<Priority> COMPARATOR =\n       new org.apache.hadoop.yarn.server.resourcemanager.resource.Priority.Comparator();\n   private static final int EPOCH_BIT_SHIFT = 40;\n "
    },
    {
        "commit_id": "1708a4cd2377c56e2cb5738720b7eaf10baf13c8",
        "commit_message": "YARN-4611. Fix scheduler load simulator to support multi-layer network\nlocation. Contributed by Ming Ma.",
        "commit_url": "https://github.com/apache/hadoop/commit/1708a4cd2377c56e2cb5738720b7eaf10baf13c8",
        "buggy_code": "node1.init(\"rack1/node1\", GB * 10, 10, 0, 1000, rm);",
        "fixed_code": "node1.init(\"/rack1/node1\", GB * 10, 10, 0, 1000, rm);",
        "patch": "@@ -51,7 +51,7 @@ public void setup() {\n   public void testNMSimulator() throws Exception {\n     // Register one node\n     NMSimulator node1 = new NMSimulator();\n-    node1.init(\"rack1/node1\", GB * 10, 10, 0, 1000, rm);\n+    node1.init(\"/rack1/node1\", GB * 10, 10, 0, 1000, rm);\n     node1.middleStep();\n \n     int numClusterNodes = rm.getResourceScheduler().getNumClusterNodes();"
    },
    {
        "commit_id": "89d1fd5dac4bccf42d82686e146b02eb60d14736",
        "commit_message": "HADOOP-12356. Fix computing CPU usage statistics on Windows. (Inigo Goiri via wangda)",
        "commit_url": "https://github.com/apache/hadoop/commit/89d1fd5dac4bccf42d82686e146b02eb60d14736",
        "buggy_code": "public float getCpuUsage() {",
        "fixed_code": "public float getCpuUsagePercentage() {",
        "patch": "@@ -120,7 +120,7 @@ public long getCumulativeCpuTime() {\n \n   /** {@inheritDoc} */\n   @Override\n-  public float getCpuUsage() {\n+  public float getCpuUsagePercentage() {\n     return getConf().getFloat(CPU_USAGE, -1);\n   }\n "
    },
    {
        "commit_id": "89d1fd5dac4bccf42d82686e146b02eb60d14736",
        "commit_message": "HADOOP-12356. Fix computing CPU usage statistics on Windows. (Inigo Goiri via wangda)",
        "commit_url": "https://github.com/apache/hadoop/commit/89d1fd5dac4bccf42d82686e146b02eb60d14736",
        "buggy_code": "public float getCpuUsage() {",
        "fixed_code": "public float getCpuUsagePercentage() {",
        "patch": "@@ -63,7 +63,7 @@ public long getCumulativeCpuTime() {\n   }\n \n   @Override\n-  public float getCpuUsage() {\n+  public float getCpuUsagePercentage() {\n     return 0;\n   }\n }"
    },
    {
        "commit_id": "89d1fd5dac4bccf42d82686e146b02eb60d14736",
        "commit_message": "HADOOP-12356. Fix computing CPU usage statistics on Windows. (Inigo Goiri via wangda)",
        "commit_url": "https://github.com/apache/hadoop/commit/89d1fd5dac4bccf42d82686e146b02eb60d14736",
        "buggy_code": "public float getCpuUsage() {",
        "fixed_code": "public float getCpuUsagePercentage() {",
        "patch": "@@ -73,7 +73,7 @@ public long getCumulativeCpuTime() {\n     }\n \n     @Override\n-    public float getCpuUsage() {\n+    public float getCpuUsagePercentage() {\n       return 0;\n     }\n "
    },
    {
        "commit_id": "edc43a9097530fd469dee47d4fefd091818331e5",
        "commit_message": "YARN-4565. Fix a bug that leads to AM resource limit not hornored when sizeBasedWeight enabled for FairOrderingPolicy. Contributed by Wangda Tan",
        "commit_url": "https://github.com/apache/hadoop/commit/edc43a9097530fd469dee47d4fefd091818331e5",
        "buggy_code": "public synchronized ResourceUsage getSchedulingResourceUsage() {",
        "fixed_code": "public ResourceUsage getSchedulingResourceUsage() {",
        "patch": "@@ -863,7 +863,7 @@ public int compareInputOrderTo(SchedulableEntity other) {\n   }\n   \n   @Override\n-  public synchronized ResourceUsage getSchedulingResourceUsage() {\n+  public ResourceUsage getSchedulingResourceUsage() {\n     return attemptResourceUsage;\n   }\n   "
    },
    {
        "commit_id": "a44ce3f14fd940601f984fbf7980aa6fdc8f23b7",
        "commit_message": "YARN-4502. Fix two AM containers get allocated when AM restart. (Vinod Kumar Vavilapalli via wangda)",
        "commit_url": "https://github.com/apache/hadoop/commit/a44ce3f14fd940601f984fbf7980aa6fdc8f23b7",
        "buggy_code": "public synchronized void recoverResourceRequests(",
        "fixed_code": "public synchronized void recoverResourceRequestsForContainer(",
        "patch": "@@ -322,7 +322,7 @@ public synchronized boolean updateResourceRequests(\n     return false;\n   }\n   \n-  public synchronized void recoverResourceRequests(\n+  public synchronized void recoverResourceRequestsForContainer(\n       List<ResourceRequest> requests) {\n     if (!isStopped) {\n       appSchedulingInfo.updateResourceRequests(requests, true);"
    },
    {
        "commit_id": "a44ce3f14fd940601f984fbf7980aa6fdc8f23b7",
        "commit_message": "YARN-4502. Fix two AM containers get allocated when AM restart. (Vinod Kumar Vavilapalli via wangda)",
        "commit_url": "https://github.com/apache/hadoop/commit/a44ce3f14fd940601f984fbf7980aa6fdc8f23b7",
        "buggy_code": "public synchronized void addPreemptContainer(ContainerId cont) {",
        "fixed_code": "public synchronized void markContainerForPreemption(ContainerId cont) {",
        "patch": "@@ -301,7 +301,7 @@ public synchronized Resource getTotalPendingRequests() {\n     return ret;\n   }\n \n-  public synchronized void addPreemptContainer(ContainerId cont) {\n+  public synchronized void markContainerForPreemption(ContainerId cont) {\n     // ignore already completed containers\n     if (liveContainers.containsKey(cont)) {\n       containersToPreempt.add(cont);"
    },
    {
        "commit_id": "150f5ae0343e872ee8bef39c57008c1389f0ba9e",
        "commit_message": "Revert \"YARN-4502. Fix two AM containers get allocated when AM restart. (Vinod Kumar Vavilapalli via wangda)\"\n\nThis reverts commit 3fe57285635e8058c34aa40a103845b49ca7d6ff.\n\nConflicts:\n\thadoop-yarn-project/CHANGES.txt",
        "commit_url": "https://github.com/apache/hadoop/commit/150f5ae0343e872ee8bef39c57008c1389f0ba9e",
        "buggy_code": "public synchronized void recoverResourceRequestsForContainer(",
        "fixed_code": "public synchronized void recoverResourceRequests(",
        "patch": "@@ -322,7 +322,7 @@ public synchronized boolean updateResourceRequests(\n     return false;\n   }\n   \n-  public synchronized void recoverResourceRequestsForContainer(\n+  public synchronized void recoverResourceRequests(\n       List<ResourceRequest> requests) {\n     if (!isStopped) {\n       appSchedulingInfo.updateResourceRequests(requests, true);"
    },
    {
        "commit_id": "150f5ae0343e872ee8bef39c57008c1389f0ba9e",
        "commit_message": "Revert \"YARN-4502. Fix two AM containers get allocated when AM restart. (Vinod Kumar Vavilapalli via wangda)\"\n\nThis reverts commit 3fe57285635e8058c34aa40a103845b49ca7d6ff.\n\nConflicts:\n\thadoop-yarn-project/CHANGES.txt",
        "commit_url": "https://github.com/apache/hadoop/commit/150f5ae0343e872ee8bef39c57008c1389f0ba9e",
        "buggy_code": "public synchronized void preemptContainer(ContainerId cont) {",
        "fixed_code": "public synchronized void addPreemptContainer(ContainerId cont) {",
        "patch": "@@ -301,7 +301,7 @@ public synchronized Resource getTotalPendingRequests() {\n     return ret;\n   }\n \n-  public synchronized void preemptContainer(ContainerId cont) {\n+  public synchronized void addPreemptContainer(ContainerId cont) {\n     // ignore already completed containers\n     if (liveContainers.containsKey(cont)) {\n       containersToPreempt.add(cont);"
    },
    {
        "commit_id": "3fe57285635e8058c34aa40a103845b49ca7d6ff",
        "commit_message": "YARN-4502. Fix two AM containers get allocated when AM restart. (Vinod Kumar Vavilapalli via wangda)\n\n(cherry picked from commit 805a9ed85eb34c8125cfb7d26d07cdfac12b3579)",
        "commit_url": "https://github.com/apache/hadoop/commit/3fe57285635e8058c34aa40a103845b49ca7d6ff",
        "buggy_code": "public synchronized void recoverResourceRequests(",
        "fixed_code": "public synchronized void recoverResourceRequestsForContainer(",
        "patch": "@@ -322,7 +322,7 @@ public synchronized boolean updateResourceRequests(\n     return false;\n   }\n   \n-  public synchronized void recoverResourceRequests(\n+  public synchronized void recoverResourceRequestsForContainer(\n       List<ResourceRequest> requests) {\n     if (!isStopped) {\n       appSchedulingInfo.updateResourceRequests(requests, true);"
    },
    {
        "commit_id": "3fe57285635e8058c34aa40a103845b49ca7d6ff",
        "commit_message": "YARN-4502. Fix two AM containers get allocated when AM restart. (Vinod Kumar Vavilapalli via wangda)\n\n(cherry picked from commit 805a9ed85eb34c8125cfb7d26d07cdfac12b3579)",
        "commit_url": "https://github.com/apache/hadoop/commit/3fe57285635e8058c34aa40a103845b49ca7d6ff",
        "buggy_code": "public synchronized void addPreemptContainer(ContainerId cont) {",
        "fixed_code": "public synchronized void preemptContainer(ContainerId cont) {",
        "patch": "@@ -301,7 +301,7 @@ public synchronized Resource getTotalPendingRequests() {\n     return ret;\n   }\n \n-  public synchronized void addPreemptContainer(ContainerId cont) {\n+  public synchronized void preemptContainer(ContainerId cont) {\n     // ignore already completed containers\n     if (liveContainers.containsKey(cont)) {\n       containersToPreempt.add(cont);"
    },
    {
        "commit_id": "adf260a728df427eb729abe8fb9ad7248991ea54",
        "commit_message": "Revert \"YARN-4502. Fix two AM containers get allocated when AM restart. (Vinod Kumar Vavilapalli via wangda)\"\n\nThis reverts commit 805a9ed85eb34c8125cfb7d26d07cdfac12b3579.",
        "commit_url": "https://github.com/apache/hadoop/commit/adf260a728df427eb729abe8fb9ad7248991ea54",
        "buggy_code": "public synchronized void recoverResourceRequestsForContainer(",
        "fixed_code": "public synchronized void recoverResourceRequests(",
        "patch": "@@ -322,7 +322,7 @@ public synchronized boolean updateResourceRequests(\n     return false;\n   }\n   \n-  public synchronized void recoverResourceRequestsForContainer(\n+  public synchronized void recoverResourceRequests(\n       List<ResourceRequest> requests) {\n     if (!isStopped) {\n       appSchedulingInfo.updateResourceRequests(requests, true);"
    },
    {
        "commit_id": "adf260a728df427eb729abe8fb9ad7248991ea54",
        "commit_message": "Revert \"YARN-4502. Fix two AM containers get allocated when AM restart. (Vinod Kumar Vavilapalli via wangda)\"\n\nThis reverts commit 805a9ed85eb34c8125cfb7d26d07cdfac12b3579.",
        "commit_url": "https://github.com/apache/hadoop/commit/adf260a728df427eb729abe8fb9ad7248991ea54",
        "buggy_code": "public synchronized void preemptContainer(ContainerId cont) {",
        "fixed_code": "public synchronized void addPreemptContainer(ContainerId cont) {",
        "patch": "@@ -301,7 +301,7 @@ public synchronized Resource getTotalPendingRequests() {\n     return ret;\n   }\n \n-  public synchronized void preemptContainer(ContainerId cont) {\n+  public synchronized void addPreemptContainer(ContainerId cont) {\n     // ignore already completed containers\n     if (liveContainers.containsKey(cont)) {\n       containersToPreempt.add(cont);"
    },
    {
        "commit_id": "805a9ed85eb34c8125cfb7d26d07cdfac12b3579",
        "commit_message": "YARN-4502. Fix two AM containers get allocated when AM restart. (Vinod Kumar Vavilapalli via wangda)",
        "commit_url": "https://github.com/apache/hadoop/commit/805a9ed85eb34c8125cfb7d26d07cdfac12b3579",
        "buggy_code": "public synchronized void recoverResourceRequests(",
        "fixed_code": "public synchronized void recoverResourceRequestsForContainer(",
        "patch": "@@ -322,7 +322,7 @@ public synchronized boolean updateResourceRequests(\n     return false;\n   }\n   \n-  public synchronized void recoverResourceRequests(\n+  public synchronized void recoverResourceRequestsForContainer(\n       List<ResourceRequest> requests) {\n     if (!isStopped) {\n       appSchedulingInfo.updateResourceRequests(requests, true);"
    },
    {
        "commit_id": "805a9ed85eb34c8125cfb7d26d07cdfac12b3579",
        "commit_message": "YARN-4502. Fix two AM containers get allocated when AM restart. (Vinod Kumar Vavilapalli via wangda)",
        "commit_url": "https://github.com/apache/hadoop/commit/805a9ed85eb34c8125cfb7d26d07cdfac12b3579",
        "buggy_code": "public synchronized void addPreemptContainer(ContainerId cont) {",
        "fixed_code": "public synchronized void preemptContainer(ContainerId cont) {",
        "patch": "@@ -301,7 +301,7 @@ public synchronized Resource getTotalPendingRequests() {\n     return ret;\n   }\n \n-  public synchronized void addPreemptContainer(ContainerId cont) {\n+  public synchronized void preemptContainer(ContainerId cont) {\n     // ignore already completed containers\n     if (liveContainers.containsKey(cont)) {\n       containersToPreempt.add(cont);"
    },
    {
        "commit_id": "b9936689c9ea37bf0050e7970643bcddfc9cfdbe",
        "commit_message": "HDFS-9615. Fix variable name typo in DFSConfigKeys. (Contributed by Ray Chiang)",
        "commit_url": "https://github.com/apache/hadoop/commit/b9936689c9ea37bf0050e7970643bcddfc9cfdbe",
        "buggy_code": "public static final int     DFS_NAMENODE_MISSING_CHECKPOINT_PERIODS_BEFORE_SHUTDONW_DEFAULT = 3;",
        "fixed_code": "public static final int     DFS_NAMENODE_MISSING_CHECKPOINT_PERIODS_BEFORE_SHUTDOWN_DEFAULT = 3;",
        "patch": "@@ -176,7 +176,7 @@ public class DFSConfigKeys extends CommonConfigurationKeys {\n   public static final String  DFS_NAMENODE_CHECKPOINT_MAX_RETRIES_KEY = \"dfs.namenode.checkpoint.max-retries\";\n   public static final int     DFS_NAMENODE_CHECKPOINT_MAX_RETRIES_DEFAULT = 3;\n   public static final String  DFS_NAMENODE_MISSING_CHECKPOINT_PERIODS_BEFORE_SHUTDOWN_KEY = \"dfs.namenode.missing.checkpoint.periods.before.shutdown\";\n-  public static final int     DFS_NAMENODE_MISSING_CHECKPOINT_PERIODS_BEFORE_SHUTDONW_DEFAULT = 3;\n+  public static final int     DFS_NAMENODE_MISSING_CHECKPOINT_PERIODS_BEFORE_SHUTDOWN_DEFAULT = 3;\n   public static final String  DFS_NAMENODE_HEARTBEAT_RECHECK_INTERVAL_KEY =\n       HdfsClientConfigKeys.DeprecatedKeys.DFS_NAMENODE_HEARTBEAT_RECHECK_INTERVAL_KEY;\n   public static final int     DFS_NAMENODE_HEARTBEAT_RECHECK_INTERVAL_DEFAULT = 5*60*1000;"
    },
    {
        "commit_id": "b9936689c9ea37bf0050e7970643bcddfc9cfdbe",
        "commit_message": "HDFS-9615. Fix variable name typo in DFSConfigKeys. (Contributed by Ray Chiang)",
        "commit_url": "https://github.com/apache/hadoop/commit/b9936689c9ea37bf0050e7970643bcddfc9cfdbe",
        "buggy_code": "DFSConfigKeys.DFS_NAMENODE_MISSING_CHECKPOINT_PERIODS_BEFORE_SHUTDONW_DEFAULT);",
        "fixed_code": "DFSConfigKeys.DFS_NAMENODE_MISSING_CHECKPOINT_PERIODS_BEFORE_SHUTDOWN_DEFAULT);",
        "patch": "@@ -731,7 +731,7 @@ public int saveNamespace(String[] argv) throws IOException {\n           DFSConfigKeys.DFS_NAMENODE_CHECKPOINT_TXNS_DEFAULT);\n       final int toleratePeriodNum = dfsConf.getInt(\n           DFSConfigKeys.DFS_NAMENODE_MISSING_CHECKPOINT_PERIODS_BEFORE_SHUTDOWN_KEY,\n-          DFSConfigKeys.DFS_NAMENODE_MISSING_CHECKPOINT_PERIODS_BEFORE_SHUTDONW_DEFAULT);\n+          DFSConfigKeys.DFS_NAMENODE_MISSING_CHECKPOINT_PERIODS_BEFORE_SHUTDOWN_DEFAULT);\n       timeWindow = checkpointPeriod * toleratePeriodNum;\n       txGap = checkpointTxnCount * toleratePeriodNum;\n       System.out.println(\"Do checkpoint if necessary before stopping \" +"
    },
    {
        "commit_id": "9f77ccad735f4843ce2c38355de9f434838d4507",
        "commit_message": "YARN-3840. Resource Manager web ui issue when sorting application by id (with application having id > 9999). Contributed by Mohammad Shahid Khan and Varun Saxena",
        "commit_url": "https://github.com/apache/hadoop/commit/9f77ccad735f4843ce2c38355de9f434838d4507",
        "buggy_code": ".append(\"\\n, {'sType':'string', 'aTargets': [ 0 ]\")",
        "fixed_code": ".append(\"\\n, {'sType':'natural', 'aTargets': [ 0 ]\")",
        "patch": "@@ -221,7 +221,7 @@ private String attemptsTableInit() {\n     .append(\"\\n{'aTargets': [ 5 ]\")\n     .append(\", 'bSearchable': false }\")\n \n-    .append(\"\\n, {'sType':'string', 'aTargets': [ 0 ]\")\n+    .append(\"\\n, {'sType':'natural', 'aTargets': [ 0 ]\")\n     .append(\", 'mRender': parseHadoopID }\")\n \n     .append(\"\\n, {'sType':'numeric', 'aTargets': [ 6, 7\")"
    },
    {
        "commit_id": "9f77ccad735f4843ce2c38355de9f434838d4507",
        "commit_message": "YARN-3840. Resource Manager web ui issue when sorting application by id (with application having id > 9999). Contributed by Mohammad Shahid Khan and Varun Saxena",
        "commit_url": "https://github.com/apache/hadoop/commit/9f77ccad735f4843ce2c38355de9f434838d4507",
        "buggy_code": ".append(\"{'sType':'string', 'aTargets': [0]\")",
        "fixed_code": ".append(\"{'sType':'natural', 'aTargets': [0]\")",
        "patch": "@@ -43,7 +43,7 @@ private String tasksTableInit() {\n       .append(\", bProcessing: true\")\n \n       .append(\"\\n, aoColumnDefs: [\\n\")\n-      .append(\"{'sType':'string', 'aTargets': [0]\")\n+      .append(\"{'sType':'natural', 'aTargets': [0]\")\n       .append(\", 'mRender': parseHadoopID }\")\n \n       .append(\"\\n, {'sType':'numeric', bSearchable:false, 'aTargets': [1]\")"
    },
    {
        "commit_id": "9f77ccad735f4843ce2c38355de9f434838d4507",
        "commit_message": "YARN-3840. Resource Manager web ui issue when sorting application by id (with application having id > 9999). Contributed by Mohammad Shahid Khan and Varun Saxena",
        "commit_url": "https://github.com/apache/hadoop/commit/9f77ccad735f4843ce2c38355de9f434838d4507",
        "buggy_code": "return sb.append(\"[\\n\").append(\"{'sType':'string', 'aTargets': [0]\")",
        "fixed_code": "return sb.append(\"[\\n\").append(\"{'sType':'natural', 'aTargets': [0]\")",
        "patch": "@@ -53,7 +53,7 @@ protected Class<? extends SubView> content() {\n \n   protected String getContainersTableColumnDefs() {\n     StringBuilder sb = new StringBuilder();\n-    return sb.append(\"[\\n\").append(\"{'sType':'string', 'aTargets': [0]\")\n+    return sb.append(\"[\\n\").append(\"{'sType':'natural', 'aTargets': [0]\")\n       .append(\", 'mRender': parseHadoopID }]\").toString();\n   }\n "
    },
    {
        "commit_id": "9f77ccad735f4843ce2c38355de9f434838d4507",
        "commit_message": "YARN-3840. Resource Manager web ui issue when sorting application by id (with application having id > 9999). Contributed by Mohammad Shahid Khan and Varun Saxena",
        "commit_url": "https://github.com/apache/hadoop/commit/9f77ccad735f4843ce2c38355de9f434838d4507",
        "buggy_code": "return sb.append(\"[\\n\").append(\"{'sType':'string', 'aTargets': [0]\")",
        "fixed_code": "return sb.append(\"[\\n\").append(\"{'sType':'natural', 'aTargets': [0]\")",
        "patch": "@@ -55,7 +55,7 @@ protected Class<? extends SubView> content() {\n \n   protected String getAttemptsTableColumnDefs() {\n     StringBuilder sb = new StringBuilder();\n-    return sb.append(\"[\\n\").append(\"{'sType':'string', 'aTargets': [0]\")\n+    return sb.append(\"[\\n\").append(\"{'sType':'natural', 'aTargets': [0]\")\n       .append(\", 'mRender': parseHadoopID }\")\n \n       .append(\"\\n, {'sType':'numeric', 'aTargets': [1]\")"
    },
    {
        "commit_id": "485c3468a8520fcde14800af3e4a075231c946de",
        "commit_message": "HADOOP-12609. Fix intermittent failure of TestDecayRpcScheduler. (Contributed by Masatake Iwasaki)",
        "commit_url": "https://github.com/apache/hadoop/commit/485c3468a8520fcde14800af3e4a075231c946de",
        "buggy_code": "timer.scheduleAtFixedRate(task, 0, this.decayPeriodMillis);",
        "fixed_code": "timer.scheduleAtFixedRate(task, decayPeriodMillis, decayPeriodMillis);",
        "patch": "@@ -151,7 +151,7 @@ public DecayRpcScheduler(int numQueues, String ns, Configuration conf) {\n     // Setup delay timer\n     Timer timer = new Timer();\n     DecayTask task = new DecayTask(this, timer);\n-    timer.scheduleAtFixedRate(task, 0, this.decayPeriodMillis);\n+    timer.scheduleAtFixedRate(task, decayPeriodMillis, decayPeriodMillis);\n \n     MetricsProxy prox = MetricsProxy.getInstance(ns);\n     prox.setDelegate(this);"
    },
    {
        "commit_id": "28dfe721b86ccbaf2ddcfb7e709b226ac766803a",
        "commit_message": "YARN-4387. Fix typo in FairScheduler log message. Contributed by Xin Wang.",
        "commit_url": "https://github.com/apache/hadoop/commit/28dfe721b86ccbaf2ddcfb7e709b226ac766803a",
        "buggy_code": "\" (after waiting for premption for \" +",
        "fixed_code": "\" (after waiting for preemption for \" +",
        "patch": "@@ -500,7 +500,7 @@ protected void warnOrKillContainer(RMContainer container) {\n         // containers on the RMNode (see SchedulerNode.releaseContainer()).\n         completedContainer(container, status, RMContainerEventType.KILL);\n         LOG.info(\"Killing container\" + container +\n-            \" (after waiting for premption for \" +\n+            \" (after waiting for preemption for \" +\n             (getClock().getTime() - time) + \"ms)\");\n       }\n     } else {"
    },
    {
        "commit_id": "007c6ce1aff2f25a499d2213c240f5a519feb8bd",
        "commit_message": "HDFS-9397. Fix typo for readChecksum() LOG.warn in BlockSender.java. (Contributed by Enrique Flores)",
        "commit_url": "https://github.com/apache/hadoop/commit/007c6ce1aff2f25a499d2213c240f5a519feb8bd",
        "buggy_code": "LOG.warn(\" Could not read or failed to veirfy checksum for data\"",
        "fixed_code": "LOG.warn(\" Could not read or failed to verify checksum for data\"",
        "patch": "@@ -644,7 +644,7 @@ private void readChecksum(byte[] buf, final int checksumOffset,\n     try {\n       checksumIn.readFully(buf, checksumOffset, checksumLen);\n     } catch (IOException e) {\n-      LOG.warn(\" Could not read or failed to veirfy checksum for data\"\n+      LOG.warn(\" Could not read or failed to verify checksum for data\"\n           + \" at offset \" + offset + \" for block \" + block, e);\n       IOUtils.closeStream(checksumIn);\n       checksumIn = null;"
    },
    {
        "commit_id": "fcd7888029a8e07cb0b22d1f47f9e7df82c4a304",
        "commit_message": "Revert \"YARN-3840. Resource Manager web ui issue when sorting application by id (with application having id > 9999) Contributed by Mohammad Shahid Khan\"\n\nThis reverts commit 8fbea531d7f7b665f6f55af54c8ebf330118ff37.\n\nConflicts:\n\thadoop-yarn-project/CHANGES.txt",
        "commit_url": "https://github.com/apache/hadoop/commit/fcd7888029a8e07cb0b22d1f47f9e7df82c4a304",
        "buggy_code": ".append(\"\\n, {'sType':'natural', 'aTargets': [ 0 ]\")",
        "fixed_code": ".append(\"\\n, {'sType':'string', 'aTargets': [ 0 ]\")",
        "patch": "@@ -221,7 +221,7 @@ private String attemptsTableInit() {\n     .append(\"\\n{'aTargets': [ 5 ]\")\n     .append(\", 'bSearchable': false }\")\n \n-    .append(\"\\n, {'sType':'natural', 'aTargets': [ 0 ]\")\n+    .append(\"\\n, {'sType':'string', 'aTargets': [ 0 ]\")\n     .append(\", 'mRender': parseHadoopID }\")\n \n     .append(\"\\n, {'sType':'numeric', 'aTargets': [ 6, 7\")"
    },
    {
        "commit_id": "fcd7888029a8e07cb0b22d1f47f9e7df82c4a304",
        "commit_message": "Revert \"YARN-3840. Resource Manager web ui issue when sorting application by id (with application having id > 9999) Contributed by Mohammad Shahid Khan\"\n\nThis reverts commit 8fbea531d7f7b665f6f55af54c8ebf330118ff37.\n\nConflicts:\n\thadoop-yarn-project/CHANGES.txt",
        "commit_url": "https://github.com/apache/hadoop/commit/fcd7888029a8e07cb0b22d1f47f9e7df82c4a304",
        "buggy_code": ".append(\"{'sType':'natural', 'aTargets': [0]\")",
        "fixed_code": ".append(\"{'sType':'string', 'aTargets': [0]\")",
        "patch": "@@ -43,7 +43,7 @@ private String tasksTableInit() {\n       .append(\", bProcessing: true\")\n \n       .append(\"\\n, aoColumnDefs: [\\n\")\n-      .append(\"{'sType':'natural', 'aTargets': [0]\")\n+      .append(\"{'sType':'string', 'aTargets': [0]\")\n       .append(\", 'mRender': parseHadoopID }\")\n \n       .append(\"\\n, {'sType':'numeric', bSearchable:false, 'aTargets': [1]\")"
    },
    {
        "commit_id": "fcd7888029a8e07cb0b22d1f47f9e7df82c4a304",
        "commit_message": "Revert \"YARN-3840. Resource Manager web ui issue when sorting application by id (with application having id > 9999) Contributed by Mohammad Shahid Khan\"\n\nThis reverts commit 8fbea531d7f7b665f6f55af54c8ebf330118ff37.\n\nConflicts:\n\thadoop-yarn-project/CHANGES.txt",
        "commit_url": "https://github.com/apache/hadoop/commit/fcd7888029a8e07cb0b22d1f47f9e7df82c4a304",
        "buggy_code": "return sb.append(\"[\\n\").append(\"{'sType':'natural', 'aTargets': [0]\")",
        "fixed_code": "return sb.append(\"[\\n\").append(\"{'sType':'string', 'aTargets': [0]\")",
        "patch": "@@ -53,7 +53,7 @@ protected Class<? extends SubView> content() {\n \n   protected String getContainersTableColumnDefs() {\n     StringBuilder sb = new StringBuilder();\n-    return sb.append(\"[\\n\").append(\"{'sType':'natural', 'aTargets': [0]\")\n+    return sb.append(\"[\\n\").append(\"{'sType':'string', 'aTargets': [0]\")\n       .append(\", 'mRender': parseHadoopID }]\").toString();\n   }\n "
    },
    {
        "commit_id": "fcd7888029a8e07cb0b22d1f47f9e7df82c4a304",
        "commit_message": "Revert \"YARN-3840. Resource Manager web ui issue when sorting application by id (with application having id > 9999) Contributed by Mohammad Shahid Khan\"\n\nThis reverts commit 8fbea531d7f7b665f6f55af54c8ebf330118ff37.\n\nConflicts:\n\thadoop-yarn-project/CHANGES.txt",
        "commit_url": "https://github.com/apache/hadoop/commit/fcd7888029a8e07cb0b22d1f47f9e7df82c4a304",
        "buggy_code": "return sb.append(\"[\\n\").append(\"{'sType':'natural', 'aTargets': [0]\")",
        "fixed_code": "return sb.append(\"[\\n\").append(\"{'sType':'string', 'aTargets': [0]\")",
        "patch": "@@ -55,7 +55,7 @@ protected Class<? extends SubView> content() {\n \n   protected String getAttemptsTableColumnDefs() {\n     StringBuilder sb = new StringBuilder();\n-    return sb.append(\"[\\n\").append(\"{'sType':'natural', 'aTargets': [0]\")\n+    return sb.append(\"[\\n\").append(\"{'sType':'string', 'aTargets': [0]\")\n       .append(\", 'mRender': parseHadoopID }\")\n \n       .append(\"\\n, {'sType':'numeric', 'aTargets': [1]\")"
    },
    {
        "commit_id": "6351d3fa638f1d901279cef9e56dc4e07ef3de11",
        "commit_message": "YARN-4183. Reverting the patch to fix behaviour change.\nRevert \"YARN-4183. Enabling generic application history forces every job to get a timeline service delegation token (jeagles)\"\n\nThis reverts commit c293c58954cdab25c8c69418b0e839883b563fa4.",
        "commit_url": "https://github.com/apache/hadoop/commit/6351d3fa638f1d901279cef9e56dc4e07ef3de11",
        "buggy_code": "conf.setBoolean(YarnConfiguration.APPLICATION_HISTORY_ENABLED, true);",
        "fixed_code": "conf.setBoolean(YarnConfiguration.TIMELINE_SERVICE_ENABLED, true);",
        "patch": "@@ -73,7 +73,7 @@ public class TestSystemMetricsPublisher {\n   @BeforeClass\n   public static void setup() throws Exception {\n     YarnConfiguration conf = new YarnConfiguration();\n-    conf.setBoolean(YarnConfiguration.APPLICATION_HISTORY_ENABLED, true);\n+    conf.setBoolean(YarnConfiguration.TIMELINE_SERVICE_ENABLED, true);\n     conf.setBoolean(YarnConfiguration.RM_SYSTEM_METRICS_PUBLISHER_ENABLED, true);\n     conf.setClass(YarnConfiguration.TIMELINE_SERVICE_STORE,\n         MemoryTimelineStore.class, TimelineStore.class);"
    },
    {
        "commit_id": "8fbea531d7f7b665f6f55af54c8ebf330118ff37",
        "commit_message": "YARN-3840. Resource Manager web ui issue when sorting application by id (with application having id > 9999) Contributed by Mohammad Shahid Khan",
        "commit_url": "https://github.com/apache/hadoop/commit/8fbea531d7f7b665f6f55af54c8ebf330118ff37",
        "buggy_code": ".append(\"\\n, {'sType':'string', 'aTargets': [ 0 ]\")",
        "fixed_code": ".append(\"\\n, {'sType':'natural', 'aTargets': [ 0 ]\")",
        "patch": "@@ -221,7 +221,7 @@ private String attemptsTableInit() {\n     .append(\"\\n{'aTargets': [ 5 ]\")\n     .append(\", 'bSearchable': false }\")\n \n-    .append(\"\\n, {'sType':'string', 'aTargets': [ 0 ]\")\n+    .append(\"\\n, {'sType':'natural', 'aTargets': [ 0 ]\")\n     .append(\", 'mRender': parseHadoopID }\")\n \n     .append(\"\\n, {'sType':'numeric', 'aTargets': [ 6, 7\")"
    },
    {
        "commit_id": "8fbea531d7f7b665f6f55af54c8ebf330118ff37",
        "commit_message": "YARN-3840. Resource Manager web ui issue when sorting application by id (with application having id > 9999) Contributed by Mohammad Shahid Khan",
        "commit_url": "https://github.com/apache/hadoop/commit/8fbea531d7f7b665f6f55af54c8ebf330118ff37",
        "buggy_code": ".append(\"{'sType':'string', 'aTargets': [0]\")",
        "fixed_code": ".append(\"{'sType':'natural', 'aTargets': [0]\")",
        "patch": "@@ -43,7 +43,7 @@ private String tasksTableInit() {\n       .append(\", bProcessing: true\")\n \n       .append(\"\\n, aoColumnDefs: [\\n\")\n-      .append(\"{'sType':'string', 'aTargets': [0]\")\n+      .append(\"{'sType':'natural', 'aTargets': [0]\")\n       .append(\", 'mRender': parseHadoopID }\")\n \n       .append(\"\\n, {'sType':'numeric', bSearchable:false, 'aTargets': [1]\")"
    },
    {
        "commit_id": "8fbea531d7f7b665f6f55af54c8ebf330118ff37",
        "commit_message": "YARN-3840. Resource Manager web ui issue when sorting application by id (with application having id > 9999) Contributed by Mohammad Shahid Khan",
        "commit_url": "https://github.com/apache/hadoop/commit/8fbea531d7f7b665f6f55af54c8ebf330118ff37",
        "buggy_code": "return sb.append(\"[\\n\").append(\"{'sType':'string', 'aTargets': [0]\")",
        "fixed_code": "return sb.append(\"[\\n\").append(\"{'sType':'natural', 'aTargets': [0]\")",
        "patch": "@@ -53,7 +53,7 @@ protected Class<? extends SubView> content() {\n \n   protected String getContainersTableColumnDefs() {\n     StringBuilder sb = new StringBuilder();\n-    return sb.append(\"[\\n\").append(\"{'sType':'string', 'aTargets': [0]\")\n+    return sb.append(\"[\\n\").append(\"{'sType':'natural', 'aTargets': [0]\")\n       .append(\", 'mRender': parseHadoopID }]\").toString();\n   }\n "
    },
    {
        "commit_id": "8fbea531d7f7b665f6f55af54c8ebf330118ff37",
        "commit_message": "YARN-3840. Resource Manager web ui issue when sorting application by id (with application having id > 9999) Contributed by Mohammad Shahid Khan",
        "commit_url": "https://github.com/apache/hadoop/commit/8fbea531d7f7b665f6f55af54c8ebf330118ff37",
        "buggy_code": "return sb.append(\"[\\n\").append(\"{'sType':'string', 'aTargets': [0]\")",
        "fixed_code": "return sb.append(\"[\\n\").append(\"{'sType':'natural', 'aTargets': [0]\")",
        "patch": "@@ -55,7 +55,7 @@ protected Class<? extends SubView> content() {\n \n   protected String getAttemptsTableColumnDefs() {\n     StringBuilder sb = new StringBuilder();\n-    return sb.append(\"[\\n\").append(\"{'sType':'string', 'aTargets': [0]\")\n+    return sb.append(\"[\\n\").append(\"{'sType':'natural', 'aTargets': [0]\")\n       .append(\", 'mRender': parseHadoopID }\")\n \n       .append(\"\\n, {'sType':'numeric', 'aTargets': [1]\")"
    },
    {
        "commit_id": "ea6b183a1a649ad2874050ade8856286728c654c",
        "commit_message": "HADOOP-12457. [JDK8] Fix a failure of compiling common by javadoc. Contributed by Akira AJISAKA.",
        "commit_url": "https://github.com/apache/hadoop/commit/ea6b183a1a649ad2874050ade8856286728c654c",
        "buggy_code": "\"-safely: option requires safety confirmation\uff0cif enabled, \" +",
        "fixed_code": "\"-safely: option requires safety confirmation, if enabled, \" +",
        "patch": "@@ -65,7 +65,7 @@ public static class Rm extends FsCommand {\n             \"-[rR]:  Recursively deletes directories.\\n\" +\n             \"-skipTrash: option bypasses trash, if enabled, and immediately \" +\n             \"deletes <src>.\\n\" +\n-            \"-safely: option requires safety confirmation\uff0cif enabled, \" +\n+            \"-safely: option requires safety confirmation, if enabled, \" +\n             \"requires confirmation before deleting large directory with more \" +\n             \"than <hadoop.shell.delete.limit.num.files> files. Delay is \" +\n             \"expected when walking over large directory recursively to count \" +"
    },
    {
        "commit_id": "37bf6141f10d6f4be138c965ea08032420b01f56",
        "commit_message": "HDFS-9291. Fix TestInterDatanodeProtocol to be FsDataset-agnostic. (lei)",
        "commit_url": "https://github.com/apache/hadoop/commit/37bf6141f10d6f4be138c965ea08032420b01f56",
        "buggy_code": "FsDatasetImpl.checkReplicaFiles(replica);",
        "fixed_code": "cluster.getFsDatasetTestUtils(datanode).checkStoredReplica(replica);",
        "patch": "@@ -359,7 +359,7 @@ public void testUpdateReplicaUnderRecovery() throws IOException {\n       Assert.assertEquals(ReplicaState.RUR, replica.getState());\n \n       //check meta data before update\n-      FsDatasetImpl.checkReplicaFiles(replica);\n+      cluster.getFsDatasetTestUtils(datanode).checkStoredReplica(replica);\n \n       //case \"THIS IS NOT SUPPOSED TO HAPPEN\"\n       //with (block length) != (stored replica's on disk length). "
    },
    {
        "commit_id": "a24c6e84205c684ef864b0fc5301dc07b3578351",
        "commit_message": "HDFS-9278. Fix preferredBlockSize typo in OIV XML output. Contributed by Nicole Pazmany.",
        "commit_url": "https://github.com/apache/hadoop/commit/a24c6e84205c684ef864b0fc5301dc07b3578351",
        "buggy_code": ".o(\"perferredBlockSize\", f.getPreferredBlockSize())",
        "fixed_code": ".o(\"preferredBlockSize\", f.getPreferredBlockSize())",
        "patch": "@@ -242,7 +242,7 @@ private void dumpINodeReference(INodeReferenceSection.INodeReference r) {\n   private void dumpINodeFile(INodeSection.INodeFile f) {\n     o(\"replication\", f.getReplication()).o(\"mtime\", f.getModificationTime())\n         .o(\"atime\", f.getAccessTime())\n-        .o(\"perferredBlockSize\", f.getPreferredBlockSize())\n+        .o(\"preferredBlockSize\", f.getPreferredBlockSize())\n         .o(\"permission\", dumpPermission(f.getPermission()));\n     dumpAcls(f.getAcl());\n     if (f.getBlocksCount() > 0) {"
    },
    {
        "commit_id": "d286032b715192ddbdd770b07d623fdc396810e2",
        "commit_message": "HDFS-9187. Fix null pointer error in Globber when FS was not constructed via FileSystem#createFileSystem (cmccabe)",
        "commit_url": "https://github.com/apache/hadoop/commit/d286032b715192ddbdd770b07d623fdc396810e2",
        "buggy_code": "this.tracer = fs.getTracer();",
        "fixed_code": "this.tracer = FsTracer.get(fs.getConf());",
        "patch": "@@ -47,7 +47,7 @@ public Globber(FileSystem fs, Path pathPattern, PathFilter filter) {\n     this.fc = null;\n     this.pathPattern = pathPattern;\n     this.filter = filter;\n-    this.tracer = fs.getTracer();\n+    this.tracer = FsTracer.get(fs.getConf());\n   }\n \n   public Globber(FileContext fc, Path pathPattern, PathFilter filter) {"
    },
    {
        "commit_id": "239d119c6707e58c9a5e0099c6d65fe956e95140",
        "commit_message": "HDFS-9196. Fix TestWebHdfsContentLength. Contributed by Masatake Iwasaki.",
        "commit_url": "https://github.com/apache/hadoop/commit/239d119c6707e58c9a5e0099c6d65fe956e95140",
        "buggy_code": "this.workingDir = makeQualified(getHomeDirectory());",
        "fixed_code": "this.workingDir = makeQualified(new Path(getHomeDirectoryString(ugi)));",
        "patch": "@@ -212,7 +212,7 @@ public synchronized void initialize(URI uri, Configuration conf\n               failoverSleepMaxMillis);\n     }\n \n-    this.workingDir = makeQualified(getHomeDirectory());\n+    this.workingDir = makeQualified(new Path(getHomeDirectoryString(ugi)));\n     this.canRefreshDelegationToken = UserGroupInformation.isSecurityEnabled();\n     this.disallowFallbackToInsecureCluster = !conf.getBoolean(\n         CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_KEY,"
    },
    {
        "commit_id": "c6cafc77e697317dad0708309b67b900a2e3a413",
        "commit_message": "HDFS-9185. Fix null tracer in ErasureCodingWorker. Contributed by Rakesh R.",
        "commit_url": "https://github.com/apache/hadoop/commit/c6cafc77e697317dad0708309b67b900a2e3a413",
        "buggy_code": "TraceScope scope = datanode.tracer.",
        "fixed_code": "final TraceScope scope = datanode.getTracer().",
        "patch": "@@ -707,7 +707,7 @@ public void verifyChecksum(final byte[] buf, final int dataOffset,\n    */\n   long sendBlock(DataOutputStream out, OutputStream baseStream, \n                  DataTransferThrottler throttler) throws IOException {\n-    TraceScope scope = datanode.tracer.\n+    final TraceScope scope = datanode.getTracer().\n         newScope(\"sendBlock_\" + block.getBlockId());\n     try {\n       return doSendBlock(out, baseStream, throttler);"
    },
    {
        "commit_id": "c6cafc77e697317dad0708309b67b900a2e3a413",
        "commit_message": "HDFS-9185. Fix null tracer in ErasureCodingWorker. Contributed by Rakesh R.",
        "commit_url": "https://github.com/apache/hadoop/commit/c6cafc77e697317dad0708309b67b900a2e3a413",
        "buggy_code": "super(datanode.tracer);",
        "fixed_code": "super(datanode.getTracer());",
        "patch": "@@ -126,7 +126,7 @@ public static DataXceiver create(Peer peer, DataNode dn,\n   \n   private DataXceiver(Peer peer, DataNode datanode,\n       DataXceiverServer dataXceiverServer) throws IOException {\n-    super(datanode.tracer);\n+    super(datanode.getTracer());\n     this.peer = peer;\n     this.dnConf = datanode.getDnConf();\n     this.socketIn = peer.getInputStream();"
    },
    {
        "commit_id": "195793c6f3e53a5c0527020477fe9c9158576f77",
        "commit_message": "MAPREDUCE-6497. Fix wrong value of JOB_FINISHED event in JobHistoryEventHandler. Contributed by Shinichi Yamashita.",
        "commit_url": "https://github.com/apache/hadoop/commit/195793c6f3e53a5c0527020477fe9c9158576f77",
        "buggy_code": "countersToJSON(jfe.getTotalCounters()));",
        "fixed_code": "countersToJSON(jfe.getMapCounters()));",
        "patch": "@@ -825,7 +825,7 @@ private void processEventForTimelineServer(HistoryEvent event, JobId jobId,\n         tEvent.addEventInfo(\"FINISHED_MAPS\", jfe.getFinishedMaps());\n         tEvent.addEventInfo(\"FINISHED_REDUCES\", jfe.getFinishedReduces());\n         tEvent.addEventInfo(\"MAP_COUNTERS_GROUPS\",\n-                countersToJSON(jfe.getTotalCounters()));\n+                countersToJSON(jfe.getMapCounters()));\n         tEvent.addEventInfo(\"REDUCE_COUNTERS_GROUPS\",\n                 countersToJSON(jfe.getReduceCounters()));\n         tEvent.addEventInfo(\"TOTAL_COUNTERS_GROUPS\","
    },
    {
        "commit_id": "dfd807afab0fae3839c9cc5d552aa0304444f956",
        "commit_message": "HADOOP-12428. Fix inconsistency between log-level guards and statements. Contributed by Jagadesh Kiran N and Jackie Chang.",
        "commit_url": "https://github.com/apache/hadoop/commit/dfd807afab0fae3839c9cc5d552aa0304444f956",
        "buggy_code": "LOG.debug(\"data:\" + StringUtils.byteToHexString(data));",
        "fixed_code": "LOG.trace(\"data:\" + StringUtils.byteToHexString(data));",
        "patch": "@@ -200,7 +200,7 @@ private synchronized void applyEdits(long firstTxId, int numTxns, byte[] data)\n     assert backupInputStream.length() == 0 : \"backup input stream is not empty\";\n     try {\n       if (LOG.isTraceEnabled()) {\n-        LOG.debug(\"data:\" + StringUtils.byteToHexString(data));\n+        LOG.trace(\"data:\" + StringUtils.byteToHexString(data));\n       }\n \n       FSEditLogLoader logLoader ="
    },
    {
        "commit_id": "dfd807afab0fae3839c9cc5d552aa0304444f956",
        "commit_message": "HADOOP-12428. Fix inconsistency between log-level guards and statements. Contributed by Jagadesh Kiran N and Jackie Chang.",
        "commit_url": "https://github.com/apache/hadoop/commit/dfd807afab0fae3839c9cc5d552aa0304444f956",
        "buggy_code": "LOG.fatal(msg, e);",
        "fixed_code": "LOG.debug(msg, e);",
        "patch": "@@ -375,7 +375,7 @@ private boolean checkLogsAvailableForRead(FSImage image, long imageTxId,\n           \"or call saveNamespace on the active node.\\n\" +\n           \"Error: \" + e.getLocalizedMessage();\n       if (LOG.isDebugEnabled()) {\n-        LOG.fatal(msg, e);\n+        LOG.debug(msg, e);\n       } else {\n         LOG.fatal(msg);\n       }"
    },
    {
        "commit_id": "dfd807afab0fae3839c9cc5d552aa0304444f956",
        "commit_message": "HADOOP-12428. Fix inconsistency between log-level guards and statements. Contributed by Jagadesh Kiran N and Jackie Chang.",
        "commit_url": "https://github.com/apache/hadoop/commit/dfd807afab0fae3839c9cc5d552aa0304444f956",
        "buggy_code": "LOG.info(String.format(\"Loaded %d edits starting from txid %d \",",
        "fixed_code": "LOG.debug(String.format(\"Loaded %d edits starting from txid %d \",",
        "patch": "@@ -260,7 +260,7 @@ void doTailEdits() throws IOException, InterruptedException {\n         throw elie;\n       } finally {\n         if (editsLoaded > 0 || LOG.isDebugEnabled()) {\n-          LOG.info(String.format(\"Loaded %d edits starting from txid %d \",\n+          LOG.debug(String.format(\"Loaded %d edits starting from txid %d \",\n               editsLoaded, lastTxnId));\n         }\n       }"
    },
    {
        "commit_id": "dfd807afab0fae3839c9cc5d552aa0304444f956",
        "commit_message": "HADOOP-12428. Fix inconsistency between log-level guards and statements. Contributed by Jagadesh Kiran N and Jackie Chang.",
        "commit_url": "https://github.com/apache/hadoop/commit/dfd807afab0fae3839c9cc5d552aa0304444f956",
        "buggy_code": "LOG.info(\"Assigned container (\" + allocated + \") \"",
        "fixed_code": "LOG.debug(\"Assigned container (\" + allocated + \") \"",
        "patch": "@@ -1122,7 +1122,7 @@ private void containerAssigned(Container allocated,\n       assignedRequests.add(allocated, assigned.attemptID);\n \n       if (LOG.isDebugEnabled()) {\n-        LOG.info(\"Assigned container (\" + allocated + \") \"\n+        LOG.debug(\"Assigned container (\" + allocated + \") \"\n             + \" to task \" + assigned.attemptID + \" on node \"\n             + allocated.getNodeId().toString());\n       }"
    },
    {
        "commit_id": "dfd807afab0fae3839c9cc5d552aa0304444f956",
        "commit_message": "HADOOP-12428. Fix inconsistency between log-level guards and statements. Contributed by Jagadesh Kiran N and Jackie Chang.",
        "commit_url": "https://github.com/apache/hadoop/commit/dfd807afab0fae3839c9cc5d552aa0304444f956",
        "buggy_code": "LOG.info(\"AFTER decResourceRequest:\" + \" applicationId=\"",
        "fixed_code": "LOG.debug(\"AFTER decResourceRequest:\" + \" applicationId=\"",
        "patch": "@@ -506,7 +506,7 @@ private void decResourceRequest(Priority priority, String resourceName,\n     addResourceRequestToAsk(remoteRequest);\n \n     if (LOG.isDebugEnabled()) {\n-      LOG.info(\"AFTER decResourceRequest:\" + \" applicationId=\"\n+      LOG.debug(\"AFTER decResourceRequest:\" + \" applicationId=\"\n           + applicationId.getId() + \" priority=\" + priority.getPriority()\n           + \" resourceName=\" + resourceName + \" numContainers=\"\n           + remoteRequest.getNumContainers() + \" #asks=\" + ask.size());"
    },
    {
        "commit_id": "dfd807afab0fae3839c9cc5d552aa0304444f956",
        "commit_message": "HADOOP-12428. Fix inconsistency between log-level guards and statements. Contributed by Jagadesh Kiran N and Jackie Chang.",
        "commit_url": "https://github.com/apache/hadoop/commit/dfd807afab0fae3839c9cc5d552aa0304444f956",
        "buggy_code": "LOG.info(\" job \" + job.getName() + \" completed \");",
        "fixed_code": "LOG.debug(\" job \" + job.getName() + \" completed \");",
        "patch": "@@ -130,7 +130,7 @@ public void run() {\n                   return;\n                 }\n                 if (LOG.isDebugEnabled()) {\n-                  LOG.info(\" job \" + job.getName() + \" completed \");\n+                  LOG.debug(\" job \" + job.getName() + \" completed \");\n                 }\n                 break;\n               }"
    },
    {
        "commit_id": "dfd807afab0fae3839c9cc5d552aa0304444f956",
        "commit_message": "HADOOP-12428. Fix inconsistency between log-level guards and statements. Contributed by Jagadesh Kiran N and Jackie Chang.",
        "commit_url": "https://github.com/apache/hadoop/commit/dfd807afab0fae3839c9cc5d552aa0304444f956",
        "buggy_code": "LOG.info(\"AFTER decResourceRequest:\" + \" applicationId=\"",
        "fixed_code": "LOG.debug(\"AFTER decResourceRequest:\" + \" applicationId=\"",
        "patch": "@@ -748,7 +748,7 @@ private void decResourceRequest(Priority priority,\n     }\n \n     if (LOG.isDebugEnabled()) {\n-      LOG.info(\"AFTER decResourceRequest:\" + \" applicationId=\"\n+      LOG.debug(\"AFTER decResourceRequest:\" + \" applicationId=\"\n           + \" priority=\" + priority.getPriority()\n           + \" resourceName=\" + resourceName + \" numContainers=\"\n           + resourceRequestInfo.remoteRequest.getNumContainers() "
    },
    {
        "commit_id": "dfd807afab0fae3839c9cc5d552aa0304444f956",
        "commit_message": "HADOOP-12428. Fix inconsistency between log-level guards and statements. Contributed by Jagadesh Kiran N and Jackie Chang.",
        "commit_url": "https://github.com/apache/hadoop/commit/dfd807afab0fae3839c9cc5d552aa0304444f956",
        "buggy_code": "LOG.info(\"Looking for service: \" + service + \". Current token is \"",
        "fixed_code": "LOG.debug(\"Looking for service: \" + service + \". Current token is \"",
        "patch": "@@ -46,7 +46,7 @@ public Token<ContainerTokenIdentifier> selectToken(Text service,\n     }\n     for (Token<? extends TokenIdentifier> token : tokens) {\n       if (LOG.isDebugEnabled()) {\n-        LOG.info(\"Looking for service: \" + service + \". Current token is \"\n+        LOG.debug(\"Looking for service: \" + service + \". Current token is \"\n             + token);\n       }\n       if (ContainerTokenIdentifier.KIND.equals(token.getKind()) && "
    },
    {
        "commit_id": "dfd807afab0fae3839c9cc5d552aa0304444f956",
        "commit_message": "HADOOP-12428. Fix inconsistency between log-level guards and statements. Contributed by Jagadesh Kiran N and Jackie Chang.",
        "commit_url": "https://github.com/apache/hadoop/commit/dfd807afab0fae3839c9cc5d552aa0304444f956",
        "buggy_code": "LOG.info(\"Looking for service: \" + service + \". Current token is \"",
        "fixed_code": "LOG.debug(\"Looking for service: \" + service + \". Current token is \"",
        "patch": "@@ -42,7 +42,7 @@ public Token<NMTokenIdentifier> selectToken(Text service,\n     }\n     for (Token<? extends TokenIdentifier> token : tokens) {\n       if (LOG.isDebugEnabled()) {\n-        LOG.info(\"Looking for service: \" + service + \". Current token is \"\n+        LOG.debug(\"Looking for service: \" + service + \". Current token is \"\n             + token);\n       }\n       if (NMTokenIdentifier.KIND.equals(token.getKind()) && "
    },
    {
        "commit_id": "c457095206e5093c577b0124ad6fc7eef6f77b0a",
        "commit_message": "HDFS-8550. Erasure Coding: Fix FindBugs Multithreaded correctness Warning. Contributed by Rakesh R.\n\nChange-Id: Ic248999a7f8e5e740d49c9b10abcf16f66dd0f98",
        "commit_url": "https://github.com/apache/hadoop/commit/c457095206e5093c577b0124ad6fc7eef6f77b0a",
        "buggy_code": "int bufOffset = (int) (rangeStartInBlockGroup % (cellSize * dataBlkNum));",
        "fixed_code": "int bufOffset = (int) (rangeStartInBlockGroup % ((long) cellSize * dataBlkNum));",
        "patch": "@@ -372,7 +372,7 @@ public static AlignedStripe[] divideOneStripe(ErasureCodingPolicy ecPolicy,\n \n     // Step 4: calculate each chunk's position in destination buffer. Since the\n     // whole read range is within a single stripe, the logic is simpler here.\n-    int bufOffset = (int) (rangeStartInBlockGroup % (cellSize * dataBlkNum));\n+    int bufOffset = (int) (rangeStartInBlockGroup % ((long) cellSize * dataBlkNum));\n     for (StripingCell cell : cells) {\n       long cellStart = cell.idxInInternalBlk * cellSize + cell.offset;\n       long cellEnd = cellStart + cell.size - 1;"
    },
    {
        "commit_id": "e1b1d7e4aebfed0dec4d7df21561ab37f73ef1d7",
        "commit_message": "YARN-4126. RM should not issue delegation tokens in unsecure mode. Contributed by Bibin A Chundatt",
        "commit_url": "https://github.com/apache/hadoop/commit/e1b1d7e4aebfed0dec4d7df21561ab37f73ef1d7",
        "buggy_code": "return true;",
        "fixed_code": "return false;",
        "patch": "@@ -1100,7 +1100,7 @@ private boolean isAllowedDelegationTokenOp() throws IOException {\n           .contains(UserGroupInformation.getCurrentUser()\n                   .getRealAuthenticationMethod());\n     } else {\n-      return true;\n+      return false;\n     }\n   }\n "
    },
    {
        "commit_id": "d9c1fab2ec2930a011b7cca4a393881d39b8f6ec",
        "commit_message": "HADOOP-12388. Fix components' version information in the web page About the Cluster. Contributed by Jun Gong.",
        "commit_url": "https://github.com/apache/hadoop/commit/d9c1fab2ec2930a011b7cca4a393881d39b8f6ec",
        "buggy_code": "return getVersion() +",
        "fixed_code": "return _getVersion() +",
        "patch": "@@ -86,7 +86,7 @@ protected String _getSrcChecksum() {\n   }\n \n   protected String _getBuildVersion(){\n-    return getVersion() +\n+    return _getVersion() +\n       \" from \" + _getRevision() +\n       \" by \" + _getUser() +\n       \" source checksum \" + _getSrcChecksum();"
    },
    {
        "commit_id": "9b78e6e33d8c117c1e909df414f20d9db56efe4b",
        "commit_message": "YARN-4087. Followup fixes after YARN-2019 regarding RM behavior when\nstate-store error occurs. Contributed by Jian He",
        "commit_url": "https://github.com/apache/hadoop/commit/9b78e6e33d8c117c1e909df414f20d9db56efe4b",
        "buggy_code": "public static final boolean DEFAULT_YARN_FAIL_FAST = true;",
        "fixed_code": "public static final boolean DEFAULT_YARN_FAIL_FAST = false;",
        "patch": "@@ -402,7 +402,7 @@ private static void addDeprecatedKeys() {\n   public static final boolean DEFAULT_RM_RECOVERY_ENABLED = false;\n \n   public static final String YARN_FAIL_FAST = YARN_PREFIX + \"fail-fast\";\n-  public static final boolean DEFAULT_YARN_FAIL_FAST = true;\n+  public static final boolean DEFAULT_YARN_FAIL_FAST = false;\n \n   public static final String RM_FAIL_FAST = RM_PREFIX + \"fail-fast\";\n "
    },
    {
        "commit_id": "9b685773ec1031e90cc2b8aedb42670721757e22",
        "commit_message": "MAPREDUCE-6442. Stack trace is missing when error occurs in client protocol provider's constructor Contributed by Chang Li.",
        "commit_url": "https://github.com/apache/hadoop/commit/9b685773ec1031e90cc2b8aedb42670721757e22",
        "buggy_code": "+ \" due to error: \" + e.getMessage());",
        "fixed_code": "+ \" due to error: \", e);",
        "patch": "@@ -111,7 +111,7 @@ private void initialize(InetSocketAddress jobTrackAddr, Configuration conf)\n         } \n         catch (Exception e) {\n           LOG.info(\"Failed to use \" + provider.getClass().getName()\n-              + \" due to error: \" + e.getMessage());\n+              + \" due to error: \", e);\n         }\n       }\n     }"
    },
    {
        "commit_id": "355eaaa33d01f06e9efe960b8888fb925e03ffb9",
        "commit_message": "HADOOP-10318. Incorrect reference to nodeFile in RumenToSLSConverter error message. Contributed by Wei Yan.",
        "commit_url": "https://github.com/apache/hadoop/commit/355eaaa33d01f06e9efe960b8888fb925e03ffb9",
        "buggy_code": "+ jsonFile.getParentFile().getAbsoluteFile());",
        "fixed_code": "+ nodeFile.getParentFile().getAbsoluteFile());",
        "patch": "@@ -110,7 +110,7 @@ public static void main(String args[]) throws Exception {\n     if (! nodeFile.getParentFile().exists()\n             && ! nodeFile.getParentFile().mkdirs()) {\n       System.err.println(\"ERROR: Cannot create output directory in path: \"\n-              + jsonFile.getParentFile().getAbsoluteFile());\n+              + nodeFile.getParentFile().getAbsoluteFile());\n       System.exit(1);\n     }\n "
    },
    {
        "commit_id": "6f0a35724f0da80146dbae4b6f6c341e1d3101f5",
        "commit_message": "HADOOP-10945. 4-digit octal umask permissions throws a parse error. Contributed by Chang Li",
        "commit_url": "https://github.com/apache/hadoop/commit/6f0a35724f0da80146dbae4b6f6c341e1d3101f5",
        "buggy_code": "Pattern.compile(\"^\\\\s*[+]?()([0-7]{3})\\\\s*$\"); // no leading 1 for sticky bit",
        "fixed_code": "Pattern.compile(\"^\\\\s*[+]?(0*)([0-7]{3})\\\\s*$\"); // no leading 1 for sticky bit",
        "patch": "@@ -33,7 +33,7 @@\n @InterfaceStability.Unstable\n class UmaskParser extends PermissionParser {\n   private static Pattern chmodOctalPattern =\n-    Pattern.compile(\"^\\\\s*[+]?()([0-7]{3})\\\\s*$\"); // no leading 1 for sticky bit\n+    Pattern.compile(\"^\\\\s*[+]?(0*)([0-7]{3})\\\\s*$\"); // no leading 1 for sticky bit\n   private static Pattern umaskSymbolicPattern =    /* not allow X or t */\n     Pattern.compile(\"\\\\G\\\\s*([ugoa]*)([+=-]+)([rwx]*)([,\\\\s]*)\\\\s*\");\n   final short umaskMode;"
    },
    {
        "commit_id": "f4ccdb11dca17db139a3746584e321d884651d01",
        "commit_message": "MAPREDUCE-6427. Fix typo in JobHistoryEventHandler. Contributed by Ray Chiang",
        "commit_url": "https://github.com/apache/hadoop/commit/f4ccdb11dca17db139a3746584e321d884651d01",
        "buggy_code": "tEvent.addEventInfo(\"WORKLFOW_ID\", jse.getWorkflowId());",
        "fixed_code": "tEvent.addEventInfo(\"WORKFLOW_ID\", jse.getWorkflowId());",
        "patch": "@@ -748,7 +748,7 @@ private void processEventForTimelineServer(HistoryEvent event, JobId jobId,\n         tEvent.addEventInfo(\"JOB_CONF_PATH\", jse.getJobConfPath());\n         tEvent.addEventInfo(\"ACLS\", jse.getJobAcls());\n         tEvent.addEventInfo(\"JOB_QUEUE_NAME\", jse.getJobQueueName());\n-        tEvent.addEventInfo(\"WORKLFOW_ID\", jse.getWorkflowId());\n+        tEvent.addEventInfo(\"WORKFLOW_ID\", jse.getWorkflowId());\n         tEvent.addEventInfo(\"WORKFLOW_NAME\", jse.getWorkflowName());\n         tEvent.addEventInfo(\"WORKFLOW_NAME_NAME\", jse.getWorkflowNodeName());\n         tEvent.addEventInfo(\"WORKFLOW_ADJACENCIES\","
    },
    {
        "commit_id": "0a93712f3b9b36d746577dca5da0f7f09756fcca",
        "commit_message": "HDFS-8734. Erasure Coding: fix one cell need two packets. Contributed by Walter Su.",
        "commit_url": "https://github.com/apache/hadoop/commit/0a93712f3b9b36d746577dca5da0f7f09756fcca",
        "buggy_code": "currentPacket.incNumChunks(1);",
        "fixed_code": "currentPacket.incNumChunks();",
        "patch": "@@ -419,7 +419,7 @@ protected synchronized void writeChunk(byte[] b, int offset, int len,\n \n     currentPacket.writeChecksum(checksum, ckoff, cklen);\n     currentPacket.writeData(b, offset, len);\n-    currentPacket.incNumChunks(1);\n+    currentPacket.incNumChunks();\n     streamer.incBytesCurBlock(len);\n \n     // If packet is full, enqueue it for transmission"
    },
    {
        "commit_id": "19295b36d90e26616accee73b1f7743aab5df692",
        "commit_message": "YARN-3381. Fix typo InvalidStateTransitonException. Contributed by Brahma Reddy Battula.",
        "commit_url": "https://github.com/apache/hadoop/commit/19295b36d90e26616accee73b1f7743aab5df692",
        "buggy_code": "throws InvalidStateTransitonException;",
        "fixed_code": "throws InvalidStateTransitionException;",
        "patch": "@@ -28,5 +28,5 @@ public interface StateMachine\n                   EVENTTYPE extends Enum<EVENTTYPE>, EVENT> {\n   public STATE getCurrentState();\n   public STATE doTransition(EVENTTYPE eventType, EVENT event)\n-        throws InvalidStateTransitonException;\n+        throws InvalidStateTransitionException;\n }"
    },
    {
        "commit_id": "c40bdb56a79fe1499c2284d493edc84620c0c078",
        "commit_message": "YARN-2194. Fix bug causing CGroups functionality to fail on RHEL7. Contributed by Wei Yan.",
        "commit_url": "https://github.com/apache/hadoop/commit/c40bdb56a79fe1499c2284d493edc84620c0c078",
        "buggy_code": "finalOpArg.append(\",\");",
        "fixed_code": "finalOpArg.append(PrivilegedOperation.LINUX_FILE_PATH_SEPARATOR);",
        "patch": "@@ -234,7 +234,7 @@ public String executePrivilegedOperation(PrivilegedOperation operation,\n \n       if (noneArgsOnly == false) {\n         //We have already appended at least one tasks file.\n-        finalOpArg.append(\",\");\n+        finalOpArg.append(PrivilegedOperation.LINUX_FILE_PATH_SEPARATOR);\n         finalOpArg.append(tasksFile);\n       } else {\n         finalOpArg.append(tasksFile);"
    },
    {
        "commit_id": "4672315e2d6abe1cee0210cf7d3e8ab114ba933c",
        "commit_message": "YARN-3770. SerializedException should also handle java.lang.Error on de-serialization. Contributed by Lavkesh Lahngir",
        "commit_url": "https://github.com/apache/hadoop/commit/4672315e2d6abe1cee0210cf7d3e8ab114ba933c",
        "buggy_code": "classType = Exception.class;",
        "fixed_code": "classType = Throwable.class;",
        "patch": "@@ -101,7 +101,7 @@ public Throwable deSerialize() {\n     } else if (RuntimeException.class.isAssignableFrom(realClass)) {\n       classType = RuntimeException.class;\n     } else {\n-      classType = Exception.class;\n+      classType = Throwable.class;\n     }\n     return instantiateException(realClass.asSubclass(classType), getMessage(),\n       cause == null ? null : cause.deSerialize());"
    },
    {
        "commit_id": "2236b577a34b069c0d1f91da99f63a199f260ac2",
        "commit_message": "HADOOP-11958. MetricsSystemImpl fails to show backtrace when an error occurs (Jason Lowe via jeagles)",
        "commit_url": "https://github.com/apache/hadoop/commit/2236b577a34b069c0d1f91da99f63a199f260ac2",
        "buggy_code": "LOG.warn(e);",
        "fixed_code": "LOG.warn(\"Error invoking metrics timer\", e);",
        "patch": "@@ -367,7 +367,7 @@ public void run() {\n             try {\n               onTimerEvent();\n             } catch (Exception e) {\n-              LOG.warn(e);\n+              LOG.warn(\"Error invoking metrics timer\", e);\n             }\n           }\n         }, millis, millis);"
    },
    {
        "commit_id": "318d2cde7cb5c05a5f87c4ee967446bb60d28ae4",
        "commit_message": "YARN-3617. Fix WindowsResourceCalculatorPlugin.getCpuFrequency() returning\nalways -1. Contributed by J.Andreina.",
        "commit_url": "https://github.com/apache/hadoop/commit/318d2cde7cb5c05a5f87c4ee967446bb60d28ae4",
        "buggy_code": "return -1;",
        "fixed_code": "return cpuFrequencyKhz;",
        "patch": "@@ -157,7 +157,7 @@ public int getNumCores() {\n   @Override\n   public long getCpuFrequency() {\n     refreshIfNeeded();\n-    return -1;\n+    return cpuFrequencyKhz;\n   }\n \n   /** {@inheritDoc} */"
    },
    {
        "commit_id": "c7729efee8727b59f2c78cd5a3ad23fa84139068",
        "commit_message": "MAPREDUCE-6389. Fix BaileyBorweinPlouffe CLI usage message. Contributed by\nBrahma Reddy Battula.",
        "commit_url": "https://github.com/apache/hadoop/commit/c7729efee8727b59f2c78cd5a3ad23fa84139068",
        "buggy_code": "System.err.println(\"Usage: java \" + getClass().getName()",
        "fixed_code": "System.err.println(\"Usage: bbp \"",
        "patch": "@@ -400,7 +400,7 @@ private static void compute(int startDigit, int nDigits, int nMaps,\n    */\n   public int run(String[] args) throws IOException {\n     if (args.length != 4) {\n-      System.err.println(\"Usage: java \" + getClass().getName()\n+      System.err.println(\"Usage: bbp \"\n           + \" <startDigit> <nDigits> <nMaps> <workingDir>\");\n       ToolRunner.printGenericCommandUsage(System.err);\n       return -1;"
    },
    {
        "commit_id": "c5d4652dedc1e4f8908117fcfc4872c3efd67b3e",
        "commit_message": "HDFS-8556. Erasure Coding: Fix usage of 'createZone' (Contributed by Vinayakumar B)",
        "commit_url": "https://github.com/apache/hadoop/commit/c5d4652dedc1e4f8908117fcfc4872c3efd67b3e",
        "buggy_code": "public static final String USAGE = \"[-s <schemaName>] <path>\";",
        "fixed_code": "public static final String USAGE = \"[-s <schemaName>] [-c <cellSize>] <path>\";",
        "patch": "@@ -83,7 +83,7 @@ protected void processPath(PathData item) throws IOException {\n    */\n   static class CreateECZoneCommand extends ECCommand {\n     public static final String NAME = \"createZone\";\n-    public static final String USAGE = \"[-s <schemaName>] <path>\";\n+    public static final String USAGE = \"[-s <schemaName>] [-c <cellSize>] <path>\";\n     public static final String DESCRIPTION = \n         \"Create a zone to encode files using a specified schema\\n\"\n         + \"Options :\\n\""
    },
    {
        "commit_id": "927577c87ca19e8b5b75722f78e2def6d9386576",
        "commit_message": "HDFS-8552. Fix hdfs CLI usage message for namenode and zkfc. Contributed by Brahma Reddy Battula",
        "commit_url": "https://github.com/apache/hadoop/commit/927577c87ca19e8b5b75722f78e2def6d9386576",
        "buggy_code": "\"Usage: java zkfc [ -formatZK [-force] [-nonInteractive] ]\";",
        "fixed_code": "\"Usage: hdfs zkfc [ -formatZK [-force] [-nonInteractive] ]\";",
        "patch": "@@ -84,7 +84,7 @@ public abstract class ZKFailoverController {\n   };\n   \n   protected static final String USAGE = \n-      \"Usage: java zkfc [ -formatZK [-force] [-nonInteractive] ]\";\n+      \"Usage: hdfs zkfc [ -formatZK [-force] [-nonInteractive] ]\";\n \n   /** Unable to format the parent znode in ZK */\n   static final int ERR_CODE_FORMAT_DENIED = 2;"
    },
    {
        "commit_id": "927577c87ca19e8b5b75722f78e2def6d9386576",
        "commit_message": "HDFS-8552. Fix hdfs CLI usage message for namenode and zkfc. Contributed by Brahma Reddy Battula",
        "commit_url": "https://github.com/apache/hadoop/commit/927577c87ca19e8b5b75722f78e2def6d9386576",
        "buggy_code": "private static final String USAGE = \"Usage: java NameNode [\"",
        "fixed_code": "private static final String USAGE = \"Usage: hdfs namenode [\"",
        "patch": "@@ -246,7 +246,7 @@ public static enum OperationCategory {\n     DFS_HA_AUTO_FAILOVER_ENABLED_KEY\n   };\n   \n-  private static final String USAGE = \"Usage: java NameNode [\"\n+  private static final String USAGE = \"Usage: hdfs namenode [\"\n       + StartupOption.BACKUP.getName() + \"] | \\n\\t[\"\n       + StartupOption.CHECKPOINT.getName() + \"] | \\n\\t[\"\n       + StartupOption.FORMAT.getName() + \" [\""
    },
    {
        "commit_id": "2b2465dfac1f147b6bb20d878b69a8cc3e85c8ad",
        "commit_message": "YARN-3778. Fix Yarn resourcemanger CLI usage. Contributed by Brahma Reddy Battula",
        "commit_url": "https://github.com/apache/hadoop/commit/2b2465dfac1f147b6bb20d878b69a8cc3e85c8ad",
        "buggy_code": "out.println(\"Usage: java ResourceManager [-format-state-store]\");",
        "fixed_code": "out.println(\"Usage: yarn resourcemanager [-format-state-store]\");",
        "patch": "@@ -1303,7 +1303,7 @@ private static void removeApplication(Configuration conf, String applicationId)\n   }\n \n   private static void printUsage(PrintStream out) {\n-    out.println(\"Usage: java ResourceManager [-format-state-store]\");\n+    out.println(\"Usage: yarn resourcemanager [-format-state-store]\");\n     out.println(\"                            \"\n         + \"[-remove-application-from-state-store <appId>]\" + \"\\n\");\n   }"
    },
    {
        "commit_id": "18dd01d6bf67f4d522b947454c1f4347d1cbbc19",
        "commit_message": "YARN-3766. Fixed the apps table column error of generic history web UI. Contributed by Xuan Gong.",
        "commit_url": "https://github.com/apache/hadoop/commit/18dd01d6bf67f4d522b947454c1f4347d1cbbc19",
        "buggy_code": "set(initID(DATATABLES, \"apps\"), WebPageUtils.appsTableInit());",
        "fixed_code": "set(initID(DATATABLES, \"apps\"), WebPageUtils.appsTableInit(false));",
        "patch": "@@ -40,7 +40,7 @@ public class AHSView extends TwoColumnLayout {\n   protected void preHead(Page.HTML<_> html) {\n     commonPreHead(html);\n     set(DATATABLES_ID, \"apps\");\n-    set(initID(DATATABLES, \"apps\"), WebPageUtils.appsTableInit());\n+    set(initID(DATATABLES, \"apps\"), WebPageUtils.appsTableInit(false));\n     setTableStyles(html, \"apps\", \".queue {width:6em}\", \".ui {width:8em}\");\n \n     // Set the correct title."
    },
    {
        "commit_id": "c9e0268216584f1df1a7c6cd25d2cfb2bc6d1d3c",
        "commit_message": "Addendum fix for HDFS-7912.",
        "commit_url": "https://github.com/apache/hadoop/commit/c9e0268216584f1df1a7c6cd25d2cfb2bc6d1d3c",
        "buggy_code": "BlockInfoContiguous bi = blocksMap.getStoredBlock(timedOutItems[i]);",
        "fixed_code": "BlockInfo bi = blocksMap.getStoredBlock(timedOutItems[i]);",
        "patch": "@@ -1819,7 +1819,7 @@ private void processPendingReplications() {\n            * Use the blockinfo from the blocksmap to be certain we're working\n            * with the most up-to-date block information (e.g. genstamp).\n            */\n-          BlockInfoContiguous bi = blocksMap.getStoredBlock(timedOutItems[i]);\n+          BlockInfo bi = blocksMap.getStoredBlock(timedOutItems[i]);\n           if (bi == null) {\n             continue;\n           }"
    },
    {
        "commit_id": "47ef869fa790dd096b576697c4245d2f3a3193fa",
        "commit_message": "HDFS-8428. Erasure Coding: Fix the NullPointerException when deleting file. Contributed by Yi Liu.",
        "commit_url": "https://github.com/apache/hadoop/commit/47ef869fa790dd096b576697c4245d2f3a3193fa",
        "buggy_code": "removeStoredBlock(storageInfo, getStoredBlock(rdbi.getBlock()), node);",
        "fixed_code": "removeStoredBlock(storageInfo, rdbi.getBlock(), node);",
        "patch": "@@ -3396,7 +3396,7 @@ public void processIncrementalBlockReport(final DatanodeID nodeID,\n     for (ReceivedDeletedBlockInfo rdbi : srdb.getBlocks()) {\n       switch (rdbi.getStatus()) {\n       case DELETED_BLOCK:\n-        removeStoredBlock(storageInfo, getStoredBlock(rdbi.getBlock()), node);\n+        removeStoredBlock(storageInfo, rdbi.getBlock(), node);\n         deleted++;\n         break;\n       case RECEIVED_BLOCK:"
    },
    {
        "commit_id": "b008348dbf9bdd5070930be5d182116c5d370f6b",
        "commit_message": "HDFS-8418. Fix the isNeededReplication calculation for Striped block in NN. Contributed by Yi Liu.",
        "commit_url": "https://github.com/apache/hadoop/commit/b008348dbf9bdd5070930be5d182116c5d370f6b",
        "buggy_code": "bc.getPreferredBlockReplication());",
        "fixed_code": "bm.getExpectedReplicaNum(bc, blockInfo));",
        "patch": "@@ -256,7 +256,7 @@ public void blockIdCK(String blockId) {\n       out.println(\"Block Id: \" + blockId);\n       out.println(\"Block belongs to: \"+iNode.getFullPathName());\n       out.println(\"No. of Expected Replica: \" +\n-          bc.getPreferredBlockReplication());\n+          bm.getExpectedReplicaNum(bc, blockInfo));\n       out.println(\"No. of live Replica: \" + numberReplicas.liveReplicas());\n       out.println(\"No. of excess Replica: \" + numberReplicas.excessReplicas());\n       out.println(\"No. of stale Replica: \" +"
    },
    {
        "commit_id": "b64f6745a45754dcf79c9c2626f3db7db2f33858",
        "commit_message": "HADOOP-11566. Add tests and fix for erasure coders to recover erased parity units. Contributed by Kai Zheng.",
        "commit_url": "https://github.com/apache/hadoop/commit/b64f6745a45754dcf79c9c2626f3db7db2f33858",
        "buggy_code": "ECChunk[] backupChunks = backupAndEraseChunks(clonedDataChunks);",
        "fixed_code": "ECChunk[] backupChunks = backupAndEraseChunks(clonedDataChunks, parityChunks);",
        "patch": "@@ -52,7 +52,7 @@ protected void testCoding(boolean usingDirectBuffer) {\n     encoder.encode(dataChunks, parityChunks);\n \n     // Backup and erase some chunks\n-    ECChunk[] backupChunks = backupAndEraseChunks(clonedDataChunks);\n+    ECChunk[] backupChunks = backupAndEraseChunks(clonedDataChunks, parityChunks);\n \n     // Decode\n     ECChunk[] inputChunks = prepareInputChunksForDecoding("
    },
    {
        "commit_id": "9798065cbb5cf5de94fe8e17ac22388f70e12dd6",
        "commit_message": "HDFS-8195. Erasure coding: Fix file quota change when we complete/commit the striped blocks. Contributed by Takuya Fukudome.",
        "commit_url": "https://github.com/apache/hadoop/commit/9798065cbb5cf5de94fe8e17ac22388f70e12dd6",
        "buggy_code": "replication, replication);;",
        "fixed_code": "replication, replication);",
        "patch": "@@ -520,7 +520,7 @@ void updateCount(INodesInPath iip, long nsDelta, long ssDelta, short replication\n     final INodeFile fileINode = iip.getLastINode().asFile();\n     EnumCounters<StorageType> typeSpaceDeltas =\n       getStorageTypeDeltas(fileINode.getStoragePolicyID(), ssDelta,\n-          replication, replication);;\n+          replication, replication);\n     updateCount(iip, iip.length() - 1,\n       new QuotaCounts.Builder().nameSpace(nsDelta).storageSpace(ssDelta * replication).\n           typeSpaces(typeSpaceDeltas).build(),"
    },
    {
        "commit_id": "f6e1160ef1e946a5f6c9503b06832e6b84c36edb",
        "commit_message": "HDFS-8145. Fix the editlog corruption exposed by failed TestAddStripedBlocks. Contributed by Jing Zhao.",
        "commit_url": "https://github.com/apache/hadoop/commit/f6e1160ef1e946a5f6c9503b06832e6b84c36edb",
        "buggy_code": "public boolean getECPolicy(INodesInPath iip) throws IOException {",
        "fixed_code": "public boolean isInECZone(INodesInPath iip) throws IOException {",
        "patch": "@@ -1237,7 +1237,7 @@ XAttr createErasureCodingZone(String src, ECSchema schema)\n     }\n   }\n \n-  public boolean getECPolicy(INodesInPath iip) throws IOException {\n+  public boolean isInECZone(INodesInPath iip) throws IOException {\n     return getECSchema(iip) != null;\n   }\n "
    },
    {
        "commit_id": "f6e1160ef1e946a5f6c9503b06832e6b84c36edb",
        "commit_message": "HDFS-8145. Fix the editlog corruption exposed by failed TestAddStripedBlocks. Contributed by Jing Zhao.",
        "commit_url": "https://github.com/apache/hadoop/commit/f6e1160ef1e946a5f6c9503b06832e6b84c36edb",
        "buggy_code": "(short) 6, (short) 3);",
        "fixed_code": "HdfsConstants.NUM_DATA_BLOCKS, HdfsConstants.NUM_PARITY_BLOCKS);",
        "patch": "@@ -158,7 +158,7 @@ private void testSaveAndLoadStripedINodeFile(FSNamesystem fsn, Configuration con\n     for (int i = 0; i < stripedBlks.length; i++) {\n       stripedBlks[i] = new BlockInfoStriped(\n               new Block(stripedBlkId + i, preferredBlockSize, timestamp),\n-              (short) 6, (short) 3);\n+              HdfsConstants.NUM_DATA_BLOCKS, HdfsConstants.NUM_PARITY_BLOCKS);\n       file.getStripedBlocksFeature().addBlock(stripedBlks[i]);\n     }\n "
    },
    {
        "commit_id": "4d0bc724f29b646e252f53d1c654a23e8526a4bf",
        "commit_message": "HDFS-8077. Erasure coding: fix bugs in EC zone and symlinks. Contributed by Jing Zhao and Zhe Zhang.",
        "commit_url": "https://github.com/apache/hadoop/commit/4d0bc724f29b646e252f53d1c654a23e8526a4bf",
        "buggy_code": "Block blk = new Block(this.getBlockId() + i, this.getGenerationStamp(), 0);",
        "fixed_code": "Block blk = new Block(this.getBlockId() + i, 0, this.getGenerationStamp());",
        "patch": "@@ -96,7 +96,7 @@ public void setExpectedLocations(DatanodeStorageInfo[] targets) {\n     for(int i = 0; i < numLocations; i++) {\n       // when creating a new block we simply sequentially assign block index to\n       // each storage\n-      Block blk = new Block(this.getBlockId() + i, this.getGenerationStamp(), 0);\n+      Block blk = new Block(this.getBlockId() + i, 0, this.getGenerationStamp());\n       replicas[i] = new ReplicaUnderConstruction(blk, targets[i],\n           ReplicaState.RBW);\n     }"
    },
    {
        "commit_id": "4d0bc724f29b646e252f53d1c654a23e8526a4bf",
        "commit_message": "HDFS-8077. Erasure coding: fix bugs in EC zone and symlinks. Contributed by Jing Zhao and Zhe Zhang.",
        "commit_url": "https://github.com/apache/hadoop/commit/4d0bc724f29b646e252f53d1c654a23e8526a4bf",
        "buggy_code": "assertEquals(0, fileByLoaded.getBlockReplication());",
        "fixed_code": "assertEquals(0, fileByLoaded.getFileReplication());",
        "patch": "@@ -199,7 +199,7 @@ private void testSaveAndLoadStripedINodeFile(FSNamesystem fsn, Configuration con\n     assertEquals(mtime, fileByLoaded.getModificationTime());\n     assertEquals(isUC ? mtime : atime, fileByLoaded.getAccessTime());\n     assertEquals(0, fileByLoaded.getContiguousBlocks().length);\n-    assertEquals(0, fileByLoaded.getBlockReplication());\n+    assertEquals(0, fileByLoaded.getFileReplication());\n     assertEquals(preferredBlockSize, fileByLoaded.getPreferredBlockSize());\n \n     //check the BlockInfoStriped"
    },
    {
        "commit_id": "4d0bc724f29b646e252f53d1c654a23e8526a4bf",
        "commit_message": "HDFS-8077. Erasure coding: fix bugs in EC zone and symlinks. Contributed by Jing Zhao and Zhe Zhang.",
        "commit_url": "https://github.com/apache/hadoop/commit/4d0bc724f29b646e252f53d1c654a23e8526a4bf",
        "buggy_code": "assertTrue(fileNode.isWithStripedBlocks());",
        "fixed_code": "assertTrue(fileNode.isStriped());",
        "patch": "@@ -84,7 +84,7 @@ public void testMissingStripedBlock() throws Exception {\n     final INodeFile fileNode = cluster.getNamesystem().getFSDirectory()\n         .getINode4Write(filePath.toString()).asFile();\n     assertFalse(fileNode.isUnderConstruction());\n-    assertTrue(fileNode.isWithStripedBlocks());\n+    assertTrue(fileNode.isStriped());\n     BlockInfo[] blocks = fileNode.getBlocks();\n     assertEquals(numBlocks, blocks.length);\n     for (BlockInfo blk : blocks) {"
    },
    {
        "commit_id": "7401e5b5e8060b6b027d714b5ceb641fcfe5b598",
        "commit_message": "YARN-3677. Fix findbugs warnings in yarn-server-resourcemanager. Contributed by Vinod Kumar Vavilapalli.",
        "commit_url": "https://github.com/apache/hadoop/commit/7401e5b5e8060b6b027d714b5ceb641fcfe5b598",
        "buggy_code": "private boolean isHDFS;",
        "fixed_code": "private volatile boolean isHDFS;",
        "patch": "@@ -100,7 +100,7 @@ public class FileSystemRMStateStore extends RMStateStore {\n   private Path dtSequenceNumberPath = null;\n   private int fsNumRetries;\n   private long fsRetryInterval;\n-  private boolean isHDFS;\n+  private volatile boolean isHDFS;\n \n   @VisibleForTesting\n   Path fsWorkingPath;"
    },
    {
        "commit_id": "0c590e1c097462979f7ee054ad9121345d58655b",
        "commit_message": "HDFS-8405. Fix a typo in NamenodeFsck.  Contributed by Takanobu Asanuma",
        "commit_url": "https://github.com/apache/hadoop/commit/0c590e1c097462979f7ee054ad9121345d58655b",
        "buggy_code": "totalDatanodes, bm.minReplication, remoteAddress).fsck();",
        "fixed_code": "totalDatanodes, remoteAddress).fsck();",
        "patch": "@@ -66,7 +66,7 @@ public Object run() throws Exception {\n               namesystem.getNumberOfDatanodes(DatanodeReportType.LIVE); \n           new NamenodeFsck(conf, nn,\n               bm.getDatanodeManager().getNetworkTopology(), pmap, out,\n-              totalDatanodes, bm.minReplication, remoteAddress).fsck();\n+              totalDatanodes, remoteAddress).fsck();\n           \n           return null;\n         }"
    },
    {
        "commit_id": "03a293aed6de101b0cae1a294f506903addcaa75",
        "commit_message": "YARN-3505 addendum: fix an issue in previous patch.",
        "commit_url": "https://github.com/apache/hadoop/commit/03a293aed6de101b0cae1a294f506903addcaa75",
        "buggy_code": "this.context.getLogAggregationStatusForApps().add(report);",
        "fixed_code": "this.context.getLogAggregationStatusForApps().add(finalReport);",
        "patch": "@@ -359,7 +359,7 @@ public Object run() throws Exception {\n         finalReport.setApplicationId(appId);\n         finalReport.setLogAggregationStatus(renameTemporaryLogFileFailed\n             ? LogAggregationStatus.FAILED : LogAggregationStatus.SUCCEEDED);\n-        this.context.getLogAggregationStatusForApps().add(report);\n+        this.context.getLogAggregationStatusForApps().add(finalReport);\n       }\n     } finally {\n       if (writer != null) {"
    },
    {
        "commit_id": "987abc99b0309a07f0a342746b2a5048d5c36ce0",
        "commit_message": "HDFS-8362. Java Compilation Error in TestHdfsConfigFields.java (Contributed by Arshad Mohammad)",
        "commit_url": "https://github.com/apache/hadoop/commit/987abc99b0309a07f0a342746b2a5048d5c36ce0",
        "buggy_code": "package org.apache.hadoop.hdfs.tools;",
        "fixed_code": "package org.apache.hadoop.tools;",
        "patch": "@@ -16,7 +16,7 @@\n  * limitations under the License.\n  */\n \n-package org.apache.hadoop.hdfs.tools;\n+package org.apache.hadoop.tools;\n \n import java.util.HashSet;\n "
    },
    {
        "commit_id": "b167fe7605deb29ec533047d79d036eb65328853",
        "commit_message": "YARN-1832. Fix wrong MockLocalizerStatus#equals implementation. Contributed by Hong Zhiguo.",
        "commit_url": "https://github.com/apache/hadoop/commit/b167fe7605deb29ec533047d79d036eb65328853",
        "buggy_code": "return getLocalizerId().equals(other)",
        "fixed_code": "return getLocalizerId().equals(other.getLocalizerId())",
        "patch": "@@ -67,7 +67,7 @@ public boolean equals(Object o) {\n       return false;\n     }\n     MockLocalizerStatus other = (MockLocalizerStatus) o;\n-    return getLocalizerId().equals(other)\n+    return getLocalizerId().equals(other.getLocalizerId())\n       && getResources().containsAll(other.getResources())\n       && other.getResources().containsAll(getResources());\n   }"
    },
    {
        "commit_id": "bb6ef2984d8f117711b806c4ebdc757bd182c06e",
        "commit_message": "MAPREDUCE-6349. Fix typo in property org.apache.hadoop.mapreduce.lib.chain.Chain.REDUCER_INPUT_VALUE_CLASS. Contributed by Ray Chiang.",
        "commit_url": "https://github.com/apache/hadoop/commit/bb6ef2984d8f117711b806c4ebdc757bd182c06e",
        "buggy_code": "\"maperduce.chain.reducer.input.value.class\";",
        "fixed_code": "\"mapreduce.chain.reducer.input.value.class\";",
        "patch": "@@ -68,7 +68,7 @@ public class Chain {\n   protected static final String REDUCER_INPUT_KEY_CLASS = \n     \"mapreduce.chain.reducer.input.key.class\";\n   protected static final String REDUCER_INPUT_VALUE_CLASS = \n-    \"maperduce.chain.reducer.input.value.class\";\n+    \"mapreduce.chain.reducer.input.value.class\";\n   protected static final String REDUCER_OUTPUT_KEY_CLASS = \n     \"mapreduce.chain.reducer.output.key.class\";\n   protected static final String REDUCER_OUTPUT_VALUE_CLASS = "
    },
    {
        "commit_id": "98a61766286321468bf801a9f17a843d7eae8d9e",
        "commit_message": "HDFS-8300. Fix unit test failures and findbugs warning caused by HDFS-8283. Contributed by Jing Zhao.",
        "commit_url": "https://github.com/apache/hadoop/commit/98a61766286321468bf801a9f17a843d7eae8d9e",
        "buggy_code": "streamer.getLastException().check();",
        "fixed_code": "streamer.getLastException().check(true);",
        "patch": "@@ -762,7 +762,7 @@ public synchronized void close() throws IOException {\n \n   protected synchronized void closeImpl() throws IOException {\n     if (isClosed()) {\n-      streamer.getLastException().check();\n+      streamer.getLastException().check(true);\n       return;\n     }\n "
    },
    {
        "commit_id": "1a2459bd4be54e64eec0eebffd941989476c2a5b",
        "commit_message": "HADOOP-11857. Fix CommandFormat#commandFormat java doc annotation. Contributed by J.Andreina.",
        "commit_url": "https://github.com/apache/hadoop/commit/1a2459bd4be54e64eec0eebffd941989476c2a5b",
        "buggy_code": "public CommandFormat(String n, int min, int max, String ... possibleOpt) {",
        "fixed_code": "public CommandFormat(String name, int min, int max, String ... possibleOpt) {",
        "patch": "@@ -43,7 +43,7 @@ public class CommandFormat {\n    * @see #CommandFormat(int, int, String...)\n    */\n   @Deprecated\n-  public CommandFormat(String n, int min, int max, String ... possibleOpt) {\n+  public CommandFormat(String name, int min, int max, String ... possibleOpt) {\n     this(min, max, possibleOpt);\n   }\n   "
    },
    {
        "commit_id": "f65eeb412d140a3808bcf99344a9f3a965918f70",
        "commit_message": "YARN-3493. RM fails to come up with error \"Failed to load/recover state\" when mem settings are changed. (Jian He via wangda)",
        "commit_url": "https://github.com/apache/hadoop/commit/f65eeb412d140a3808bcf99344a9f3a965918f70",
        "buggy_code": "RMServerUtils.validateResourceRequests(ask,",
        "fixed_code": "RMServerUtils.normalizeAndValidateRequests(ask,",
        "patch": "@@ -499,7 +499,7 @@ public AllocateResponse allocate(AllocateRequest request)\n               \n       // sanity check\n       try {\n-        RMServerUtils.validateResourceRequests(ask,\n+        RMServerUtils.normalizeAndValidateRequests(ask,\n             rScheduler.getMaximumResourceCapability(), app.getQueue(),\n             rScheduler);\n       } catch (InvalidResourceRequestException e) {"
    },
    {
        "commit_id": "369ddc67bdaf61cca3f2f766ab504e2932f6fb72",
        "commit_message": "HDFS-8153. Error Message points to wrong parent directory in case of path component name length error. Contributed by Anu Engineer.",
        "commit_url": "https://github.com/apache/hadoop/commit/369ddc67bdaf61cca3f2f766ab504e2932f6fb72",
        "buggy_code": "final String parentPath = existing.getPath(pos - 1);",
        "fixed_code": "final String parentPath = existing.getPath();",
        "patch": "@@ -972,7 +972,7 @@ public INodesInPath addLastINode(INodesInPath existing, INode inode,\n     // original location because a quota violation would cause the the item\n     // to go \"poof\".  The fs limits must be bypassed for the same reason.\n     if (checkQuota) {\n-      final String parentPath = existing.getPath(pos - 1);\n+      final String parentPath = existing.getPath();\n       verifyMaxComponentLength(inode.getLocalNameBytes(), parentPath);\n       verifyMaxDirItems(parent, parentPath);\n     }"
    },
    {
        "commit_id": "60da0e49e7316892d63e9c7cdc3214057e68009a",
        "commit_message": "HDFS-8084. Move dfs.client.failover.* confs from DFSConfigKeys to HdfsClientConfigKeys.Failover and fix typos in the dfs.http.client.* configuration keys.",
        "commit_url": "https://github.com/apache/hadoop/commit/60da0e49e7316892d63e9c7cdc3214057e68009a",
        "buggy_code": "conf.setBoolean(HdfsClientConfigKeys.WebHdfsRetry.RETRY_POLICY_ENABLED_KEY, true);",
        "fixed_code": "conf.setBoolean(HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_KEY, true);",
        "patch": "@@ -875,7 +875,7 @@ public static void namenodeRestartTest(final Configuration conf,\n     final Path dir = new Path(\"/testNamenodeRestart\");\n \n     if (isWebHDFS) {\n-      conf.setBoolean(HdfsClientConfigKeys.WebHdfsRetry.RETRY_POLICY_ENABLED_KEY, true);\n+      conf.setBoolean(HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_KEY, true);\n     } else {\n       conf.setBoolean(HdfsClientConfigKeys.Retry.POLICY_ENABLED_KEY, true);\n     }"
    },
    {
        "commit_id": "7af086a515d573dc90ea4deec7f4e3f23622e0e8",
        "commit_message": "YARN-3459. Fix failiure of TestLog4jWarningErrorMetricsAppender. (Varun Vasudev via wangda)",
        "commit_url": "https://github.com/apache/hadoop/commit/7af086a515d573dc90ea4deec7f4e3f23622e0e8",
        "buggy_code": "Thread.sleep(2000);",
        "fixed_code": "Thread.sleep(3000);",
        "patch": "@@ -84,7 +84,7 @@ public void testPurge() throws Exception {\n     Assert.assertEquals(1, appender.getErrorCounts(cutoff).get(0).longValue());\n     Assert.assertEquals(1, appender.getErrorMessagesAndCounts(cutoff).get(0)\n       .size());\n-    Thread.sleep(2000);\n+    Thread.sleep(3000);\n     Assert.assertEquals(1, appender.getErrorCounts(cutoff).size());\n     Assert.assertEquals(0, appender.getErrorCounts(cutoff).get(0).longValue());\n     Assert.assertEquals(0, appender.getErrorMessagesAndCounts(cutoff).get(0)"
    },
    {
        "commit_id": "e817cedcdc262630206630d4a58d1051ceab8794",
        "commit_message": "HDFS-7951. Fix NPE for TestFsDatasetImpl#testAddVolumeFailureReleasesInUseLock on Linux. (Contributed by Xiaoyu Yao)",
        "commit_url": "https://github.com/apache/hadoop/commit/e817cedcdc262630206630d4a58d1051ceab8794",
        "buggy_code": "when(storage.prepareVolume(eq(datanode), eq(badDir),",
        "fixed_code": "when(storage.prepareVolume(eq(datanode), eq(badDir.getAbsoluteFile()),",
        "patch": "@@ -330,7 +330,7 @@ public void testAddVolumeFailureReleasesInUseLock() throws IOException {\n     Storage.StorageDirectory sd = createStorageDirectory(badDir);\n     sd.lock();\n     DataStorage.VolumeBuilder builder = new DataStorage.VolumeBuilder(storage, sd);\n-    when(storage.prepareVolume(eq(datanode), eq(badDir),\n+    when(storage.prepareVolume(eq(datanode), eq(badDir.getAbsoluteFile()),\n         Matchers.<List<NamespaceInfo>>any()))\n         .thenReturn(builder);\n "
    },
    {
        "commit_id": "32b43304563c2430c00bc3e142a962d2bc5f4d58",
        "commit_message": "Revert \"YARN-3181. FairScheduler: Fix up outdated findbugs issues. (kasha)\"\n\nThis reverts commit c2b185def846f5577a130003a533b9c377b58fab.",
        "commit_url": "https://github.com/apache/hadoop/commit/32b43304563c2430c00bc3e142a962d2bc5f4d58",
        "buggy_code": "public void reloadAllocations() throws IOException,",
        "fixed_code": "public synchronized void reloadAllocations() throws IOException,",
        "patch": "@@ -201,7 +201,7 @@ public synchronized void setReloadListener(Listener reloadListener) {\n    * @throws ParserConfigurationException if XML parser is misconfigured.\n    * @throws SAXException if config file is malformed.\n    */\n-  public void reloadAllocations() throws IOException,\n+  public synchronized void reloadAllocations() throws IOException,\n       ParserConfigurationException, SAXException, AllocationConfigurationException {\n     if (allocFile == null) {\n       return;"
    },
    {
        "commit_id": "bc9cb3e271b22069a15ca110cd60c860250aaab2",
        "commit_message": "HDFS-7915. The DataNode can sometimes allocate a ShortCircuitShm slot and fail to tell the DFSClient about it because of a network error (cmccabe)",
        "commit_url": "https://github.com/apache/hadoop/commit/bc9cb3e271b22069a15ca110cd60c860250aaab2",
        "buggy_code": "slotId, proto.getMaxVersion());",
        "fixed_code": "slotId, proto.getMaxVersion(), true);",
        "patch": "@@ -186,7 +186,7 @@ private void opRequestShortCircuitFds(DataInputStream in) throws IOException {\n     try {\n       requestShortCircuitFds(PBHelper.convert(proto.getHeader().getBlock()),\n           PBHelper.convert(proto.getHeader().getToken()),\n-          slotId, proto.getMaxVersion());\n+          slotId, proto.getMaxVersion(), true);\n     } finally {\n       if (traceScope != null) traceScope.close();\n     }"
    },
    {
        "commit_id": "32741cf3d25d85a92e3deb11c302cc2a718d71dd",
        "commit_message": "Revert \"HDFS-7915. The DataNode can sometimes allocate a ShortCircuitShm slot and fail to tell the DFSClient about it because of a network error (cmccabe)\" (jenkins didn't run yet)\n\nThis reverts commit 5aa892ed486d42ae6b94c4866b92cd2b382ea640.",
        "commit_url": "https://github.com/apache/hadoop/commit/32741cf3d25d85a92e3deb11c302cc2a718d71dd",
        "buggy_code": "slotId, proto.getMaxVersion(), true);",
        "fixed_code": "slotId, proto.getMaxVersion());",
        "patch": "@@ -186,7 +186,7 @@ private void opRequestShortCircuitFds(DataInputStream in) throws IOException {\n     try {\n       requestShortCircuitFds(PBHelper.convert(proto.getHeader().getBlock()),\n           PBHelper.convert(proto.getHeader().getToken()),\n-          slotId, proto.getMaxVersion(), true);\n+          slotId, proto.getMaxVersion());\n     } finally {\n       if (traceScope != null) traceScope.close();\n     }"
    },
    {
        "commit_id": "5aa892ed486d42ae6b94c4866b92cd2b382ea640",
        "commit_message": "HDFS-7915. The DataNode can sometimes allocate a ShortCircuitShm slot and fail to tell the DFSClient about it because of a network error (cmccabe)",
        "commit_url": "https://github.com/apache/hadoop/commit/5aa892ed486d42ae6b94c4866b92cd2b382ea640",
        "buggy_code": "slotId, proto.getMaxVersion());",
        "fixed_code": "slotId, proto.getMaxVersion(), true);",
        "patch": "@@ -186,7 +186,7 @@ private void opRequestShortCircuitFds(DataInputStream in) throws IOException {\n     try {\n       requestShortCircuitFds(PBHelper.convert(proto.getHeader().getBlock()),\n           PBHelper.convert(proto.getHeader().getToken()),\n-          slotId, proto.getMaxVersion());\n+          slotId, proto.getMaxVersion(), true);\n     } finally {\n       if (traceScope != null) traceScope.close();\n     }"
    },
    {
        "commit_id": "d1c6accb6f87b08975175580e15f1ff1fe29ab04",
        "commit_message": "HADOOP-11602. Fix toUpperCase/toLowerCase to use Locale.ENGLISH. (ozawa)",
        "commit_url": "https://github.com/apache/hadoop/commit/d1c6accb6f87b08975175580e15f1ff1fe29ab04",
        "buggy_code": "encoding = enValueOfFunc.apply(en.toUpperCase(Locale.ENGLISH));",
        "fixed_code": "encoding = enValueOfFunc.apply(StringUtils.toUpperCase(en));",
        "patch": "@@ -79,7 +79,7 @@ protected void processOptions(LinkedList<String> args) throws IOException {\n       String en = StringUtils.popOptionWithArgument(\"-e\", args);\n       if (en != null) {\n         try {\n-          encoding = enValueOfFunc.apply(en.toUpperCase(Locale.ENGLISH));\n+          encoding = enValueOfFunc.apply(StringUtils.toUpperCase(en));\n         } catch (IllegalArgumentException e) {\n           throw new IllegalArgumentException(\n               \"Invalid/unsupported encoding option specified: \" + en);"
    },
    {
        "commit_id": "d1c6accb6f87b08975175580e15f1ff1fe29ab04",
        "commit_message": "HADOOP-11602. Fix toUpperCase/toLowerCase to use Locale.ENGLISH. (ozawa)",
        "commit_url": "https://github.com/apache/hadoop/commit/d1c6accb6f87b08975175580e15f1ff1fe29ab04",
        "buggy_code": "for (String line : hexdump.toUpperCase().split(\"\\n\")) {",
        "fixed_code": "for (String line : StringUtils.toUpperCase(hexdump).split(\"\\n\")) {",
        "patch": "@@ -1296,7 +1296,7 @@ private static byte[] hexDumpToBytes(String hexdump) {\n     \n     StringBuilder hexString = new StringBuilder();\n     \n-    for (String line : hexdump.toUpperCase().split(\"\\n\")) {\n+    for (String line : StringUtils.toUpperCase(hexdump).split(\"\\n\")) {\n       hexString.append(line.substring(0, LAST_HEX_COL).replace(\" \", \"\"));\n     }\n     return StringUtils.hexStringToByte(hexString.toString());"
    },
    {
        "commit_id": "d1c6accb6f87b08975175580e15f1ff1fe29ab04",
        "commit_message": "HADOOP-11602. Fix toUpperCase/toLowerCase to use Locale.ENGLISH. (ozawa)",
        "commit_url": "https://github.com/apache/hadoop/commit/d1c6accb6f87b08975175580e15f1ff1fe29ab04",
        "buggy_code": "sb.append(qop.name().toLowerCase());",
        "fixed_code": "sb.append(org.apache.hadoop.util.StringUtils.toLowerCase(qop.name()));",
        "patch": "@@ -181,7 +181,7 @@ static String getQOPNames (QualityOfProtection[] qops){\n     StringBuilder sb = new StringBuilder();\n     int i = 0;\n     for (QualityOfProtection qop:qops){\n-     sb.append(qop.name().toLowerCase());\n+     sb.append(org.apache.hadoop.util.StringUtils.toLowerCase(qop.name()));\n      if (++i < qops.length){\n        sb.append(\",\");\n      }"
    },
    {
        "commit_id": "d1c6accb6f87b08975175580e15f1ff1fe29ab04",
        "commit_message": "HADOOP-11602. Fix toUpperCase/toLowerCase to use Locale.ENGLISH. (ozawa)",
        "commit_url": "https://github.com/apache/hadoop/commit/d1c6accb6f87b08975175580e15f1ff1fe29ab04",
        "buggy_code": "return Enum.valueOf(klass, str.toUpperCase());",
        "fixed_code": "return Enum.valueOf(klass, StringUtils.toUpperCase(str));",
        "patch": "@@ -34,7 +34,7 @@ public EnumParam(String name, Class<E> e, E defaultValue) {\n \n   @Override\n   protected E parse(String str) throws Exception {\n-    return Enum.valueOf(klass, str.toUpperCase());\n+    return Enum.valueOf(klass, StringUtils.toUpperCase(str));\n   }\n \n   @Override"
    },
    {
        "commit_id": "d1c6accb6f87b08975175580e15f1ff1fe29ab04",
        "commit_message": "HADOOP-11602. Fix toUpperCase/toLowerCase to use Locale.ENGLISH. (ozawa)",
        "commit_url": "https://github.com/apache/hadoop/commit/d1c6accb6f87b08975175580e15f1ff1fe29ab04",
        "buggy_code": "String cmd = opts.getCommand().toString().toLowerCase();",
        "fixed_code": "String cmd = StringUtils.toLowerCase(opts.getCommand().toString());",
        "patch": "@@ -587,7 +587,7 @@ private int processStartupCommand(CommandLineOpts opts) throws Exception {\n       return 0;\n     }\n     \n-    String cmd = opts.getCommand().toString().toLowerCase();\n+    String cmd = StringUtils.toLowerCase(opts.getCommand().toString());\n     \n     int exitCode = 0;\n     try {"
    },
    {
        "commit_id": "d1c6accb6f87b08975175580e15f1ff1fe29ab04",
        "commit_message": "HADOOP-11602. Fix toUpperCase/toLowerCase to use Locale.ENGLISH. (ozawa)",
        "commit_url": "https://github.com/apache/hadoop/commit/d1c6accb6f87b08975175580e15f1ff1fe29ab04",
        "buggy_code": "String scheme = uriScheme.toUpperCase();",
        "fixed_code": "String scheme = StringUtils.toUpperCase(uriScheme);",
        "patch": "@@ -116,7 +116,7 @@ public static enum Counter {\n    * BYTES_READ counter and second one is of the BYTES_WRITTEN counter.\n    */\n   protected static String[] getFileSystemCounterNames(String uriScheme) {\n-    String scheme = uriScheme.toUpperCase();\n+    String scheme = StringUtils.toUpperCase(uriScheme);\n     return new String[]{scheme+\"_BYTES_READ\", scheme+\"_BYTES_WRITTEN\"};\n   }\n   "
    },
    {
        "commit_id": "d1c6accb6f87b08975175580e15f1ff1fe29ab04",
        "commit_message": "HADOOP-11602. Fix toUpperCase/toLowerCase to use Locale.ENGLISH. (ozawa)",
        "commit_url": "https://github.com/apache/hadoop/commit/d1c6accb6f87b08975175580e15f1ff1fe29ab04",
        "buggy_code": "if(driverClassName.toLowerCase().contains(\"oracle\")) {",
        "fixed_code": "if(StringUtils.toLowerCase(driverClassName).contains(\"oracle\")) {",
        "patch": "@@ -102,7 +102,7 @@ private void startHsqldbServer() {\n   \n   private void createConnection(String driverClassName\n       , String url) throws Exception {\n-    if(driverClassName.toLowerCase().contains(\"oracle\")) {\n+    if(StringUtils.toLowerCase(driverClassName).contains(\"oracle\")) {\n       isOracle = true;\n     }\n     Class.forName(driverClassName);"
    },
    {
        "commit_id": "d1c6accb6f87b08975175580e15f1ff1fe29ab04",
        "commit_message": "HADOOP-11602. Fix toUpperCase/toLowerCase to use Locale.ENGLISH. (ozawa)",
        "commit_url": "https://github.com/apache/hadoop/commit/d1c6accb6f87b08975175580e15f1ff1fe29ab04",
        "buggy_code": "return Values.valueOf(name.toUpperCase());",
        "fixed_code": "return Values.valueOf(StringUtils.toUpperCase(name));",
        "patch": "@@ -433,7 +433,7 @@ private static Values getPre21Value(String name) {\n       return Values.SUCCESS;\n     }\n     \n-    return Values.valueOf(name.toUpperCase());\n+    return Values.valueOf(StringUtils.toUpperCase(name));\n   }\n \n   private void processTaskUpdatedEvent(TaskUpdatedEvent event) {"
    },
    {
        "commit_id": "d1c6accb6f87b08975175580e15f1ff1fe29ab04",
        "commit_message": "HADOOP-11602. Fix toUpperCase/toLowerCase to use Locale.ENGLISH. (ozawa)",
        "commit_url": "https://github.com/apache/hadoop/commit/d1c6accb6f87b08975175580e15f1ff1fe29ab04",
        "buggy_code": "stateFilter = NodeState.valueOf(type.toUpperCase());",
        "fixed_code": "stateFilter = NodeState.valueOf(StringUtils.toUpperCase(type));",
        "patch": "@@ -77,7 +77,7 @@ protected void render(Block html) {\n               .th(\".nodeManagerVersion\", \"Version\")._()._().tbody();\n       NodeState stateFilter = null;\n       if (type != null && !type.isEmpty()) {\n-        stateFilter = NodeState.valueOf(type.toUpperCase());\n+        stateFilter = NodeState.valueOf(StringUtils.toUpperCase(type));\n       }\n       Collection<RMNode> rmNodes = this.rm.getRMContext().getRMNodes().values();\n       boolean isInactive = false;"
    },
    {
        "commit_id": "73bcfa99af61e5202f030510db8954c17cba43cc",
        "commit_message": "HDFS-7831. Fix the starting index and end condition of the loop in FileDiffList.findEarlierSnapshotBlocks(). Contributed by Konstantin Shvachko.",
        "commit_url": "https://github.com/apache/hadoop/commit/73bcfa99af61e5202f030510db8954c17cba43cc",
        "buggy_code": "for(i = i >= 0 ? i : -i; i < diffs.size(); i--) {",
        "fixed_code": "for(i = i >= 0 ? i : -i-2; i >= 0; i--) {",
        "patch": "@@ -63,7 +63,7 @@ public BlockInfoContiguous[] findEarlierSnapshotBlocks(int snapshotId) {\n     List<FileDiff> diffs = this.asList();\n     int i = Collections.binarySearch(diffs, snapshotId);\n     BlockInfoContiguous[] blocks = null;\n-    for(i = i >= 0 ? i : -i; i < diffs.size(); i--) {\n+    for(i = i >= 0 ? i : -i-2; i >= 0; i--) {\n       blocks = diffs.get(i).getBlocks();\n       if(blocks != null) {\n         break;"
    },
    {
        "commit_id": "9cedad11d8d2197a54732667a15344983de5c437",
        "commit_message": "Revert \"HADOOP-11602. Fix toUpperCase/toLowerCase to use Locale.ENGLISH. (ozawa)\"\n\nThis reverts commit 946456c6d88780abe0251b098dd771e9e1e93ab3.\n\nConflicts:\n\thadoop-common-project/hadoop-common/CHANGES.txt\n\thadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/QuotaByStorageTypeEntry.java",
        "commit_url": "https://github.com/apache/hadoop/commit/9cedad11d8d2197a54732667a15344983de5c437",
        "buggy_code": ".toUpperCase(Locale.ENGLISH));",
        "fixed_code": ".toUpperCase(Locale.US));",
        "patch": "@@ -617,6 +617,6 @@ private InitMode initMode() {\n     String m = System.getProperty(MS_INIT_MODE_KEY);\n     String m2 = m == null ? System.getenv(MS_INIT_MODE_KEY) : m;\n     return InitMode.valueOf((m2 == null ? InitMode.NORMAL.name() : m2)\n-                            .toUpperCase(Locale.ENGLISH));\n+                            .toUpperCase(Locale.US));\n   }\n }"
    },
    {
        "commit_id": "9cedad11d8d2197a54732667a15344983de5c437",
        "commit_message": "Revert \"HADOOP-11602. Fix toUpperCase/toLowerCase to use Locale.ENGLISH. (ozawa)\"\n\nThis reverts commit 946456c6d88780abe0251b098dd771e9e1e93ab3.\n\nConflicts:\n\thadoop-common-project/hadoop-common/CHANGES.txt\n\thadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/QuotaByStorageTypeEntry.java",
        "commit_url": "https://github.com/apache/hadoop/commit/9cedad11d8d2197a54732667a15344983de5c437",
        "buggy_code": "String[] words = split(s.toLowerCase(Locale.ENGLISH), ESCAPE_CHAR, '_');",
        "fixed_code": "String[] words = split(s.toLowerCase(Locale.US), ESCAPE_CHAR, '_');",
        "patch": "@@ -901,7 +901,7 @@ public static String join(CharSequence separator, String[] strings) {\n    */\n   public static String camelize(String s) {\n     StringBuilder sb = new StringBuilder();\n-    String[] words = split(s.toLowerCase(Locale.ENGLISH), ESCAPE_CHAR, '_');\n+    String[] words = split(s.toLowerCase(Locale.US), ESCAPE_CHAR, '_');\n \n     for (String word : words)\n       sb.append(org.apache.commons.lang.StringUtils.capitalize(word));"
    },
    {
        "commit_id": "9cedad11d8d2197a54732667a15344983de5c437",
        "commit_message": "Revert \"HADOOP-11602. Fix toUpperCase/toLowerCase to use Locale.ENGLISH. (ozawa)\"\n\nThis reverts commit 946456c6d88780abe0251b098dd771e9e1e93ab3.\n\nConflicts:\n\thadoop-common-project/hadoop-common/CHANGES.txt\n\thadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/QuotaByStorageTypeEntry.java",
        "commit_url": "https://github.com/apache/hadoop/commit/9cedad11d8d2197a54732667a15344983de5c437",
        "buggy_code": "String cmd = opts.getCommand().toString().toLowerCase(Locale.ENGLISH);",
        "fixed_code": "String cmd = opts.getCommand().toString().toLowerCase();",
        "patch": "@@ -587,7 +587,7 @@ private int processStartupCommand(CommandLineOpts opts) throws Exception {\n       return 0;\n     }\n     \n-    String cmd = opts.getCommand().toString().toLowerCase(Locale.ENGLISH);\n+    String cmd = opts.getCommand().toString().toLowerCase();\n     \n     int exitCode = 0;\n     try {"
    },
    {
        "commit_id": "9cedad11d8d2197a54732667a15344983de5c437",
        "commit_message": "Revert \"HADOOP-11602. Fix toUpperCase/toLowerCase to use Locale.ENGLISH. (ozawa)\"\n\nThis reverts commit 946456c6d88780abe0251b098dd771e9e1e93ab3.\n\nConflicts:\n\thadoop-common-project/hadoop-common/CHANGES.txt\n\thadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/QuotaByStorageTypeEntry.java",
        "commit_url": "https://github.com/apache/hadoop/commit/9cedad11d8d2197a54732667a15344983de5c437",
        "buggy_code": "toString().toLowerCase(Locale.ENGLISH));",
        "fixed_code": "toString().toLowerCase(Locale.US));",
        "patch": "@@ -227,7 +227,7 @@ public void tasks() {\n       try {\n         String tt = $(TASK_TYPE);\n         tt = tt.isEmpty() ? \"All\" : StringUtils.capitalize(MRApps.taskType(tt).\n-            toString().toLowerCase(Locale.ENGLISH));\n+            toString().toLowerCase(Locale.US));\n         setTitle(join(tt, \" Tasks for \", $(JOB_ID)));\n       } catch (Exception e) {\n         LOG.error(\"Failed to render tasks page with task type : \""
    },
    {
        "commit_id": "9cedad11d8d2197a54732667a15344983de5c437",
        "commit_message": "Revert \"HADOOP-11602. Fix toUpperCase/toLowerCase to use Locale.ENGLISH. (ozawa)\"\n\nThis reverts commit 946456c6d88780abe0251b098dd771e9e1e93ab3.\n\nConflicts:\n\thadoop-common-project/hadoop-common/CHANGES.txt\n\thadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/QuotaByStorageTypeEntry.java",
        "commit_url": "https://github.com/apache/hadoop/commit/9cedad11d8d2197a54732667a15344983de5c437",
        "buggy_code": "String fixed = scheme.toUpperCase(Locale.ENGLISH);",
        "fixed_code": "String fixed = scheme.toUpperCase(Locale.US);",
        "patch": "@@ -227,7 +227,7 @@ else if (counters[ord] == null) {\n   }\n \n   private String checkScheme(String scheme) {\n-    String fixed = scheme.toUpperCase(Locale.ENGLISH);\n+    String fixed = scheme.toUpperCase(Locale.US);\n     String interned = schemes.putIfAbsent(fixed, fixed);\n     if (schemes.size() > MAX_NUM_SCHEMES) {\n       // mistakes or abuses"
    },
    {
        "commit_id": "9cedad11d8d2197a54732667a15344983de5c437",
        "commit_message": "Revert \"HADOOP-11602. Fix toUpperCase/toLowerCase to use Locale.ENGLISH. (ozawa)\"\n\nThis reverts commit 946456c6d88780abe0251b098dd771e9e1e93ab3.\n\nConflicts:\n\thadoop-common-project/hadoop-common/CHANGES.txt\n\thadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/QuotaByStorageTypeEntry.java",
        "commit_url": "https://github.com/apache/hadoop/commit/9cedad11d8d2197a54732667a15344983de5c437",
        "buggy_code": "String lowerOs = OS.toLowerCase(Locale.ENGLISH);",
        "fixed_code": "String lowerOs = OS.toLowerCase();",
        "patch": "@@ -43,7 +43,7 @@ public Environment() throws IOException {\n     // http://lopica.sourceforge.net/os.html\n     String command = null;\n     String OS = System.getProperty(\"os.name\");\n-    String lowerOs = OS.toLowerCase(Locale.ENGLISH);\n+    String lowerOs = OS.toLowerCase();\n     if (OS.indexOf(\"Windows\") > -1) {\n       command = \"cmd /C set\";\n     } else if (lowerOs.indexOf(\"ix\") > -1 || lowerOs.indexOf(\"linux\") > -1"
    },
    {
        "commit_id": "946456c6d88780abe0251b098dd771e9e1e93ab3",
        "commit_message": "HADOOP-11602. Fix toUpperCase/toLowerCase to use Locale.ENGLISH. (ozawa)",
        "commit_url": "https://github.com/apache/hadoop/commit/946456c6d88780abe0251b098dd771e9e1e93ab3",
        "buggy_code": ".toUpperCase(Locale.US));",
        "fixed_code": ".toUpperCase(Locale.ENGLISH));",
        "patch": "@@ -617,6 +617,6 @@ private InitMode initMode() {\n     String m = System.getProperty(MS_INIT_MODE_KEY);\n     String m2 = m == null ? System.getenv(MS_INIT_MODE_KEY) : m;\n     return InitMode.valueOf((m2 == null ? InitMode.NORMAL.name() : m2)\n-                            .toUpperCase(Locale.US));\n+                            .toUpperCase(Locale.ENGLISH));\n   }\n }"
    },
    {
        "commit_id": "946456c6d88780abe0251b098dd771e9e1e93ab3",
        "commit_message": "HADOOP-11602. Fix toUpperCase/toLowerCase to use Locale.ENGLISH. (ozawa)",
        "commit_url": "https://github.com/apache/hadoop/commit/946456c6d88780abe0251b098dd771e9e1e93ab3",
        "buggy_code": "String[] words = split(s.toLowerCase(Locale.US), ESCAPE_CHAR, '_');",
        "fixed_code": "String[] words = split(s.toLowerCase(Locale.ENGLISH), ESCAPE_CHAR, '_');",
        "patch": "@@ -901,7 +901,7 @@ public static String join(CharSequence separator, String[] strings) {\n    */\n   public static String camelize(String s) {\n     StringBuilder sb = new StringBuilder();\n-    String[] words = split(s.toLowerCase(Locale.US), ESCAPE_CHAR, '_');\n+    String[] words = split(s.toLowerCase(Locale.ENGLISH), ESCAPE_CHAR, '_');\n \n     for (String word : words)\n       sb.append(org.apache.commons.lang.StringUtils.capitalize(word));"
    },
    {
        "commit_id": "946456c6d88780abe0251b098dd771e9e1e93ab3",
        "commit_message": "HADOOP-11602. Fix toUpperCase/toLowerCase to use Locale.ENGLISH. (ozawa)",
        "commit_url": "https://github.com/apache/hadoop/commit/946456c6d88780abe0251b098dd771e9e1e93ab3",
        "buggy_code": "String cmd = opts.getCommand().toString().toLowerCase();",
        "fixed_code": "String cmd = opts.getCommand().toString().toLowerCase(Locale.ENGLISH);",
        "patch": "@@ -587,7 +587,7 @@ private int processStartupCommand(CommandLineOpts opts) throws Exception {\n       return 0;\n     }\n     \n-    String cmd = opts.getCommand().toString().toLowerCase();\n+    String cmd = opts.getCommand().toString().toLowerCase(Locale.ENGLISH);\n     \n     int exitCode = 0;\n     try {"
    },
    {
        "commit_id": "946456c6d88780abe0251b098dd771e9e1e93ab3",
        "commit_message": "HADOOP-11602. Fix toUpperCase/toLowerCase to use Locale.ENGLISH. (ozawa)",
        "commit_url": "https://github.com/apache/hadoop/commit/946456c6d88780abe0251b098dd771e9e1e93ab3",
        "buggy_code": "toString().toLowerCase(Locale.US));",
        "fixed_code": "toString().toLowerCase(Locale.ENGLISH));",
        "patch": "@@ -227,7 +227,7 @@ public void tasks() {\n       try {\n         String tt = $(TASK_TYPE);\n         tt = tt.isEmpty() ? \"All\" : StringUtils.capitalize(MRApps.taskType(tt).\n-            toString().toLowerCase(Locale.US));\n+            toString().toLowerCase(Locale.ENGLISH));\n         setTitle(join(tt, \" Tasks for \", $(JOB_ID)));\n       } catch (Exception e) {\n         LOG.error(\"Failed to render tasks page with task type : \""
    },
    {
        "commit_id": "946456c6d88780abe0251b098dd771e9e1e93ab3",
        "commit_message": "HADOOP-11602. Fix toUpperCase/toLowerCase to use Locale.ENGLISH. (ozawa)",
        "commit_url": "https://github.com/apache/hadoop/commit/946456c6d88780abe0251b098dd771e9e1e93ab3",
        "buggy_code": "String fixed = scheme.toUpperCase(Locale.US);",
        "fixed_code": "String fixed = scheme.toUpperCase(Locale.ENGLISH);",
        "patch": "@@ -227,7 +227,7 @@ else if (counters[ord] == null) {\n   }\n \n   private String checkScheme(String scheme) {\n-    String fixed = scheme.toUpperCase(Locale.US);\n+    String fixed = scheme.toUpperCase(Locale.ENGLISH);\n     String interned = schemes.putIfAbsent(fixed, fixed);\n     if (schemes.size() > MAX_NUM_SCHEMES) {\n       // mistakes or abuses"
    },
    {
        "commit_id": "946456c6d88780abe0251b098dd771e9e1e93ab3",
        "commit_message": "HADOOP-11602. Fix toUpperCase/toLowerCase to use Locale.ENGLISH. (ozawa)",
        "commit_url": "https://github.com/apache/hadoop/commit/946456c6d88780abe0251b098dd771e9e1e93ab3",
        "buggy_code": "String lowerOs = OS.toLowerCase();",
        "fixed_code": "String lowerOs = OS.toLowerCase(Locale.ENGLISH);",
        "patch": "@@ -43,7 +43,7 @@ public Environment() throws IOException {\n     // http://lopica.sourceforge.net/os.html\n     String command = null;\n     String OS = System.getProperty(\"os.name\");\n-    String lowerOs = OS.toLowerCase();\n+    String lowerOs = OS.toLowerCase(Locale.ENGLISH);\n     if (OS.indexOf(\"Windows\") > -1) {\n       command = \"cmd /C set\";\n     } else if (lowerOs.indexOf(\"ix\") > -1 || lowerOs.indexOf(\"linux\") > -1"
    },
    {
        "commit_id": "2f0f756b26ea83e142a5b9379fa75862c2fc6ad5",
        "commit_message": "HADOOP-11600. Fix up source codes to be compiled with Guava 17.0. (ozawa)",
        "commit_url": "https://github.com/apache/hadoop/commit/2f0f756b26ea83e142a5b9379fa75862c2fc6ad5",
        "buggy_code": "import com.google.common.io.LimitInputStream;",
        "fixed_code": "import org.apache.hadoop.util.LimitInputStream;",
        "patch": "@@ -19,7 +19,6 @@\n \n import com.google.common.base.Preconditions;\n import com.google.common.collect.Lists;\n-import com.google.common.io.LimitInputStream;\n import org.apache.commons.io.FileUtils;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.permission.PermissionStatus;\n@@ -33,6 +32,7 @@\n import org.apache.hadoop.hdfs.server.namenode.FsImageProto.INodeSection.INode;\n import org.apache.hadoop.hdfs.server.namenode.INodeId;\n import org.apache.hadoop.io.IOUtils;\n+import org.apache.hadoop.util.LimitInputStream;\n import org.apache.hadoop.util.Time;\n import org.fusesource.leveldbjni.JniDBFactory;\n import org.iq80.leveldb.DB;"
    },
    {
        "commit_id": "814afa46efef201cb782072432fc744e1cb9c463",
        "commit_message": "MAPREDUCE-6225. Fix new findbug warnings in hadoop-mapreduce-client-core. Contributed by Varun Saxena",
        "commit_url": "https://github.com/apache/hadoop/commit/814afa46efef201cb782072432fc744e1cb9c463",
        "buggy_code": "if (info == null || ((info != null) && isUnderConstruction(info))) {",
        "fixed_code": "if (info == null || isUnderConstruction(info)) {",
        "patch": "@@ -145,7 +145,7 @@ private IndexInformation readIndexFileToCache(Path indexFileName,\n    */\n   public void removeMap(String mapId) {\n     IndexInformation info = cache.get(mapId);\n-    if (info == null || ((info != null) && isUnderConstruction(info))) {\n+    if (info == null || isUnderConstruction(info)) {\n       return;\n     }\n     info = cache.remove(mapId);"
    },
    {
        "commit_id": "814afa46efef201cb782072432fc744e1cb9c463",
        "commit_message": "MAPREDUCE-6225. Fix new findbug warnings in hadoop-mapreduce-client-core. Contributed by Varun Saxena",
        "commit_url": "https://github.com/apache/hadoop/commit/814afa46efef201cb782072432fc744e1cb9c463",
        "buggy_code": "setTotalLogFileSize(Long.valueOf(propValue));",
        "fixed_code": "setTotalLogFileSize(Long.parseLong(propValue));",
        "patch": "@@ -75,7 +75,7 @@ private synchronized void setOptionsFromSystemProperties() {\n \n     if (maxEvents == null) {\n       String propValue = System.getProperty(LOGSIZE_PROPERTY, \"0\");\n-      setTotalLogFileSize(Long.valueOf(propValue));\n+      setTotalLogFileSize(Long.parseLong(propValue));\n     }\n   }\n   "
    },
    {
        "commit_id": "814afa46efef201cb782072432fc744e1cb9c463",
        "commit_message": "MAPREDUCE-6225. Fix new findbug warnings in hadoop-mapreduce-client-core. Contributed by Varun Saxena",
        "commit_url": "https://github.com/apache/hadoop/commit/814afa46efef201cb782072432fc744e1cb9c463",
        "buggy_code": "Integer fn = new Integer(fieldSpec);",
        "fixed_code": "Integer fn = Integer.valueOf(fieldSpec);",
        "patch": "@@ -90,7 +90,7 @@ private static int extractFields(String[] fieldListSpec,\n       }\n       pos = fieldSpec.indexOf('-');\n       if (pos < 0) {\n-        Integer fn = new Integer(fieldSpec);\n+        Integer fn = Integer.valueOf(fieldSpec);\n         fieldList.add(fn);\n       } else {\n         String start = fieldSpec.substring(0, pos);"
    },
    {
        "commit_id": "814afa46efef201cb782072432fc744e1cb9c463",
        "commit_message": "MAPREDUCE-6225. Fix new findbug warnings in hadoop-mapreduce-client-core. Contributed by Varun Saxena",
        "commit_url": "https://github.com/apache/hadoop/commit/814afa46efef201cb782072432fc744e1cb9c463",
        "buggy_code": "return value == null ? defaultValue : value;",
        "fixed_code": "return value;",
        "patch": "@@ -59,7 +59,7 @@ public static synchronized <T> T getValue(String bundleName, String key,\n     catch (Exception e) {\n       return defaultValue;\n     }\n-    return value == null ? defaultValue : value;\n+    return value;\n   }\n \n   private static String getLookupKey(String key, String suffix) {"
    },
    {
        "commit_id": "c2b185def846f5577a130003a533b9c377b58fab",
        "commit_message": "YARN-3181. FairScheduler: Fix up outdated findbugs issues. (kasha)",
        "commit_url": "https://github.com/apache/hadoop/commit/c2b185def846f5577a130003a533b9c377b58fab",
        "buggy_code": "public synchronized void reloadAllocations() throws IOException,",
        "fixed_code": "public void reloadAllocations() throws IOException,",
        "patch": "@@ -201,7 +201,7 @@ public synchronized void setReloadListener(Listener reloadListener) {\n    * @throws ParserConfigurationException if XML parser is misconfigured.\n    * @throws SAXException if config file is malformed.\n    */\n-  public synchronized void reloadAllocations() throws IOException,\n+  public void reloadAllocations() throws IOException,\n       ParserConfigurationException, SAXException, AllocationConfigurationException {\n     if (allocFile == null) {\n       return;"
    },
    {
        "commit_id": "f80c9888fa0c1a11967560be3c37dfc1e30da2c3",
        "commit_message": "HDFS-7736. Fix typos in dfsadmin/fsck/snapshotDiff usage messages. Contributed by Brahma Reddy Battula.",
        "commit_url": "https://github.com/apache/hadoop/commit/f80c9888fa0c1a11967560be3c37dfc1e30da2c3",
        "buggy_code": "private static final String USAGE = \"Usage: DFSck <path> \"",
        "fixed_code": "private static final String USAGE = \"Usage: hdfs fsck <path> \"",
        "patch": "@@ -75,7 +75,7 @@ public class DFSck extends Configured implements Tool {\n     HdfsConfiguration.init();\n   }\n \n-  private static final String USAGE = \"Usage: DFSck <path> \"\n+  private static final String USAGE = \"Usage: hdfs fsck <path> \"\n       + \"[-list-corruptfileblocks | \"\n       + \"[-move | -delete | -openforwrite] \"\n       + \"[-files [-blocks [-locations | -racks]]]] \""
    },
    {
        "commit_id": "f80c9888fa0c1a11967560be3c37dfc1e30da2c3",
        "commit_message": "HDFS-7736. Fix typos in dfsadmin/fsck/snapshotDiff usage messages. Contributed by Brahma Reddy Battula.",
        "commit_url": "https://github.com/apache/hadoop/commit/f80c9888fa0c1a11967560be3c37dfc1e30da2c3",
        "buggy_code": "String description = \"LsSnapshottableDir: \\n\" +",
        "fixed_code": "String description = \"hdfs lsSnapshottableDir: \\n\" +",
        "patch": "@@ -37,7 +37,7 @@\n public class LsSnapshottableDir extends Configured implements Tool {\n   @Override\n   public int run(String[] argv) throws Exception {\n-    String description = \"LsSnapshottableDir: \\n\" +\n+    String description = \"hdfs lsSnapshottableDir: \\n\" +\n         \"\\tGet the list of snapshottable directories that are owned by the current user.\\n\" +\n         \"\\tReturn all the snapshottable directories if the current user is a super user.\\n\";\n "
    },
    {
        "commit_id": "f80c9888fa0c1a11967560be3c37dfc1e30da2c3",
        "commit_message": "HDFS-7736. Fix typos in dfsadmin/fsck/snapshotDiff usage messages. Contributed by Brahma Reddy Battula.",
        "commit_url": "https://github.com/apache/hadoop/commit/f80c9888fa0c1a11967560be3c37dfc1e30da2c3",
        "buggy_code": "String description = \"SnapshotDiff <snapshotDir> <from> <to>:\\n\" +",
        "fixed_code": "String description = \"hdfs snapshotDiff <snapshotDir> <from> <to>:\\n\" +",
        "patch": "@@ -61,7 +61,7 @@ private static String getSnapshotName(String name) {\n   \n   @Override\n   public int run(String[] argv) throws Exception {\n-    String description = \"SnapshotDiff <snapshotDir> <from> <to>:\\n\" +\n+    String description = \"hdfs snapshotDiff <snapshotDir> <from> <to>:\\n\" +\n     \"\\tGet the difference between two snapshots, \\n\" + \n     \"\\tor between a snapshot and the current tree of a directory.\\n\" +\n     \"\\tFor <from>/<to>, users can use \\\".\\\" to present the current status,\\n\" +"
    },
    {
        "commit_id": "b77ff37686e01b7497d3869fbc62789a5b123c0a",
        "commit_message": "YARN-3149. Fix typo in message for invalid application id. Contributed\nby Bibin A Chundatt.",
        "commit_url": "https://github.com/apache/hadoop/commit/b77ff37686e01b7497d3869fbc62789a5b123c0a",
        "buggy_code": "throw new IllegalArgumentException(\"Invalid AppAttemptId: \"",
        "fixed_code": "throw new IllegalArgumentException(\"Invalid ApplicationId: \"",
        "patch": "@@ -204,7 +204,7 @@ public static ApplicationId toApplicationId(\n     try {\n       return toApplicationId(it);\n     } catch (NumberFormatException n) {\n-      throw new IllegalArgumentException(\"Invalid AppAttemptId: \"\n+      throw new IllegalArgumentException(\"Invalid ApplicationId: \"\n           + appIdStr, n);\n     }\n   }"
    },
    {
        "commit_id": "20660b7a67b7f2815b1e27b98dce2b2682399505",
        "commit_message": "HDFS-7709. Fix findbug warnings in httpfs. Contributed by Rakesh R.",
        "commit_url": "https://github.com/apache/hadoop/commit/20660b7a67b7f2815b1e27b98dce2b2682399505",
        "buggy_code": "if (hadoopConfDir == null) {",
        "fixed_code": "if (!hadoopConfDir.exists()) {",
        "patch": "@@ -177,7 +177,7 @@ protected void init() throws ServiceException {\n \n     String hadoopConfDirProp = getServiceConfig().get(HADOOP_CONF_DIR, getServer().getConfigDir());\n     File hadoopConfDir = new File(hadoopConfDirProp).getAbsoluteFile();\n-    if (hadoopConfDir == null) {\n+    if (!hadoopConfDir.exists()) {\n       hadoopConfDir = new File(getServer().getConfigDir()).getAbsoluteFile();\n     }\n     if (!hadoopConfDir.exists()) {"
    },
    {
        "commit_id": "34fe11c987730932f99dec6eb458a22624eb075b",
        "commit_message": "MAPREDUCE-6243. Fix findbugs warnings in hadoop-rumen. Contributed by Masatake Iwasaki.",
        "commit_url": "https://github.com/apache/hadoop/commit/34fe11c987730932f99dec6eb458a22624eb075b",
        "buggy_code": "return input != null;",
        "fixed_code": "return true;",
        "patch": "@@ -559,7 +559,7 @@ private boolean setNextDirectoryInputStream() throws FileNotFoundException,\n     input =\n         maybeUncompressedPath(new Path(inputDirectoryPath, currentFileName));\n \n-    return input != null;\n+    return true;\n   }\n \n   private String readInputLine() throws IOException {"
    },
    {
        "commit_id": "34fe11c987730932f99dec6eb458a22624eb075b",
        "commit_message": "MAPREDUCE-6243. Fix findbugs warnings in hadoop-rumen. Contributed by Masatake Iwasaki.",
        "commit_url": "https://github.com/apache/hadoop/commit/34fe11c987730932f99dec6eb458a22624eb075b",
        "buggy_code": "if (finishTime != null && \"success\".equalsIgnoreCase(status)) {",
        "fixed_code": "if (\"success\".equalsIgnoreCase(status)) {",
        "patch": "@@ -67,7 +67,7 @@ HistoryEvent maybeEmitEvent(ParsedLine line, String taskAttemptIDName,\n         MapAttempt20LineHistoryEventEmitter that =\n             (MapAttempt20LineHistoryEventEmitter) thatg;\n \n-        if (finishTime != null && \"success\".equalsIgnoreCase(status)) {\n+        if (\"success\".equalsIgnoreCase(status)) {\n           return new MapAttemptFinishedEvent\n             (taskAttemptID,\n               that.originalTaskType, status,"
    },
    {
        "commit_id": "34fe11c987730932f99dec6eb458a22624eb075b",
        "commit_message": "MAPREDUCE-6243. Fix findbugs warnings in hadoop-rumen. Contributed by Masatake Iwasaki.",
        "commit_url": "https://github.com/apache/hadoop/commit/34fe11c987730932f99dec6eb458a22624eb075b",
        "buggy_code": "if (finishTime != null && shuffleFinish != null && sortFinish != null",
        "fixed_code": "if (shuffleFinish != null && sortFinish != null",
        "patch": "@@ -66,7 +66,7 @@ HistoryEvent maybeEmitEvent(ParsedLine line, String taskAttemptIDName,\n         String shuffleFinish = line.get(\"SHUFFLE_FINISHED\");\n         String sortFinish = line.get(\"SORT_FINISHED\");\n \n-        if (finishTime != null && shuffleFinish != null && sortFinish != null\n+        if (shuffleFinish != null && sortFinish != null\n             && \"success\".equalsIgnoreCase(status)) {\n           ReduceAttempt20LineHistoryEventEmitter that =\n               (ReduceAttempt20LineHistoryEventEmitter) thatg;"
    },
    {
        "commit_id": "26dee1486b70237a2a47f910472e9aa81ffad349",
        "commit_message": "YARN-3058. Fix error message of tokens' activation delay configuration. Contributed by Yi Liu.",
        "commit_url": "https://github.com/apache/hadoop/commit/26dee1486b70237a2a47f910472e9aa81ffad349",
        "buggy_code": "+ \" should be more than 2 X \"",
        "fixed_code": "+ \" should be more than 3 X \"",
        "patch": "@@ -96,7 +96,7 @@ public AMRMTokenSecretManager(Configuration conf, RMContext rmContext) {\n     if (rollingInterval <= activationDelay * 2) {\n       throw new IllegalArgumentException(\n           YarnConfiguration.RM_AMRM_TOKEN_MASTER_KEY_ROLLING_INTERVAL_SECS\n-              + \" should be more than 2 X \"\n+              + \" should be more than 3 X \"\n               + YarnConfiguration.RM_AM_EXPIRY_INTERVAL_MS);\n     }\n   }"
    },
    {
        "commit_id": "26dee1486b70237a2a47f910472e9aa81ffad349",
        "commit_message": "YARN-3058. Fix error message of tokens' activation delay configuration. Contributed by Yi Liu.",
        "commit_url": "https://github.com/apache/hadoop/commit/26dee1486b70237a2a47f910472e9aa81ffad349",
        "buggy_code": "+ \" should be more than 2 X \"",
        "fixed_code": "+ \" should be more than 3 X \"",
        "patch": "@@ -78,7 +78,7 @@ public NMTokenSecretManagerInRM(Configuration conf) {\n     if (rollingInterval <= activationDelay * 2) {\n       throw new IllegalArgumentException(\n           YarnConfiguration.RM_NMTOKEN_MASTER_KEY_ROLLING_INTERVAL_SECS\n-              + \" should be more than 2 X \"\n+              + \" should be more than 3 X \"\n               + YarnConfiguration.RM_NM_EXPIRY_INTERVAL_MS);\n     }\n     appAttemptToNodeKeyMap ="
    },
    {
        "commit_id": "26c2de36e2dd1b2ddedc155e49fb2ec31366d5f8",
        "commit_message": "HADOOP-11432. Fix SymlinkBaseTest#testCreateLinkUsingPartQualPath2. (Liang Xie via gera)",
        "commit_url": "https://github.com/apache/hadoop/commit/26c2de36e2dd1b2ddedc155e49fb2ec31366d5f8",
        "buggy_code": "\"No AbstractFileSystem configured for scheme: null\", e);",
        "fixed_code": "AbstractFileSystem.NO_ABSTRACT_FS_ERROR, e);",
        "patch": "@@ -578,7 +578,7 @@ public void testCreateLinkUsingPartQualPath2() throws IOException {\n       // Expected\n       if (wrapper instanceof FileContextTestWrapper) {\n         GenericTestUtils.assertExceptionContains(\n-            \"No AbstractFileSystem configured for scheme: null\", e);\n+            AbstractFileSystem.NO_ABSTRACT_FS_ERROR, e);\n       } else if (wrapper instanceof FileSystemTestWrapper) {\n         assertEquals(\"No FileSystem for scheme: null\", e.getMessage());\n       }"
    },
    {
        "commit_id": "b7f4a3156c0f5c600816c469637237ba6c9b330c",
        "commit_message": "HDFS-7496. Fix FsVolume removal race conditions on the DataNode by reference-counting the volume instances (lei via cmccabe)",
        "commit_url": "https://github.com/apache/hadoop/commit/b7f4a3156c0f5c600816c469637237ba6c9b330c",
        "buggy_code": "public synchronized ReplicaInPipelineInterface createRbw(",
        "fixed_code": "public synchronized ReplicaHandler createRbw(",
        "patch": "@@ -97,7 +97,7 @@ public FsDatasetChecker(DataStorage storage, Configuration conf) {\n      * correctly propagate the hint to FsDatasetSpi.\n      */\n     @Override\n-    public synchronized ReplicaInPipelineInterface createRbw(\n+    public synchronized ReplicaHandler createRbw(\n         StorageType storageType, ExtendedBlock b, boolean allowLazyPersist)\n         throws IOException {\n       assertThat(b.getLocalBlock().getNumBytes(), is(EXPECTED_BLOCK_LENGTH));"
    },
    {
        "commit_id": "b7f4a3156c0f5c600816c469637237ba6c9b330c",
        "commit_message": "HDFS-7496. Fix FsVolume removal race conditions on the DataNode by reference-counting the volume instances (lei via cmccabe)",
        "commit_url": "https://github.com/apache/hadoop/commit/b7f4a3156c0f5c600816c469637237ba6c9b330c",
        "buggy_code": "StorageType.DEFAULT, block, false);",
        "fixed_code": "StorageType.DEFAULT, block, false).getReplica();",
        "patch": "@@ -562,7 +562,7 @@ public void testNotMatchedReplicaID() throws IOException {\n       LOG.debug(\"Running \" + GenericTestUtils.getMethodName());\n     }\n     ReplicaInPipelineInterface replicaInfo = dn.data.createRbw(\n-        StorageType.DEFAULT, block, false);\n+        StorageType.DEFAULT, block, false).getReplica();\n     ReplicaOutputStreams streams = null;\n     try {\n       streams = replicaInfo.createStreams(true,"
    },
    {
        "commit_id": "b7f4a3156c0f5c600816c469637237ba6c9b330c",
        "commit_message": "HDFS-7496. Fix FsVolume removal race conditions on the DataNode by reference-counting the volume instances (lei via cmccabe)",
        "commit_url": "https://github.com/apache/hadoop/commit/b7f4a3156c0f5c600816c469637237ba6c9b330c",
        "buggy_code": "StorageType.DEFAULT, b, false);",
        "fixed_code": "StorageType.DEFAULT, b, false).getReplica();",
        "patch": "@@ -67,7 +67,7 @@ int addSomeBlocks(SimulatedFSDataset fsdataset, int startingBlockId)\n       // we pass expected len as zero, - fsdataset should use the sizeof actual\n       // data written\n       ReplicaInPipelineInterface bInfo = fsdataset.createRbw(\n-          StorageType.DEFAULT, b, false);\n+          StorageType.DEFAULT, b, false).getReplica();\n       ReplicaOutputStreams out = bInfo.createStreams(true,\n           DataChecksum.newDataChecksum(DataChecksum.Type.CRC32, 512));\n       try {"
    },
    {
        "commit_id": "4a4450836c8972480b9387b5e31bab57ae2b5baa",
        "commit_message": "HDFS-5631. Change BlockMetadataHeader.readHeader(..), ChunkChecksum class and constructor to public; and fix FsDatasetSpi to use generic type instead of FsVolumeImpl.  Contributed by David Powell and Joe Pallas",
        "commit_url": "https://github.com/apache/hadoop/commit/4a4450836c8972480b9387b5e31bab57ae2b5baa",
        "buggy_code": "static BlockMetadataHeader readHeader(RandomAccessFile raf) throws IOException {",
        "fixed_code": "public static BlockMetadataHeader readHeader(RandomAccessFile raf) throws IOException {",
        "patch": "@@ -162,7 +162,7 @@ public static BlockMetadataHeader readHeader(File file) throws IOException {\n    * The current file position will be altered by this method.\n    * If an error occurs, the file is <em>not</em> closed.\n    */\n-  static BlockMetadataHeader readHeader(RandomAccessFile raf) throws IOException {\n+  public static BlockMetadataHeader readHeader(RandomAccessFile raf) throws IOException {\n     byte[] buf = new byte[getHeaderSize()];\n     raf.seek(0);\n     raf.readFully(buf, 0, buf.length);"
    },
    {
        "commit_id": "4a4450836c8972480b9387b5e31bab57ae2b5baa",
        "commit_message": "HDFS-5631. Change BlockMetadataHeader.readHeader(..), ChunkChecksum class and constructor to public; and fix FsDatasetSpi to use generic type instead of FsVolumeImpl.  Contributed by David Powell and Joe Pallas",
        "commit_url": "https://github.com/apache/hadoop/commit/4a4450836c8972480b9387b5e31bab57ae2b5baa",
        "buggy_code": "long creationTime, File[] savedFiles, FsVolumeImpl targetVolume);",
        "fixed_code": "long creationTime, File[] savedFiles, V targetVolume);",
        "patch": "@@ -503,7 +503,7 @@ public void submitBackgroundSyncFileRangeRequest(final ExtendedBlock block,\n    * Callback from RamDiskAsyncLazyPersistService upon async lazy persist task end\n    */\n    public void onCompleteLazyPersist(String bpId, long blockId,\n-      long creationTime, File[] savedFiles, FsVolumeImpl targetVolume);\n+      long creationTime, File[] savedFiles, V targetVolume);\n \n    /**\n     * Callback from RamDiskAsyncLazyPersistService upon async lazy persist task fail"
    },
    {
        "commit_id": "4a4450836c8972480b9387b5e31bab57ae2b5baa",
        "commit_message": "HDFS-5631. Change BlockMetadataHeader.readHeader(..), ChunkChecksum class and constructor to public; and fix FsDatasetSpi to use generic type instead of FsVolumeImpl.  Contributed by David Powell and Joe Pallas",
        "commit_url": "https://github.com/apache/hadoop/commit/4a4450836c8972480b9387b5e31bab57ae2b5baa",
        "buggy_code": "long creationTime, File[] savedFiles, FsVolumeImpl targetVolume) {",
        "fixed_code": "long creationTime, File[] savedFiles, FsVolumeSpi targetVolume) {",
        "patch": "@@ -1253,7 +1253,7 @@ public void submitBackgroundSyncFileRangeRequest(ExtendedBlock block,\n \n   @Override\n   public void onCompleteLazyPersist(String bpId, long blockId,\n-      long creationTime, File[] savedFiles, FsVolumeImpl targetVolume) {\n+      long creationTime, File[] savedFiles, FsVolumeSpi targetVolume) {\n     throw new UnsupportedOperationException();\n   }\n "
    },
    {
        "commit_id": "e843a0a8cee5c704a5d28cf14b5a4050094d341b",
        "commit_message": "HDFS-7637. Fix the check condition for reserved path. Contributed by Yi Liu.",
        "commit_url": "https://github.com/apache/hadoop/commit/e843a0a8cee5c704a5d28cf14b5a4050094d341b",
        "buggy_code": "return src.startsWith(DOT_RESERVED_PATH_PREFIX);",
        "fixed_code": "return src.startsWith(DOT_RESERVED_PATH_PREFIX + Path.SEPARATOR);",
        "patch": "@@ -1311,7 +1311,7 @@ public static boolean isReservedName(INode inode) {\n \n   /** Check if a given path is reserved */\n   public static boolean isReservedName(String src) {\n-    return src.startsWith(DOT_RESERVED_PATH_PREFIX);\n+    return src.startsWith(DOT_RESERVED_PATH_PREFIX + Path.SEPARATOR);\n   }\n \n   static boolean isReservedRawName(String src) {"
    },
    {
        "commit_id": "60cbcff2f7363e5cc386284981cac67abc965ee7",
        "commit_message": "HDFS-7606. Fix potential NPE in INodeFile.getBlocks(). Contributed by Byron Wong.",
        "commit_url": "https://github.com/apache/hadoop/commit/60cbcff2f7363e5cc386284981cac67abc965ee7",
        "buggy_code": "snapshotBlocks = getDiffs().findLaterSnapshotBlocks(diff.getSnapshotId());",
        "fixed_code": "snapshotBlocks = getDiffs().findLaterSnapshotBlocks(snapshot);",
        "patch": "@@ -432,7 +432,7 @@ public BlockInfo[] getBlocks(int snapshot) {\n       return snapshotBlocks;\n     // Blocks are not in the current snapshot\n     // Find next snapshot with blocks present or return current file blocks\n-    snapshotBlocks = getDiffs().findLaterSnapshotBlocks(diff.getSnapshotId());\n+    snapshotBlocks = getDiffs().findLaterSnapshotBlocks(snapshot);\n     return (snapshotBlocks == null) ? getBlocks() : snapshotBlocks;\n   }\n "
    },
    {
        "commit_id": "c4cba6165a3afbf4f1f8ff6b7f11286772d70d6f",
        "commit_message": "HADOOP-11465. Fix findbugs warnings in hadoop-gridmix. (Contributed by Varun Saxena)",
        "commit_url": "https://github.com/apache/hadoop/commit/c4cba6165a3afbf4f1f8ff6b7f11286772d70d6f",
        "buggy_code": "fileSize = Long.valueOf(parts[parts.length - 1]);",
        "fixed_code": "fileSize = Long.parseLong(parts[parts.length - 1]);",
        "patch": "@@ -124,7 +124,7 @@ long validateFileNameFormat(Path path) throws FileNotFoundException {\n     } else {\n       String[] parts = path.toUri().getPath().split(\"\\\\.\");\n       try {\n-        fileSize = Long.valueOf(parts[parts.length - 1]);\n+        fileSize = Long.parseLong(parts[parts.length - 1]);\n         valid = (fileSize >= 0);\n       } catch (NumberFormatException e) {\n         valid = false;"
    },
    {
        "commit_id": "9803ae374f69942aec82ec6eeeb9722523a1ade0",
        "commit_message": "HADOOP-11459. Fix recent findbugs in ActiveStandbyElector, NetUtils and ShellBasedIdMapping (Contributed by Vinayakumar B)",
        "commit_url": "https://github.com/apache/hadoop/commit/9803ae374f69942aec82ec6eeeb9722523a1ade0",
        "buggy_code": "canonicalizedHostCache.put(host, fqHost);",
        "fixed_code": "canonicalizedHostCache.putIfAbsent(host, fqHost);",
        "patch": "@@ -288,7 +288,7 @@ private static String canonicalizeHost(String host) {\n       try {\n         fqHost = SecurityUtil.getByName(host).getHostName();\n         // slight race condition, but won't hurt\n-        canonicalizedHostCache.put(host, fqHost);\n+        canonicalizedHostCache.putIfAbsent(host, fqHost);\n       } catch (UnknownHostException e) {\n         fqHost = host;\n       }"
    },
    {
        "commit_id": "9803ae374f69942aec82ec6eeeb9722523a1ade0",
        "commit_message": "HADOOP-11459. Fix recent findbugs in ActiveStandbyElector, NetUtils and ShellBasedIdMapping (Contributed by Vinayakumar B)",
        "commit_url": "https://github.com/apache/hadoop/commit/9803ae374f69942aec82ec6eeeb9722523a1ade0",
        "buggy_code": "private void initStaticMapping() throws IOException {",
        "fixed_code": "private synchronized void initStaticMapping() throws IOException {",
        "patch": "@@ -290,7 +290,7 @@ private static boolean isInteger(final String s) {\n     return true;\n   }\n \n-  private void initStaticMapping() throws IOException {\n+  private synchronized void initStaticMapping() throws IOException {\n     staticMapping = new StaticMapping(\n         new HashMap<Integer, Integer>(), new HashMap<Integer, Integer>());\n     if (staticMappingFile.exists()) {"
    },
    {
        "commit_id": "399d25884a99f3e0b2ef65eaf9f3149d0d523f13",
        "commit_message": "HADOOP-11448. Fix findbugs warnings in FileBasedIPList. (ozawa)",
        "commit_url": "https://github.com/apache/hadoop/commit/399d25884a99f3e0b2ef65eaf9f3149d0d523f13",
        "buggy_code": "String[] lines = new String[0];",
        "fixed_code": "String[] lines;",
        "patch": "@@ -50,7 +50,7 @@ public class FileBasedIPList implements IPList {\n \n   public FileBasedIPList(String fileName) {\n     this.fileName = fileName;\n-    String[] lines = new String[0];\n+    String[] lines;\n     try {\n       lines = readLines(fileName);\n     } catch (IOException e) {"
    },
    {
        "commit_id": "a696fbb001b946ae75f3b8e962839c2fd3decfa1",
        "commit_message": "YARN-2939. Fix new findbugs warnings in hadoop-yarn-common. (Li Lu via junping_du)",
        "commit_url": "https://github.com/apache/hadoop/commit/a696fbb001b946ae75f3b8e962839c2fd3decfa1",
        "buggy_code": "return Integer.valueOf(getPriority()).toString();",
        "fixed_code": "return Integer.toString(getPriority());",
        "patch": "@@ -68,7 +68,7 @@ public void setPriority(int priority) {\n   \n   @Override\n   public String toString() {\n-    return Integer.valueOf(getPriority()).toString();\n+    return Integer.toString(getPriority());\n   }\n \n }  "
    },
    {
        "commit_id": "a696fbb001b946ae75f3b8e962839c2fd3decfa1",
        "commit_message": "YARN-2939. Fix new findbugs warnings in hadoop-yarn-common. (Li Lu via junping_du)",
        "commit_url": "https://github.com/apache/hadoop/commit/a696fbb001b946ae75f3b8e962839c2fd3decfa1",
        "buggy_code": "System.err.printf(\"Usage: %s <GraphName> <class[,class[,...]]> <OutputFile>\\n\",",
        "fixed_code": "System.err.printf(\"Usage: %s <GraphName> <class[,class[,...]]> <OutputFile>%n\",",
        "patch": "@@ -56,7 +56,7 @@ public static Graph getGraphFromClasses(String graphName, List<String> classes)\n \n   public static void main(String [] args) throws Exception {\n     if (args.length < 3) {\n-      System.err.printf(\"Usage: %s <GraphName> <class[,class[,...]]> <OutputFile>\\n\",\n+      System.err.printf(\"Usage: %s <GraphName> <class[,class[,...]]> <OutputFile>%n\",\n           VisualizeStateMachine.class.getName());\n       System.exit(1);\n     }"
    },
    {
        "commit_id": "a4876c130f1627e59ef055e586640d1933fc49af",
        "commit_message": "HDFS-7552. Change FsVolumeList toString() to fix TestDataNodeVolumeFailureToleration (Liang Xie via Colin P. McCabe)",
        "commit_url": "https://github.com/apache/hadoop/commit/a4876c130f1627e59ef055e586640d1933fc49af",
        "buggy_code": "return volumes.toString();",
        "fixed_code": "return Arrays.toString(volumes.get());",
        "patch": "@@ -213,7 +213,7 @@ List<FsVolumeImpl> checkDirs() {\n \n   @Override\n   public String toString() {\n-    return volumes.toString();\n+    return Arrays.toString(volumes.get());\n   }\n \n   /**"
    },
    {
        "commit_id": "b9f6d0c956f0278c8b9b83e05b523a442a730ebb",
        "commit_message": "HDFS-7515. Fix new findbugs warnings in hadoop-hdfs. Contributed by Haohui Mai.",
        "commit_url": "https://github.com/apache/hadoop/commit/b9f6d0c956f0278c8b9b83e05b523a442a730ebb",
        "buggy_code": "if (res != null && !deletionHookAdded) {",
        "fixed_code": "if (!deletionHookAdded) {",
        "patch": "@@ -727,7 +727,7 @@ FileLock tryLock() throws IOException {\n         file.close();\n         throw e;\n       }\n-      if (res != null && !deletionHookAdded) {\n+      if (!deletionHookAdded) {\n         // If the file existed prior to our startup, we didn't\n         // call deleteOnExit above. But since we successfully locked\n         // the dir, we can take care of cleaning it up."
    },
    {
        "commit_id": "b9f6d0c956f0278c8b9b83e05b523a442a730ebb",
        "commit_message": "HDFS-7515. Fix new findbugs warnings in hadoop-hdfs. Contributed by Haohui Mai.",
        "commit_url": "https://github.com/apache/hadoop/commit/b9f6d0c956f0278c8b9b83e05b523a442a730ebb",
        "buggy_code": "errorMsgBuilder.append(String.format(\"Failed to remove %s: %s\\n\",",
        "fixed_code": "errorMsgBuilder.append(String.format(\"Failed to remove %s: %s%n\",",
        "patch": "@@ -425,7 +425,7 @@ synchronized void removeVolumes(Collection<StorageLocation> locations)\n           LOG.warn(String.format(\n             \"I/O error attempting to unlock storage directory %s.\",\n             sd.getRoot()), e);\n-          errorMsgBuilder.append(String.format(\"Failed to remove %s: %s\\n\",\n+          errorMsgBuilder.append(String.format(\"Failed to remove %s: %s%n\",\n               sd.getRoot(), e.getMessage()));\n         }\n       }"
    },
    {
        "commit_id": "b9f6d0c956f0278c8b9b83e05b523a442a730ebb",
        "commit_message": "HDFS-7515. Fix new findbugs warnings in hadoop-hdfs. Contributed by Haohui Mai.",
        "commit_url": "https://github.com/apache/hadoop/commit/b9f6d0c956f0278c8b9b83e05b523a442a730ebb",
        "buggy_code": "long startTxId = Long.valueOf(staleInprogressEditsMatch.group(1));",
        "fixed_code": "long startTxId = Long.parseLong(staleInprogressEditsMatch.group(1));",
        "patch": "@@ -300,7 +300,7 @@ private static List<EditLogFile> matchEditLogs(File[] filesInStorage,\n             .matcher(name);\n         if (staleInprogressEditsMatch.matches()) {\n           try {\n-            long startTxId = Long.valueOf(staleInprogressEditsMatch.group(1));\n+            long startTxId = Long.parseLong(staleInprogressEditsMatch.group(1));\n             ret.add(new EditLogFile(f, startTxId, HdfsConstants.INVALID_TXID,\n                 true));\n             continue;"
    },
    {
        "commit_id": "b9f6d0c956f0278c8b9b83e05b523a442a730ebb",
        "commit_message": "HDFS-7515. Fix new findbugs warnings in hadoop-hdfs. Contributed by Haohui Mai.",
        "commit_url": "https://github.com/apache/hadoop/commit/b9f6d0c956f0278c8b9b83e05b523a442a730ebb",
        "buggy_code": "if (xAttrs == null || xAttrs.isEmpty()) {",
        "fixed_code": "if (xAttrs.isEmpty()) {",
        "patch": "@@ -100,7 +100,7 @@ static void checkPermissionForApi(FSPermissionChecker pc,\n   static List<XAttr> filterXAttrsForApi(FSPermissionChecker pc,\n       List<XAttr> xAttrs, boolean isRawPath) {\n     assert xAttrs != null : \"xAttrs can not be null\";\n-    if (xAttrs == null || xAttrs.isEmpty()) {\n+    if (xAttrs.isEmpty()) {\n       return xAttrs;\n     }\n     "
    },
    {
        "commit_id": "b9f6d0c956f0278c8b9b83e05b523a442a730ebb",
        "commit_message": "HDFS-7515. Fix new findbugs warnings in hadoop-hdfs. Contributed by Haohui Mai.",
        "commit_url": "https://github.com/apache/hadoop/commit/b9f6d0c956f0278c8b9b83e05b523a442a730ebb",
        "buggy_code": "out.printf(\"Change property %s\\n\\tFrom: \\\"%s\\\"\\n\\tTo: \\\"%s\\\"\\n\",",
        "fixed_code": "out.printf(\"Change property %s%n\\tFrom: \\\"%s\\\"%n\\tTo: \\\"%s\\\"%n\",",
        "patch": "@@ -1476,7 +1476,7 @@ int getReconfigurationStatus(String nodeType, String address,\n           } else {\n             out.print(\"FAILED: \");\n           }\n-          out.printf(\"Change property %s\\n\\tFrom: \\\"%s\\\"\\n\\tTo: \\\"%s\\\"\\n\",\n+          out.printf(\"Change property %s%n\\tFrom: \\\"%s\\\"%n\\tTo: \\\"%s\\\"%n\",\n               result.getKey().prop, result.getKey().oldVal,\n               result.getKey().newVal);\n           if (result.getValue().isPresent()) {"
    },
    {
        "commit_id": "b9f6d0c956f0278c8b9b83e05b523a442a730ebb",
        "commit_message": "HDFS-7515. Fix new findbugs warnings in hadoop-hdfs. Contributed by Haohui Mai.",
        "commit_url": "https://github.com/apache/hadoop/commit/b9f6d0c956f0278c8b9b83e05b523a442a730ebb",
        "buggy_code": "fileSize += Long.valueOf(value);",
        "fixed_code": "fileSize += Long.parseLong(value);",
        "patch": "@@ -144,7 +144,7 @@ void visit(ImageElement element, String value) throws IOException {\n     \n     // Special case of file size, which is sum of the num bytes in each block\n     if(element == ImageElement.NUM_BYTES)\n-      fileSize += Long.valueOf(value);\n+      fileSize += Long.parseLong(value);\n     \n     if(elements.containsKey(element) && element != ImageElement.NUM_BYTES)\n       elements.put(element, value);"
    },
    {
        "commit_id": "bbd6a3277678a60d472e76a207f25a916220946c",
        "commit_message": "HADOOP-10482. Fix various findbugs warnings in hadoop-common. Contributed by Haohui Mai.",
        "commit_url": "https://github.com/apache/hadoop/commit/bbd6a3277678a60d472e76a207f25a916220946c",
        "buggy_code": "long randomPosition = Math.abs(r.nextLong()) % totalAvailable;",
        "fixed_code": "long randomPosition = (r.nextLong() >>> 1) % totalAvailable;",
        "patch": "@@ -372,7 +372,7 @@ public synchronized Path getLocalPathForWrite(String pathStr, long size,\n         // Keep rolling the wheel till we get a valid path\n         Random r = new java.util.Random();\n         while (numDirsSearched < numDirs && returnPath == null) {\n-          long randomPosition = Math.abs(r.nextLong()) % totalAvailable;\n+          long randomPosition = (r.nextLong() >>> 1) % totalAvailable;\n           int dir = 0;\n           while (randomPosition > availableOnDisk[dir]) {\n             randomPosition -= availableOnDisk[dir];"
    },
    {
        "commit_id": "bbd6a3277678a60d472e76a207f25a916220946c",
        "commit_message": "HADOOP-10482. Fix various findbugs warnings in hadoop-common. Contributed by Haohui Mai.",
        "commit_url": "https://github.com/apache/hadoop/commit/bbd6a3277678a60d472e76a207f25a916220946c",
        "buggy_code": "protected static final SimpleDateFormat dateFormat =",
        "fixed_code": "protected final SimpleDateFormat dateFormat =",
        "patch": "@@ -57,7 +57,7 @@ public static void registerCommands(CommandFactory factory) {\n \t\t  \n   \n \n-  protected static final SimpleDateFormat dateFormat = \n+  protected final SimpleDateFormat dateFormat =\n     new SimpleDateFormat(\"yyyy-MM-dd HH:mm\");\n \n   protected int maxRepl = 3, maxLen = 10, maxOwner = 0, maxGroup = 0;"
    },
    {
        "commit_id": "bbd6a3277678a60d472e76a207f25a916220946c",
        "commit_message": "HADOOP-10482. Fix various findbugs warnings in hadoop-common. Contributed by Haohui Mai.",
        "commit_url": "https://github.com/apache/hadoop/commit/bbd6a3277678a60d472e76a207f25a916220946c",
        "buggy_code": "if (in == null || decompressor == null) {",
        "fixed_code": "if (decompressor == null) {",
        "patch": "@@ -40,7 +40,7 @@ public DecompressorStream(InputStream in, Decompressor decompressor,\n   throws IOException {\n     super(in);\n \n-    if (in == null || decompressor == null) {\n+    if (decompressor == null) {\n       throw new NullPointerException();\n     } else if (bufferSize <= 0) {\n       throw new IllegalArgumentException(\"Illegal bufferSize\");"
    },
    {
        "commit_id": "bbd6a3277678a60d472e76a207f25a916220946c",
        "commit_message": "HADOOP-10482. Fix various findbugs warnings in hadoop-common. Contributed by Haohui Mai.",
        "commit_url": "https://github.com/apache/hadoop/commit/bbd6a3277678a60d472e76a207f25a916220946c",
        "buggy_code": "nKids = Integer.valueOf(sKids);",
        "fixed_code": "nKids = Integer.parseInt(sKids);",
        "patch": "@@ -55,7 +55,7 @@ public void init(String contextName, ContextFactory factory) {\n     int nKids;\n     try {\n       String sKids = getAttribute(ARITY_LABEL);\n-      nKids = Integer.valueOf(sKids);\n+      nKids = Integer.parseInt(sKids);\n     } catch (Exception e) {\n       LOG.error(\"Unable to initialize composite metric \" + contextName +\n                 \": could not init arity\", e);"
    },
    {
        "commit_id": "d777a1e4ca8e7cf0ce8967f79dd475468906c733",
        "commit_message": "HADOOP-11369. Fix new findbugs warnings in hadoop-mapreduce-client, non-core directories. Contributed by Li Lu.",
        "commit_url": "https://github.com/apache/hadoop/commit/d777a1e4ca8e7cf0ce8967f79dd475468906c733",
        "buggy_code": "} else if (old != null && !old.isMovePending()) {",
        "fixed_code": "} else if (!old.isMovePending()) {",
        "patch": "@@ -848,7 +848,7 @@ public void run() {\n             }\n           });\n         }\n-      } else if (old != null && !old.isMovePending()) {\n+      } else if (!old.isMovePending()) {\n         //This is a duplicate so just delete it\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Duplicate: deleting\");"
    },
    {
        "commit_id": "3c72f54ef581b4f3e2eb84e1e24e459c38d3f769",
        "commit_message": "YARN-2461. Fix PROCFS_USE_SMAPS_BASED_RSS_ENABLED property in YarnConfiguration. (rchiang via rkanter)",
        "commit_url": "https://github.com/apache/hadoop/commit/3c72f54ef581b4f3e2eb84e1e24e459c38d3f769",
        "buggy_code": "\".container-monitor.procfs-tree.smaps-based-rss.enabled\";",
        "fixed_code": "\"container-monitor.procfs-tree.smaps-based-rss.enabled\";",
        "patch": "@@ -819,7 +819,7 @@ private static void addDeprecatedKeys() {\n   public static final String NM_CONTAINER_MON_PROCESS_TREE =\n     NM_PREFIX + \"container-monitor.process-tree.class\";\n   public static final String PROCFS_USE_SMAPS_BASED_RSS_ENABLED = NM_PREFIX +\n-      \".container-monitor.procfs-tree.smaps-based-rss.enabled\";\n+      \"container-monitor.procfs-tree.smaps-based-rss.enabled\";\n   public static final boolean DEFAULT_PROCFS_USE_SMAPS_BASED_RSS_ENABLED =\n       false;\n   "
    },
    {
        "commit_id": "f6452eb2592a9350bc3f6ce1e354ea55b275ff83",
        "commit_message": "HDFS-7472. Fix typo in message of ReplicaNotFoundException. Contributed by Masatake Iwasaki.",
        "commit_url": "https://github.com/apache/hadoop/commit/f6452eb2592a9350bc3f6ce1e354ea55b275ff83",
        "buggy_code": "\"Cannot append to a replica with unexpeted generation stamp \";",
        "fixed_code": "\"Cannot append to a replica with unexpected generation stamp \";",
        "patch": "@@ -37,7 +37,7 @@ public class ReplicaNotFoundException extends IOException {\n   public final static String NON_EXISTENT_REPLICA =\n     \"Cannot append to a non-existent replica \";\n   public final static String UNEXPECTED_GS_REPLICA =\n-    \"Cannot append to a replica with unexpeted generation stamp \";\n+    \"Cannot append to a replica with unexpected generation stamp \";\n \n   public ReplicaNotFoundException() {\n     super();"
    },
    {
        "commit_id": "392c3aaea8e8f156b76e418157fa347256283c56",
        "commit_message": "YARN-2894. Fixed a bug regarding application view acl when RM fails over. Contributed by Rohith Sharmaks",
        "commit_url": "https://github.com/apache/hadoop/commit/392c3aaea8e8f156b76e418157fa347256283c56",
        "buggy_code": "return new ClusterMetricsInfo(this.rm, this.rm.getRMContext());",
        "fixed_code": "return new ClusterMetricsInfo(this.rm);",
        "patch": "@@ -205,7 +205,7 @@ public ClusterInfo getClusterInfo() {\n   @Produces({ MediaType.APPLICATION_JSON, MediaType.APPLICATION_XML })\n   public ClusterMetricsInfo getClusterMetricsInfo() {\n     init();\n-    return new ClusterMetricsInfo(this.rm, this.rm.getRMContext());\n+    return new ClusterMetricsInfo(this.rm);\n   }\n \n   @GET"
    },
    {
        "commit_id": "f636f9d9439742d7ebaaf21f7e22652403572c61",
        "commit_message": "HDFS-7419. Improve error messages for DataNode hot swap drive feature (Lei Xu via Colin P. Mccabe)",
        "commit_url": "https://github.com/apache/hadoop/commit/f636f9d9439742d7ebaaf21f7e22652403572c61",
        "buggy_code": "errorMessage = e.toString();",
        "fixed_code": "errorMessage = e.getCause().getMessage();",
        "patch": "@@ -128,7 +128,7 @@ public void run() {\n         try {\n           this.parent.reconfigurePropertyImpl(change.prop, change.newVal);\n         } catch (ReconfigurationException e) {\n-          errorMessage = e.toString();\n+          errorMessage = e.getCause().getMessage();\n         }\n         results.put(change, Optional.fromNullable(errorMessage));\n       }"
    },
    {
        "commit_id": "bcd402ae380ead1234bfdfc53f485d3fb1391288",
        "commit_message": "HADOOP-11312. Fix unit tests to not use uppercase key names.",
        "commit_url": "https://github.com/apache/hadoop/commit/bcd402ae380ead1234bfdfc53f485d3fb1391288",
        "buggy_code": "private static final String TEST_KEY = \"testKey\";",
        "fixed_code": "private static final String TEST_KEY = \"test_key\";",
        "patch": "@@ -115,7 +115,7 @@ public class TestRpcProgramNfs3 {\n   static SecurityHandler securityHandler;\n   static SecurityHandler securityHandlerUnpriviledged;\n   static String testdir = \"/tmp\";\n-  private static final String TEST_KEY = \"testKey\";\n+  private static final String TEST_KEY = \"test_key\";\n   private static FileSystemTestHelper fsHelper;\n   private static File testRootDir;\n "
    },
    {
        "commit_id": "bcd402ae380ead1234bfdfc53f485d3fb1391288",
        "commit_message": "HADOOP-11312. Fix unit tests to not use uppercase key names.",
        "commit_url": "https://github.com/apache/hadoop/commit/bcd402ae380ead1234bfdfc53f485d3fb1391288",
        "buggy_code": "private final String TEST_KEY = \"testKey\";",
        "fixed_code": "private final String TEST_KEY = \"test_key\";",
        "patch": "@@ -48,7 +48,7 @@ public class TestEncryptionZonesWithHA {\n   private FileSystemTestHelper fsHelper;\n   private File testRootDir;\n \n-  private final String TEST_KEY = \"testKey\";\n+  private final String TEST_KEY = \"test_key\";\n \n \n   @Before"
    },
    {
        "commit_id": "bcd402ae380ead1234bfdfc53f485d3fb1391288",
        "commit_message": "HADOOP-11312. Fix unit tests to not use uppercase key names.",
        "commit_url": "https://github.com/apache/hadoop/commit/bcd402ae380ead1234bfdfc53f485d3fb1391288",
        "buggy_code": "private final String TEST_KEY = \"testKey\";",
        "fixed_code": "private final String TEST_KEY = \"test_key\";",
        "patch": "@@ -56,7 +56,7 @@ public class TestReservedRawPaths {\n   private MiniDFSCluster cluster;\n   private HdfsAdmin dfsAdmin;\n   private DistributedFileSystem fs;\n-  private final String TEST_KEY = \"testKey\";\n+  private final String TEST_KEY = \"test_key\";\n \n   protected FileSystemTestWrapper fsWrapper;\n   protected FileContextTestWrapper fcWrapper;"
    },
    {
        "commit_id": "eace218411a7733abb8dfca6aaa4eb0557e25e0c",
        "commit_message": "HADOOP-11289. Fix typo in RpcUtil log message. Contributed by Charles Lamb.",
        "commit_url": "https://github.com/apache/hadoop/commit/eace218411a7733abb8dfca6aaa4eb0557e25e0c",
        "buggy_code": "LOG.info(\"Malfromed RPC request from \" + e.getRemoteAddress());",
        "fixed_code": "LOG.info(\"Malformed RPC request from \" + e.getRemoteAddress());",
        "patch": "@@ -125,7 +125,7 @@ public void messageReceived(ChannelHandlerContext ctx, MessageEvent e)\n         info = new RpcInfo(callHeader, dataBuffer, ctx, e.getChannel(),\n             e.getRemoteAddress());\n       } catch (Exception exc) {\n-        LOG.info(\"Malfromed RPC request from \" + e.getRemoteAddress());\n+        LOG.info(\"Malformed RPC request from \" + e.getRemoteAddress());\n       }\n \n       if (info != null) {"
    },
    {
        "commit_id": "5c0381c96aa79196829edbca497c649eb6776944",
        "commit_message": "YARN-2790. Fixed a NodeManager bug that was causing log-aggregation to fail beyond HFDS delegation-token expiry even when RM is a proxy-user (YARN-2704). Contributed by Jian He.",
        "commit_url": "https://github.com/apache/hadoop/commit/5c0381c96aa79196829edbca497c649eb6776944",
        "buggy_code": "public void setSystemCrendentials(",
        "fixed_code": "public void setSystemCrendentialsForApps(",
        "patch": "@@ -433,7 +433,7 @@ public Map<ApplicationId, Credentials> getSystemCredentialsForApps() {\n       return systemCredentials;\n     }\n \n-    public void setSystemCrendentials(\n+    public void setSystemCrendentialsForApps(\n         Map<ApplicationId, Credentials> systemCredentials) {\n       this.systemCredentials = systemCredentials;\n     }"
    },
    {
        "commit_id": "5c0381c96aa79196829edbca497c649eb6776944",
        "commit_message": "YARN-2790. Fixed a NodeManager bug that was causing log-aggregation to fail beyond HFDS delegation-token expiry even when RM is a proxy-user (YARN-2704). Contributed by Jian He.",
        "commit_url": "https://github.com/apache/hadoop/commit/5c0381c96aa79196829edbca497c649eb6776944",
        "buggy_code": ".setSystemCrendentials(parseCredentials(systemCredentials));",
        "fixed_code": ".setSystemCrendentialsForApps(parseCredentials(systemCredentials));",
        "patch": "@@ -626,7 +626,7 @@ public void run() {\n                 response.getSystemCredentialsForApps();\n             if (systemCredentials != null && !systemCredentials.isEmpty()) {\n               ((NMContext) context)\n-                .setSystemCrendentials(parseCredentials(systemCredentials));\n+                .setSystemCrendentialsForApps(parseCredentials(systemCredentials));\n             }\n           } catch (ConnectException e) {\n             //catch and throw the exception if tried MAX wait time to connect RM"
    },
    {
        "commit_id": "05b66ca0749bdb03d1df3b95199eb4f331409f7d",
        "commit_message": "HADOOP-11247. Fix a couple javac warnings in NFS. Contributed by Brandon Li.",
        "commit_url": "https://github.com/apache/hadoop/commit/05b66ca0749bdb03d1df3b95199eb4f331409f7d",
        "buggy_code": "public static final String STATIC_ID_MAPPING_FILE_DEFAULT = \"/etc/usergroupid.map\";",
        "fixed_code": "public static final String STATIC_ID_MAPPING_FILE_DEFAULT = \"/etc/nfs.map\";",
        "patch": "@@ -32,5 +32,5 @@ public class IdMappingConstant {\n   \n   // Used for finding the configured static mapping file.\n   public static final String STATIC_ID_MAPPING_FILE_KEY = \"static.id.mapping.file\";\n-  public static final String STATIC_ID_MAPPING_FILE_DEFAULT = \"/etc/usergroupid.map\";\n+  public static final String STATIC_ID_MAPPING_FILE_DEFAULT = \"/etc/nfs.map\";\n }"
    },
    {
        "commit_id": "05b66ca0749bdb03d1df3b95199eb4f331409f7d",
        "commit_message": "HADOOP-11247. Fix a couple javac warnings in NFS. Contributed by Brandon Li.",
        "commit_url": "https://github.com/apache/hadoop/commit/05b66ca0749bdb03d1df3b95199eb4f331409f7d",
        "buggy_code": "Nfs3Constant.NFS_STATIC_MAPPING_FILE_DEFAULT);",
        "fixed_code": "IdMappingConstant.STATIC_ID_MAPPING_FILE_DEFAULT);",
        "patch": "@@ -174,7 +174,7 @@ public RpcProgramNfs3(NfsConfiguration config, DatagramSocket registrationSocket\n     this.config = config;\n     config.set(FsPermission.UMASK_LABEL, \"000\");\n     iug = new ShellBasedIdMapping(config,\n-        Nfs3Constant.NFS_STATIC_MAPPING_FILE_DEFAULT);\n+        IdMappingConstant.STATIC_ID_MAPPING_FILE_DEFAULT);\n \n     aixCompatMode = config.getBoolean(\n         NfsConfigKeys.AIX_COMPAT_MODE_KEY,"
    },
    {
        "commit_id": "018664550507981297fd9f91e29408e6b7801ea9",
        "commit_message": "YARN-2743. Fixed a bug in ResourceManager that was causing RMDelegationToken identifiers to be tampered and thus causing app submission failures in secure mode. Contributed by Jian He.",
        "commit_url": "https://github.com/apache/hadoop/commit/018664550507981297fd9f91e29408e6b7801ea9",
        "buggy_code": "static boolean isEqual(Object a, Object b) {",
        "fixed_code": "protected static boolean isEqual(Object a, Object b) {",
        "patch": "@@ -159,7 +159,7 @@ public int getMasterKeyId() {\n     return masterKeyId;\n   }\n \n-  static boolean isEqual(Object a, Object b) {\n+  protected static boolean isEqual(Object a, Object b) {\n     return a == null ? b == null : a.equals(b);\n   }\n   "
    },
    {
        "commit_id": "65d95b1a520d4ffdf024dbdfcf11d855a3948056",
        "commit_message": "YARN-2723. Fix rmadmin -replaceLabelsOnNode does not correctly parse\nport. Contributed by Naganarasimha G R",
        "commit_url": "https://github.com/apache/hadoop/commit/65d95b1a520d4ffdf024dbdfcf11d855a3948056",
        "buggy_code": "port = Integer.valueOf(nodeIdStr.substring(nodeIdStr.indexOf(\":\")));",
        "fixed_code": "port = Integer.valueOf(nodeIdStr.substring(nodeIdStr.indexOf(\":\") + 1));",
        "patch": "@@ -454,7 +454,7 @@ private Map<NodeId, Set<String>> buildNodeLabelsFromStr(String args)\n       int port;\n       if (nodeIdStr.contains(\":\")) {\n         nodeName = nodeIdStr.substring(0, nodeIdStr.indexOf(\":\"));\n-        port = Integer.valueOf(nodeIdStr.substring(nodeIdStr.indexOf(\":\")));\n+        port = Integer.valueOf(nodeIdStr.substring(nodeIdStr.indexOf(\":\") + 1));\n       } else {\n         nodeName = nodeIdStr;\n         port = 0;"
    },
    {
        "commit_id": "b3d8a642a938da9de680b479585a7c2014b8965c",
        "commit_message": "HDFS-7283. Bump DataNode OOM log from WARN to ERROR. Contributed by Stephen Chu.",
        "commit_url": "https://github.com/apache/hadoop/commit/b3d8a642a938da9de680b479585a7c2014b8965c",
        "buggy_code": "LOG.warn(\"DataNode is out of memory. Will retry in 30 seconds.\", ie);",
        "fixed_code": "LOG.error(\"DataNode is out of memory. Will retry in 30 seconds.\", ie);",
        "patch": "@@ -161,7 +161,7 @@ public void run() {\n         // DataNode can run out of memory if there is too many transfers.\n         // Log the event, Sleep for 30 seconds, other transfers may complete by\n         // then.\n-        LOG.warn(\"DataNode is out of memory. Will retry in 30 seconds.\", ie);\n+        LOG.error(\"DataNode is out of memory. Will retry in 30 seconds.\", ie);\n         try {\n           Thread.sleep(30 * 1000);\n         } catch (InterruptedException e) {"
    },
    {
        "commit_id": "1ceb3269337e321e352b7cd9f946c5c52dcfddfa",
        "commit_message": "HDFS-7198. Fix \"unchecked conversion\" warning in DFSClient#getPathTraceScope (cmccabe)",
        "commit_url": "https://github.com/apache/hadoop/commit/1ceb3269337e321e352b7cd9f946c5c52dcfddfa",
        "buggy_code": "public static Sampler createSampler(Configuration conf) {",
        "fixed_code": "public static Sampler<?> createSampler(Configuration conf) {",
        "patch": "@@ -30,7 +30,7 @@ public class TraceSamplerFactory {\n   private static final Logger LOG =\n       LoggerFactory.getLogger(TraceSamplerFactory.class);\n \n-  public static Sampler createSampler(Configuration conf) {\n+  public static Sampler<?> createSampler(Configuration conf) {\n     String samplerStr = conf.get(CommonConfigurationKeys.HADOOP_TRACE_SAMPLER,\n         CommonConfigurationKeys.HADOOP_TRACE_SAMPLER_DEFAULT);\n     if (samplerStr.equals(\"NeverSampler\")) {"
    },
    {
        "commit_id": "1ceb3269337e321e352b7cd9f946c5c52dcfddfa",
        "commit_message": "HDFS-7198. Fix \"unchecked conversion\" warning in DFSClient#getPathTraceScope (cmccabe)",
        "commit_url": "https://github.com/apache/hadoop/commit/1ceb3269337e321e352b7cd9f946c5c52dcfddfa",
        "buggy_code": "private final Sampler traceSampler;",
        "fixed_code": "private final Sampler<?> traceSampler;",
        "patch": "@@ -277,7 +277,7 @@ public class DFSClient implements java.io.Closeable, RemotePeerFactory,\n   @VisibleForTesting\n   KeyProvider provider;\n   private final SpanReceiverHost spanReceiverHost;\n-  private final Sampler traceSampler;\n+  private final Sampler<?> traceSampler;\n \n   /**\n    * DFSClient configuration "
    },
    {
        "commit_id": "5e10a13bb4759984494c6a870c7f08fb6693c9c0",
        "commit_message": "YARN-2576. Making test patch pass in branch. Contributed by Subru Krishnan and Carlo Curino.\n(cherry picked from commit 90ac0be86b898aefec5471db4027554c8e1b310c)",
        "commit_url": "https://github.com/apache/hadoop/commit/5e10a13bb4759984494c6a870c7f08fb6693c9c0",
        "buggy_code": "public static final long DEFAULT_RESERVATION_WINDOW = 0L;",
        "fixed_code": "public static final long DEFAULT_RESERVATION_WINDOW = 86400000L;",
        "patch": "@@ -204,7 +204,7 @@ public QueueMapping(MappingType type, String source, String queue) {\n       \"instantaneous-max-capacity\";\n \n   @Private\n-  public static final long DEFAULT_RESERVATION_WINDOW = 0L;\n+  public static final long DEFAULT_RESERVATION_WINDOW = 86400000L;\n \n   @Private\n   public static final String RESERVATION_ADMISSION_POLICY ="
    },
    {
        "commit_id": "f679ca38ce0365c97f1dba79e333a8de18733b8a",
        "commit_message": "HADOOP-11160. Fix typo in nfs3 server duplicate entry reporting. Contributed by Charles Lamb.",
        "commit_url": "https://github.com/apache/hadoop/commit/f679ca38ce0365c97f1dba79e333a8de18733b8a",
        "buggy_code": "+ \"<getent passwd | cut -d: -f1,3> and <getent group | cut -d: -f1,3> on Linux systms,\\n\"",
        "fixed_code": "+ \"<getent passwd | cut -d: -f1,3> and <getent group | cut -d: -f1,3> on Linux systems,\\n\"",
        "patch": "@@ -114,7 +114,7 @@ private void checkAndUpdateMaps() {\n       + \"The host system with duplicated user/group name or id might work fine most of the time by itself.\\n\"\n       + \"However when NFS gateway talks to HDFS, HDFS accepts only user and group name.\\n\"\n       + \"Therefore, same name means the same user or same group. To find the duplicated names/ids, one can do:\\n\"\n-      + \"<getent passwd | cut -d: -f1,3> and <getent group | cut -d: -f1,3> on Linux systms,\\n\"\n+      + \"<getent passwd | cut -d: -f1,3> and <getent group | cut -d: -f1,3> on Linux systems,\\n\"\n       + \"<dscl . -list /Users UniqueID> and <dscl . -list /Groups PrimaryGroupID> on MacOS.\";\n   \n   private static void reportDuplicateEntry(final String header,"
    },
    {
        "commit_id": "263977969b98a2d7a26a7c11e4467c9de836fbe6",
        "commit_message": "Fix test build break after merging from trunk",
        "commit_url": "https://github.com/apache/hadoop/commit/263977969b98a2d7a26a7c11e4467c9de836fbe6",
        "buggy_code": "new HdfsFileStatus(0, false, 1, 1024, 0, 0, new FsPermission(",
        "fixed_code": "new HdfsFileStatus(0, false, 1, 1024, false, 0, 0, new FsPermission(",
        "patch": "@@ -716,7 +716,7 @@ public void testVersionAndSuiteNegotiation() throws Exception {\n   private static void mockCreate(ClientProtocol mcp,\n       CipherSuite suite, CryptoProtocolVersion version) throws Exception {\n     Mockito.doReturn(\n-        new HdfsFileStatus(0, false, 1, 1024, 0, 0, new FsPermission(\n+        new HdfsFileStatus(0, false, 1, 1024, false, 0, 0, new FsPermission(\n             (short) 777), \"owner\", \"group\", new byte[0], new byte[0],\n             1010, 0, new FileEncryptionInfo(suite,\n             version, new byte[suite.getAlgorithmBlockSize()],"
    },
    {
        "commit_id": "f614cb71d47a34a5e7ccb1057f4f61d3cd179f55",
        "commit_message": "MAPREDUCE-6109. Fix minor typo in distcp -p usage text (Charles Lamb via aw)",
        "commit_url": "https://github.com/apache/hadoop/commit/f614cb71d47a34a5e7ccb1057f4f61d3cd179f55",
        "buggy_code": "\"preservation is independent of the -p flag.\" +",
        "fixed_code": "\"preservation is independent of the -p flag. \" +",
        "patch": "@@ -54,7 +54,7 @@ public enum DistCpOptionSwitch {\n           \"and timestamps. \" +\n           \"raw.* xattrs are preserved when both the source and destination \" +\n           \"paths are in the /.reserved/raw hierarchy (HDFS only). raw.* xattr\" +\n-          \"preservation is independent of the -p flag.\" +\n+          \"preservation is independent of the -p flag. \" +\n           \"Refer to the DistCp documentation for more details.\")),\n \n   /**"
    },
    {
        "commit_id": "332e2e23ba6f0748a46c0bda76f426d9cad73edd",
        "commit_message": "HDFS-7105. Fix TestJournalNode#testFailToStartWithBadConfig to match log output change. Contributed by Ray Chiang.",
        "commit_url": "https://github.com/apache/hadoop/commit/332e2e23ba6f0748a46c0bda76f426d9cad73edd",
        "buggy_code": "assertJNFailsToStart(conf, \"Can not create directory\");",
        "fixed_code": "assertJNFailsToStart(conf, \"Cannot create directory\");",
        "patch": "@@ -287,7 +287,7 @@ public void testFailToStartWithBadConfig() throws Exception {\n     // Directory which cannot be created\n     conf.set(DFSConfigKeys.DFS_JOURNALNODE_EDITS_DIR_KEY,\n         Shell.WINDOWS ? \"\\\\\\\\cannotBeCreated\" : \"/proc/does-not-exist\");\n-    assertJNFailsToStart(conf, \"Can not create directory\");\n+    assertJNFailsToStart(conf, \"Cannot create directory\");\n   }\n \n   private static void assertJNFailsToStart(Configuration conf,"
    },
    {
        "commit_id": "9f03a7c018bb2c497cd0ef758f1a3e08e8163d06",
        "commit_message": "HADOOP-10946. Fix a bunch of typos in log messages (Ray Chiang via aw)",
        "commit_url": "https://github.com/apache/hadoop/commit/9f03a7c018bb2c497cd0ef758f1a3e08e8163d06",
        "buggy_code": "return \"A record version mismatch occured. Expecting v\"",
        "fixed_code": "return \"A record version mismatch occurred. Expecting v\"",
        "patch": "@@ -41,7 +41,7 @@ public VersionMismatchException(byte expectedVersionIn, byte foundVersionIn){\n   /** Returns a string representation of this object. */\n   @Override\n   public String toString(){\n-    return \"A record version mismatch occured. Expecting v\"\n+    return \"A record version mismatch occurred. Expecting v\"\n       + expectedVersion + \", found v\" + foundVersion; \n   }\n }"
    },
    {
        "commit_id": "9f03a7c018bb2c497cd0ef758f1a3e08e8163d06",
        "commit_message": "HADOOP-10946. Fix a bunch of typos in log messages (Ray Chiang via aw)",
        "commit_url": "https://github.com/apache/hadoop/commit/9f03a7c018bb2c497cd0ef758f1a3e08e8163d06",
        "buggy_code": "MetricsUtil.LOG.error(\"Unexpected attrubute suffix\");",
        "fixed_code": "MetricsUtil.LOG.error(\"Unexpected attribute suffix\");",
        "patch": "@@ -160,7 +160,7 @@ else if (attributeName.endsWith(MIN_TIME))\n       else if (attributeName.endsWith(MAX_TIME))\n         return or.getMaxTime();\n       else {\n-        MetricsUtil.LOG.error(\"Unexpected attrubute suffix\");\n+        MetricsUtil.LOG.error(\"Unexpected attribute suffix\");\n         throw new AttributeNotFoundException();\n       }\n     } else {"
    },
    {
        "commit_id": "9f03a7c018bb2c497cd0ef758f1a3e08e8163d06",
        "commit_message": "HADOOP-10946. Fix a bunch of typos in log messages (Ray Chiang via aw)",
        "commit_url": "https://github.com/apache/hadoop/commit/9f03a7c018bb2c497cd0ef758f1a3e08e8163d06",
        "buggy_code": "throw new DiskErrorException(\"Can not create directory: \"",
        "fixed_code": "throw new DiskErrorException(\"Cannot create directory: \"",
        "patch": "@@ -102,7 +102,7 @@ public static void checkDirs(File dir) throws DiskErrorException {\n    */\n   public static void checkDir(File dir) throws DiskErrorException {\n     if (!mkdirsWithExistsCheck(dir)) {\n-      throw new DiskErrorException(\"Can not create directory: \"\n+      throw new DiskErrorException(\"Cannot create directory: \"\n                                    + dir.toString());\n     }\n     checkDirAccess(dir);"
    },
    {
        "commit_id": "9f03a7c018bb2c497cd0ef758f1a3e08e8163d06",
        "commit_message": "HADOOP-10946. Fix a bunch of typos in log messages (Ray Chiang via aw)",
        "commit_url": "https://github.com/apache/hadoop/commit/9f03a7c018bb2c497cd0ef758f1a3e08e8163d06",
        "buggy_code": "LOG.debug(\"Renaming directory to a itself is disallowed. src=\" + src",
        "fixed_code": "LOG.debug(\"Renaming directory to itself is disallowed. src=\" + src",
        "patch": "@@ -1095,7 +1095,7 @@ public boolean rename(Path src, Path dst) throws IOException {\n       if (dstKey.startsWith(srcKey + PATH_DELIMITER)) {\n \n         if (LOG.isDebugEnabled()) {\n-          LOG.debug(\"Renaming directory to a itself is disallowed. src=\" + src\n+          LOG.debug(\"Renaming directory to itself is disallowed. src=\" + src\n               + \" dest=\" + dst);\n         }\n         return false;"
    },
    {
        "commit_id": "9f03a7c018bb2c497cd0ef758f1a3e08e8163d06",
        "commit_message": "HADOOP-10946. Fix a bunch of typos in log messages (Ray Chiang via aw)",
        "commit_url": "https://github.com/apache/hadoop/commit/9f03a7c018bb2c497cd0ef758f1a3e08e8163d06",
        "buggy_code": "LOG.error(\"Unexcpected exception\", e);",
        "fixed_code": "LOG.error(\"Unexpected exception\", e);",
        "patch": "@@ -681,7 +681,7 @@ public void run() {\n           } catch (IOException e) {\n             LOG.warn(\"Failure killing \" + job.getJobName(), e);\n           } catch (Exception e) {\n-            LOG.error(\"Unexcpected exception\", e);\n+            LOG.error(\"Unexpected exception\", e);\n           }\n         }\n         LOG.info(\"Done.\");"
    },
    {
        "commit_id": "9f03a7c018bb2c497cd0ef758f1a3e08e8163d06",
        "commit_message": "HADOOP-10946. Fix a bunch of typos in log messages (Ray Chiang via aw)",
        "commit_url": "https://github.com/apache/hadoop/commit/9f03a7c018bb2c497cd0ef758f1a3e08e8163d06",
        "buggy_code": "LOG.error(\"Job not Successful!\");",
        "fixed_code": "LOG.error(\"Job not successful!\");",
        "patch": "@@ -1016,7 +1016,7 @@ public int submitAndMonitorJob() throws IOException {\n       if (background_) {\n         LOG.info(\"Job is running in background.\");\n       } else if (!jc_.monitorAndPrintJob(jobConf_, running_)) {\n-        LOG.error(\"Job not Successful!\");\n+        LOG.error(\"Job not successful!\");\n         return 1;\n       }\n       LOG.info(\"Output directory: \" + output_);"
    },
    {
        "commit_id": "76ccb80fadad50a4465e7e26bc21ae7016036207",
        "commit_message": "HDFS-7088. Archival Storage: fix TestBalancer and TestBalancerWithMultipleNameNodes. Contributed by Tsz Wo Nicholas Sze.",
        "commit_url": "https://github.com/apache/hadoop/commit/76ccb80fadad50a4465e7e26bc21ae7016036207",
        "buggy_code": "Dispatcher.setBlockMoveWaitTime(1000L);",
        "fixed_code": "TestBalancer.initTestSetup();",
        "patch": "@@ -44,7 +44,7 @@ public class TestBalancerWithHANameNodes {\n   ClientProtocol client;\n \n   static {\n-    Dispatcher.setBlockMoveWaitTime(1000L);\n+    TestBalancer.initTestSetup();\n   }\n \n   /**"
    },
    {
        "commit_id": "76ccb80fadad50a4465e7e26bc21ae7016036207",
        "commit_message": "HDFS-7088. Archival Storage: fix TestBalancer and TestBalancerWithMultipleNameNodes. Contributed by Tsz Wo Nicholas Sze.",
        "commit_url": "https://github.com/apache/hadoop/commit/76ccb80fadad50a4465e7e26bc21ae7016036207",
        "buggy_code": "Dispatcher.setBlockMoveWaitTime(1000L) ;",
        "fixed_code": "TestBalancer.initTestSetup();",
        "patch": "@@ -73,7 +73,7 @@ public class TestBalancerWithMultipleNameNodes {\n   private static final Random RANDOM = new Random();\n \n   static {\n-    Dispatcher.setBlockMoveWaitTime(1000L) ;\n+    TestBalancer.initTestSetup();\n   }\n \n   /** Common objects used in various methods. */"
    },
    {
        "commit_id": "76ccb80fadad50a4465e7e26bc21ae7016036207",
        "commit_message": "HDFS-7088. Archival Storage: fix TestBalancer and TestBalancerWithMultipleNameNodes. Contributed by Tsz Wo Nicholas Sze.",
        "commit_url": "https://github.com/apache/hadoop/commit/76ccb80fadad50a4465e7e26bc21ae7016036207",
        "buggy_code": "Dispatcher.setBlockMoveWaitTime(1000L) ;",
        "fixed_code": "TestBalancer.initTestSetup();",
        "patch": "@@ -75,7 +75,7 @@ public class TestBalancerWithNodeGroup {\n   static final int DEFAULT_BLOCK_SIZE = 100;\n \n   static {\n-    Dispatcher.setBlockMoveWaitTime(1000L) ;\n+    TestBalancer.initTestSetup();\n   }\n \n   static Configuration createConf() {"
    },
    {
        "commit_id": "bd2e409de128dcf4143f69c1d572e03a51511517",
        "commit_message": "HDFS-7072. Fix TestBlockManager and TestStorageMover.  Contributed by Jing Zhao",
        "commit_url": "https://github.com/apache/hadoop/commit/bd2e409de128dcf4143f69c1d572e03a51511517",
        "buggy_code": "private long remaining;",
        "fixed_code": "private volatile long remaining;",
        "patch": "@@ -109,7 +109,7 @@ public void remove() {\n \n   private long capacity;\n   private long dfsUsed;\n-  private long remaining;\n+  private volatile long remaining;\n   private long blockPoolUsed;\n \n   private volatile BlockInfo blockList = null;"
    },
    {
        "commit_id": "4603e4481f0486afcce6b106d4a92a6e90e5b6d9",
        "commit_message": "HDFS-7064. Fix unit test failures in HDFS-6581 branch. (Contributed by Xiaoyu Yao)",
        "commit_url": "https://github.com/apache/hadoop/commit/4603e4481f0486afcce6b106d4a92a6e90e5b6d9",
        "buggy_code": "assertThat(locations.size(), is(4));",
        "fixed_code": "assertThat(locations.size(), is(5));",
        "patch": "@@ -51,7 +51,7 @@ public void testDataDirParsing() throws Throwable {\n     String locations1 = \"[disk]/dir0,[DISK]/dir1,[sSd]/dir2,[disK]/dir3,[ram_disk]/dir4\";\n     conf.set(DFS_DATANODE_DATA_DIR_KEY, locations1);\n     locations = DataNode.getStorageLocations(conf);\n-    assertThat(locations.size(), is(4));\n+    assertThat(locations.size(), is(5));\n     assertThat(locations.get(0).getStorageType(), is(StorageType.DISK));\n     assertThat(locations.get(0).getUri(), is(dir0.toURI()));\n     assertThat(locations.get(1).getStorageType(), is(StorageType.DISK));"
    },
    {
        "commit_id": "0c26412be4b3ec40130b7200506c957f0402ecbc",
        "commit_message": "HDFS-6965. NN continues to issue block locations for DNs with full\ndisks. Contributed by Rushabh Shah.",
        "commit_url": "https://github.com/apache/hadoop/commit/0c26412be4b3ec40130b7200506c957f0402ecbc",
        "buggy_code": "if (requiredSize > node.getRemaining() - scheduledSize) {",
        "fixed_code": "if (requiredSize > storage.getRemaining() - scheduledSize) {",
        "patch": "@@ -635,7 +635,7 @@ private boolean isGoodTarget(DatanodeStorageInfo storage,\n     \n     final long requiredSize = blockSize * HdfsConstants.MIN_BLOCKS_FOR_WRITE;\n     final long scheduledSize = blockSize * node.getBlocksScheduled();\n-    if (requiredSize > node.getRemaining() - scheduledSize) {\n+    if (requiredSize > storage.getRemaining() - scheduledSize) {\n       logNodeIsNotChosen(storage, \"the node does not have enough space \");\n       return false;\n     }"
    },
    {
        "commit_id": "b100949404843ed245ef4e118291f55b3fdc81b8",
        "commit_message": "HADOOP-9989. Bug introduced in HADOOP-9374, which parses the -tokenCacheFile as binary file but set it to the configuration as JSON file. (zxu via tucu)",
        "commit_url": "https://github.com/apache/hadoop/commit/b100949404843ed245ef4e118291f55b3fdc81b8",
        "buggy_code": "conf.set(\"mapreduce.job.credentials.json\", p.toString(),",
        "fixed_code": "conf.set(\"mapreduce.job.credentials.binary\", p.toString(),",
        "patch": "@@ -332,7 +332,7 @@ private void processGeneralOptions(Configuration conf,\n       }\n       UserGroupInformation.getCurrentUser().addCredentials(\n           Credentials.readTokenStorageFile(p, conf));\n-      conf.set(\"mapreduce.job.credentials.json\", p.toString(),\n+      conf.set(\"mapreduce.job.credentials.binary\", p.toString(),\n                \"from -tokenCacheFile command line option\");\n \n     }"
    },
    {
        "commit_id": "b100949404843ed245ef4e118291f55b3fdc81b8",
        "commit_message": "HADOOP-9989. Bug introduced in HADOOP-9374, which parses the -tokenCacheFile as binary file but set it to the configuration as JSON file. (zxu via tucu)",
        "commit_url": "https://github.com/apache/hadoop/commit/b100949404843ed245ef4e118291f55b3fdc81b8",
        "buggy_code": "String fileName = conf.get(\"mapreduce.job.credentials.json\");",
        "fixed_code": "String fileName = conf.get(\"mapreduce.job.credentials.binary\");",
        "patch": "@@ -249,7 +249,7 @@ public void testTokenCacheOption() throws IOException {\n     creds.writeTokenStorageFile(tmpPath, conf);\n \n     new GenericOptionsParser(conf, args);\n-    String fileName = conf.get(\"mapreduce.job.credentials.json\");\n+    String fileName = conf.get(\"mapreduce.job.credentials.binary\");\n     assertNotNull(\"files is null\", fileName);\n     assertEquals(\"files option does not match\", tmpPath.toString(), fileName);\n     "
    },
    {
        "commit_id": "f949f6b54825dac61511a5761837e2fd14437239",
        "commit_message": "HDFS-6981. Fix DN upgrade with layout version change. (Arpit Agarwal)",
        "commit_url": "https://github.com/apache/hadoop/commit/f949f6b54825dac61511a5761837e2fd14437239",
        "buggy_code": "private void handleRollingUpgradeStatus(HeartbeatResponse resp) {",
        "fixed_code": "private void handleRollingUpgradeStatus(HeartbeatResponse resp) throws IOException {",
        "patch": "@@ -627,7 +627,7 @@ private synchronized void cleanUp() {\n     bpos.shutdownActor(this);\n   }\n \n-  private void handleRollingUpgradeStatus(HeartbeatResponse resp) {\n+  private void handleRollingUpgradeStatus(HeartbeatResponse resp) throws IOException {\n     RollingUpgradeStatus rollingUpgradeStatus = resp.getRollingUpdateStatus();\n     if (rollingUpgradeStatus != null &&\n         rollingUpgradeStatus.getBlockPoolId().compareTo(bpos.getBlockPoolId()) != 0) {"
    },
    {
        "commit_id": "867f0f141cc935c1f67680533cc22f25b804f720",
        "commit_message": "HDFS-7029. Archival Storage: fix TestDFSInotifyEventInputStream and TestDistributedFileSystem. Contributed by Tsz Wo Nicholas Sze.",
        "commit_url": "https://github.com/apache/hadoop/commit/867f0f141cc935c1f67680533cc22f25b804f720",
        "buggy_code": "Assert.assertTrue(FSEditLogOpCodes.values().length == 46);",
        "fixed_code": "Assert.assertTrue(FSEditLogOpCodes.values().length == 47);",
        "patch": "@@ -64,7 +64,7 @@ private static Event waitForNextEvent(DFSInotifyEventInputStream eis)\n    */\n   @Test\n   public void testOpcodeCount() {\n-    Assert.assertTrue(FSEditLogOpCodes.values().length == 46);\n+    Assert.assertTrue(FSEditLogOpCodes.values().length == 47);\n   }\n \n "
    },
    {
        "commit_id": "d989ac04449dc33da5e2c32a7f24d59cc92de536",
        "commit_message": "MAPREDUCE-5972. Fix typo 'programatically' in job.xml (and a few other places) (Akira AJISAKA via aw)",
        "commit_url": "https://github.com/apache/hadoop/commit/d989ac04449dc33da5e2c32a7f24d59cc92de536",
        "buggy_code": "&& \"programatically\".equals(resource)) {",
        "fixed_code": "&& \"programmatically\".equals(resource)) {",
        "patch": "@@ -63,7 +63,7 @@ public void testWriteJson() throws Exception {\n       String resource = (String)propertyInfo.get(\"resource\");\n       System.err.println(\"k: \" + key + \" v: \" + val + \" r: \" + resource);\n       if (TEST_KEY.equals(key) && TEST_VAL.equals(val)\n-          && \"programatically\".equals(resource)) {\n+          && \"programmatically\".equals(resource)) {\n         foundSetting = true;\n       }\n     }"
    },
    {
        "commit_id": "7c91f9b1484d487e792dca051fbd418697049422",
        "commit_message": "MAPREDUCE-6074. native-task: Fix release audit warnings",
        "commit_url": "https://github.com/apache/hadoop/commit/7c91f9b1484d487e792dca051fbd418697049422",
        "buggy_code": "fs.delete(path);",
        "fixed_code": "fs.delete(path, true);",
        "patch": "@@ -124,7 +124,7 @@ public void startUp() throws Exception {\n     final ScenarioConfiguration conf = new ScenarioConfiguration();\n     final FileSystem fs = FileSystem.get(conf);\n     final Path path = new Path(TestConstants.NATIVETASK_COMPRESS_TEST_INPUTDIR);\n-    fs.delete(path);\n+    fs.delete(path, true);\n     if (!fs.exists(path)) {\n       new TestInputFile(hadoopConf.getInt(\n           TestConstants.NATIVETASK_COMPRESS_FILESIZE, 100000),"
    },
    {
        "commit_id": "7c91f9b1484d487e792dca051fbd418697049422",
        "commit_message": "MAPREDUCE-6074. native-task: Fix release audit warnings",
        "commit_url": "https://github.com/apache/hadoop/commit/7c91f9b1484d487e792dca051fbd418697049422",
        "buggy_code": "final Job job = new Job(conf, jobName);",
        "fixed_code": "final Job job = Job.getInstance(conf, jobName);",
        "patch": "@@ -101,7 +101,7 @@ private Job getJob(Configuration conf, String jobName,\n       fs.delete(new Path(outputpath), true);\n     }\n     fs.close();\n-    final Job job = new Job(conf, jobName);\n+    final Job job = Job.getInstance(conf, jobName);\n     job.setJarByClass(NonSortTestMR.class);\n     job.setMapperClass(NonSortTestMR.Map.class);\n     job.setReducerClass(NonSortTestMR.KeyHashSumReduce.class);"
    },
    {
        "commit_id": "08a9ac7098cb4ae684f40cf2513e3137110cc7e4",
        "commit_message": "HDFS-6942. Fix typos in log messages. Contributed by Ray Chiang.",
        "commit_url": "https://github.com/apache/hadoop/commit/08a9ac7098cb4ae684f40cf2513e3137110cc7e4",
        "buggy_code": "LOG.info(\"Cookie cound't be found: \" + new String(startAfter)",
        "fixed_code": "LOG.info(\"Cookie couldn't be found: \" + new String(startAfter)",
        "patch": "@@ -1423,7 +1423,7 @@ private DirectoryListing listPaths(DFSClient dfsClient, String dirFileIdPath,\n         throw io;\n       }\n       // This happens when startAfter was just deleted\n-      LOG.info(\"Cookie cound't be found: \" + new String(startAfter)\n+      LOG.info(\"Cookie couldn't be found: \" + new String(startAfter)\n           + \", do listing from beginning\");\n       dlisting = dfsClient\n           .listPaths(dirFileIdPath, HdfsFileStatus.EMPTY_NAME);"
    },
    {
        "commit_id": "08a9ac7098cb4ae684f40cf2513e3137110cc7e4",
        "commit_message": "HDFS-6942. Fix typos in log messages. Contributed by Ray Chiang.",
        "commit_url": "https://github.com/apache/hadoop/commit/08a9ac7098cb4ae684f40cf2513e3137110cc7e4",
        "buggy_code": "+ \", targests=\" + Arrays.asList(targets));",
        "fixed_code": "+ \", targets=\" + Arrays.asList(targets));",
        "patch": "@@ -1744,7 +1744,7 @@ private class DataTransfer implements Runnable {\n             + b + \" (numBytes=\" + b.getNumBytes() + \")\"\n             + \", stage=\" + stage\n             + \", clientname=\" + clientname\n-            + \", targests=\" + Arrays.asList(targets));\n+            + \", targets=\" + Arrays.asList(targets));\n       }\n       this.targets = targets;\n       this.targetStorageTypes = targetStorageTypes;"
    },
    {
        "commit_id": "08a9ac7098cb4ae684f40cf2513e3137110cc7e4",
        "commit_message": "HDFS-6942. Fix typos in log messages. Contributed by Ray Chiang.",
        "commit_url": "https://github.com/apache/hadoop/commit/08a9ac7098cb4ae684f40cf2513e3137110cc7e4",
        "buggy_code": "LOG.info(\"Successfully opened for appends\");",
        "fixed_code": "LOG.info(\"Successfully opened for append\");",
        "patch": "@@ -125,7 +125,7 @@ private void recoverFile(final FileSystem fs) throws Exception {\n     while (!recovered && tries-- > 0) {\n       try {\n         out = fs.append(file1);\n-        LOG.info(\"Successfully opened for appends\");\n+        LOG.info(\"Successfully opened for append\");\n         recovered = true;\n       } catch (IOException e) {\n         LOG.info(\"Failed open for append, waiting on lease recovery\");"
    },
    {
        "commit_id": "08a9ac7098cb4ae684f40cf2513e3137110cc7e4",
        "commit_message": "HDFS-6942. Fix typos in log messages. Contributed by Ray Chiang.",
        "commit_url": "https://github.com/apache/hadoop/commit/08a9ac7098cb4ae684f40cf2513e3137110cc7e4",
        "buggy_code": "LOG.info(\"Read an compressed iamge and store it as uncompressed.\");",
        "fixed_code": "LOG.info(\"Read a compressed image and store it as uncompressed.\");",
        "patch": "@@ -441,7 +441,7 @@ public void testCompression() throws IOException {\n     checkNameSpace(conf);\n \n     // read an image compressed in Gzip and store it uncompressed\n-    LOG.info(\"Read an compressed iamge and store it as uncompressed.\");\n+    LOG.info(\"Read a compressed image and store it as uncompressed.\");\n     conf.setBoolean(DFSConfigKeys.DFS_IMAGE_COMPRESS_KEY, false);\n     checkNameSpace(conf);\n "
    },
    {
        "commit_id": "3de66011c2e80d7c458a67f80042af986fcc677d",
        "commit_message": "YARN-2450. Fix typos in log messages. Contributed by Ray Chiang.",
        "commit_url": "https://github.com/apache/hadoop/commit/3de66011c2e80d7c458a67f80042af986fcc677d",
        "buggy_code": "LOG.fatal(\"Error running CLient\", t);",
        "fixed_code": "LOG.fatal(\"Error running Client\", t);",
        "patch": "@@ -197,7 +197,7 @@ public static void main(String[] args) {\n       }\n       result = client.run();\n     } catch (Throwable t) {\n-      LOG.fatal(\"Error running CLient\", t);\n+      LOG.fatal(\"Error running Client\", t);\n       System.exit(1);\n     }\n     if (result) {"
    },
    {
        "commit_id": "3de66011c2e80d7c458a67f80042af986fcc677d",
        "commit_message": "YARN-2450. Fix typos in log messages. Contributed by Ray Chiang.",
        "commit_url": "https://github.com/apache/hadoop/commit/3de66011c2e80d7c458a67f80042af986fcc677d",
        "buggy_code": "LOG.info(\"Done Loading applications from FS state store\");",
        "fixed_code": "LOG.info(\"Done loading applications from FS state store\");",
        "patch": "@@ -300,7 +300,7 @@ private void loadRMAppState(RMState rmState) throws Exception {\n         assert appState != null;\n         appState.attempts.put(attemptState.getAttemptId(), attemptState);\n       }\n-      LOG.info(\"Done Loading applications from FS state store\");\n+      LOG.info(\"Done loading applications from FS state store\");\n     } catch (Exception e) {\n       LOG.error(\"Failed to load state.\", e);\n       throw e;"
    },
    {
        "commit_id": "3de66011c2e80d7c458a67f80042af986fcc677d",
        "commit_message": "YARN-2450. Fix typos in log messages. Contributed by Ray Chiang.",
        "commit_url": "https://github.com/apache/hadoop/commit/3de66011c2e80d7c458a67f80042af986fcc677d",
        "buggy_code": "LOG.debug(\"Done Loading applications from ZK state store\");",
        "fixed_code": "LOG.debug(\"Done loading applications from ZK state store\");",
        "patch": "@@ -608,7 +608,7 @@ private void loadApplicationAttemptState(ApplicationState appState,\n         appState.attempts.put(attemptState.getAttemptId(), attemptState);\n       }\n     }\n-    LOG.debug(\"Done Loading applications from ZK state store\");\n+    LOG.debug(\"Done loading applications from ZK state store\");\n   }\n \n   @Override"
    },
    {
        "commit_id": "3de66011c2e80d7c458a67f80042af986fcc677d",
        "commit_message": "YARN-2450. Fix typos in log messages. Contributed by Ray Chiang.",
        "commit_url": "https://github.com/apache/hadoop/commit/3de66011c2e80d7c458a67f80042af986fcc677d",
        "buggy_code": "LOG.debug(\"Canceling token \" + tokenWithConf.token.getService());",
        "fixed_code": "LOG.debug(\"Cancelling token \" + tokenWithConf.token.getService());",
        "patch": "@@ -289,7 +289,7 @@ public void run() {\n           tokenWithConf = queue.take();\n           final TokenWithConf current = tokenWithConf;\n           if (LOG.isDebugEnabled()) {\n-            LOG.debug(\"Canceling token \" + tokenWithConf.token.getService());\n+            LOG.debug(\"Cancelling token \" + tokenWithConf.token.getService());\n           }\n           // need to use doAs so that http can find the kerberos tgt\n           UserGroupInformation.getLoginUser()"
    },
    {
        "commit_id": "fef8554be80c01519870ad2969f6c9f3df4d6a7f",
        "commit_message": "Fix typos in log messages. Contributed by Ray Chiang",
        "commit_url": "https://github.com/apache/hadoop/commit/fef8554be80c01519870ad2969f6c9f3df4d6a7f",
        "buggy_code": "LOG.info(\"Canceling commit\");",
        "fixed_code": "LOG.info(\"Cancelling commit\");",
        "patch": "@@ -202,7 +202,7 @@ private synchronized void jobCommitEnded() {\n   private synchronized void cancelJobCommit() {\n     Thread threadCommitting = jobCommitThread;\n     if (threadCommitting != null && threadCommitting.isAlive()) {\n-      LOG.info(\"Canceling commit\");\n+      LOG.info(\"Cancelling commit\");\n       threadCommitting.interrupt();\n \n       // wait up to configured timeout for commit thread to finish"
    },
    {
        "commit_id": "fef8554be80c01519870ad2969f6c9f3df4d6a7f",
        "commit_message": "Fix typos in log messages. Contributed by Ray Chiang",
        "commit_url": "https://github.com/apache/hadoop/commit/fef8554be80c01519870ad2969f6c9f3df4d6a7f",
        "buggy_code": "LOG.info(\"RMCommunicator notified that iSignalled is: \"",
        "fixed_code": "LOG.info(\"RMCommunicator notified that isSignalled is: \"",
        "patch": "@@ -335,7 +335,7 @@ public void setShouldUnregister(boolean shouldUnregister) {\n   \n   public void setSignalled(boolean isSignalled) {\n     this.isSignalled = isSignalled;\n-    LOG.info(\"RMCommunicator notified that iSignalled is: \" \n+    LOG.info(\"RMCommunicator notified that isSignalled is: \" \n         + isSignalled);\n   }\n "
    },
    {
        "commit_id": "fef8554be80c01519870ad2969f6c9f3df4d6a7f",
        "commit_message": "Fix typos in log messages. Contributed by Ray Chiang",
        "commit_url": "https://github.com/apache/hadoop/commit/fef8554be80c01519870ad2969f6c9f3df4d6a7f",
        "buggy_code": "LOG.debug(\"No Space available. Available: \" + availableSize +",
        "fixed_code": "LOG.debug(\"No space available. Available: \" + availableSize +",
        "patch": "@@ -607,7 +607,7 @@ int reserve(int requestedSize) {\n \n     int reserve(int requestedSize, int minSize) {\n       if (availableSize < minSize) {\n-        LOG.debug(\"No Space available. Available: \" + availableSize + \n+        LOG.debug(\"No space available. Available: \" + availableSize + \n             \" MinSize: \" + minSize);\n         return 0;\n       } else {"
    },
    {
        "commit_id": "48aa3b7274b73e022835268123d3711e28e7d48e",
        "commit_message": "Fix typos in log messages. Contributed by Ray Chiang",
        "commit_url": "https://github.com/apache/hadoop/commit/48aa3b7274b73e022835268123d3711e28e7d48e",
        "buggy_code": "LOG.info(\"Canceling commit\");",
        "fixed_code": "LOG.info(\"Cancelling commit\");",
        "patch": "@@ -202,7 +202,7 @@ private synchronized void jobCommitEnded() {\n   private synchronized void cancelJobCommit() {\n     Thread threadCommitting = jobCommitThread;\n     if (threadCommitting != null && threadCommitting.isAlive()) {\n-      LOG.info(\"Canceling commit\");\n+      LOG.info(\"Cancelling commit\");\n       threadCommitting.interrupt();\n \n       // wait up to configured timeout for commit thread to finish"
    },
    {
        "commit_id": "48aa3b7274b73e022835268123d3711e28e7d48e",
        "commit_message": "Fix typos in log messages. Contributed by Ray Chiang",
        "commit_url": "https://github.com/apache/hadoop/commit/48aa3b7274b73e022835268123d3711e28e7d48e",
        "buggy_code": "LOG.info(\"RMCommunicator notified that iSignalled is: \"",
        "fixed_code": "LOG.info(\"RMCommunicator notified that isSignalled is: \"",
        "patch": "@@ -335,7 +335,7 @@ public void setShouldUnregister(boolean shouldUnregister) {\n   \n   public void setSignalled(boolean isSignalled) {\n     this.isSignalled = isSignalled;\n-    LOG.info(\"RMCommunicator notified that iSignalled is: \" \n+    LOG.info(\"RMCommunicator notified that isSignalled is: \" \n         + isSignalled);\n   }\n "
    },
    {
        "commit_id": "48aa3b7274b73e022835268123d3711e28e7d48e",
        "commit_message": "Fix typos in log messages. Contributed by Ray Chiang",
        "commit_url": "https://github.com/apache/hadoop/commit/48aa3b7274b73e022835268123d3711e28e7d48e",
        "buggy_code": "LOG.debug(\"No Space available. Available: \" + availableSize +",
        "fixed_code": "LOG.debug(\"No space available. Available: \" + availableSize +",
        "patch": "@@ -607,7 +607,7 @@ int reserve(int requestedSize) {\n \n     int reserve(int requestedSize, int minSize) {\n       if (availableSize < minSize) {\n-        LOG.debug(\"No Space available. Available: \" + availableSize + \n+        LOG.debug(\"No space available. Available: \" + availableSize + \n             \" MinSize: \" + minSize);\n         return 0;\n       } else {"
    },
    {
        "commit_id": "cfeaf4cd093a83db5c84ce04a4438a2a60663df9",
        "commit_message": "HADOOP-10964. Small fix for NetworkTopologyWithNodeGroup#sortByDistance. Contributed by Yi Liu.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1618103 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/cfeaf4cd093a83db5c84ce04a4438a2a60663df9",
        "buggy_code": "super.sortByDistance(reader, nodes, nodes.length, seed,",
        "fixed_code": "super.sortByDistance(reader, nodes, activeLen, seed,",
        "patch": "@@ -293,7 +293,7 @@ public void sortByDistance(Node reader, Node[] nodes, int activeLen,\n         return;\n       }\n     }\n-    super.sortByDistance(reader, nodes, nodes.length, seed,\n+    super.sortByDistance(reader, nodes, activeLen, seed,\n         randomizeBlockLocationsPerBlock);\n   }\n "
    },
    {
        "commit_id": "0350ea3c72b8cb036f8f066c12f400dd9e45c439",
        "commit_message": "YARN-1918. Typo in description and error message for yarn.resourcemanager.cluster-id (Anandha L Ranganathan via aw)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1618070 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/0350ea3c72b8cb036f8f066c12f400dd9e45c439",
        "buggy_code": "throw new HadoopIllegalArgumentException(\"Configuration doesn't specify\" +",
        "fixed_code": "throw new HadoopIllegalArgumentException(\"Configuration doesn't specify \" +",
        "patch": "@@ -1370,7 +1370,7 @@ public static boolean useHttps(Configuration conf) {\n   public static String getClusterId(Configuration conf) {\n     String clusterId = conf.get(YarnConfiguration.RM_CLUSTER_ID);\n     if (clusterId == null) {\n-      throw new HadoopIllegalArgumentException(\"Configuration doesn't specify\" +\n+      throw new HadoopIllegalArgumentException(\"Configuration doesn't specify \" +\n           YarnConfiguration.RM_CLUSTER_ID);\n     }\n     return clusterId;"
    },
    {
        "commit_id": "2b5e0444246e82093f58a9658b4508f272077379",
        "commit_message": "HDFS-6817. Fix findbugs and other warnings. (yliu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/fs-encryption@1616092 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/2b5e0444246e82093f58a9658b4508f272077379",
        "buggy_code": "private class EncryptionZoneInt {",
        "fixed_code": "private static class EncryptionZoneInt {",
        "patch": "@@ -41,7 +41,7 @@ public class EncryptionZoneManager {\n    * external representation of an EZ is embodied in an EncryptionZone and\n    * contains the EZ's pathname.\n    */\n-  private class EncryptionZoneInt {\n+  private static class EncryptionZoneInt {\n     private final String keyName;\n     private final long inodeId;\n "
    },
    {
        "commit_id": "7ba5913797c49d5001ad95558eadd119c3361060",
        "commit_message": "HDFS-6667. In HDFS HA mode, Distcp/SLive with webhdfs on secure cluster fails with Client cannot authenticate via:[TOKEN, KERBEROS] error. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1611508 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/7ba5913797c49d5001ad95558eadd119c3361060",
        "buggy_code": "public static final String HA_DT_SERVICE_PREFIX = \"ha-hdfs:\";",
        "fixed_code": "public static final String HA_DT_SERVICE_PREFIX = \"ha-\";",
        "patch": "@@ -124,7 +124,7 @@ public static enum DatanodeReportType {\n    * of a delgation token, indicating that the URI is a logical (HA)\n    * URI.\n    */\n-  public static final String HA_DT_SERVICE_PREFIX = \"ha-hdfs:\";\n+  public static final String HA_DT_SERVICE_PREFIX = \"ha-\";\n \n \n   /**"
    },
    {
        "commit_id": "7ba5913797c49d5001ad95558eadd119c3361060",
        "commit_message": "HDFS-6667. In HDFS HA mode, Distcp/SLive with webhdfs on secure cluster fails with Client cannot authenticate via:[TOKEN, KERBEROS] error. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1611508 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/7ba5913797c49d5001ad95558eadd119c3361060",
        "buggy_code": "HAUtil.buildTokenServiceForLogicalUri(uri)",
        "fixed_code": "HAUtil.buildTokenServiceForLogicalUri(uri, getScheme())",
        "patch": "@@ -158,7 +158,7 @@ public synchronized void initialize(URI uri, Configuration conf\n     // getCanonicalUri() in order to handle the case where no port is\n     // specified in the URI\n     this.tokenServiceName = isLogicalUri ?\n-        HAUtil.buildTokenServiceForLogicalUri(uri)\n+        HAUtil.buildTokenServiceForLogicalUri(uri, getScheme())\n         : SecurityUtil.buildTokenService(getCanonicalUri());\n \n     if (!isHA) {"
    },
    {
        "commit_id": "4dcc08b656b0b8f5db61b3befe3daf7b7aa7d288",
        "commit_message": "Addendum patch for HADOOP-10468 TestMetricsSystemImpl.testMultiThreadedPublish fails intermediately. Contributed by Akira AJISAKA\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1610829 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/4dcc08b656b0b8f5db61b3befe3daf7b7aa7d288",
        "buggy_code": "super(c, prefix, \".\");",
        "fixed_code": "super(c, prefix.toLowerCase(Locale.US), \".\");",
        "patch": "@@ -85,7 +85,7 @@ class MetricsConfig extends SubsetConfiguration {\n   private ClassLoader pluginLoader;\n \n   MetricsConfig(Configuration c, String prefix) {\n-    super(c, prefix, \".\");\n+    super(c, prefix.toLowerCase(Locale.US), \".\");\n   }\n \n   static MetricsConfig create(String prefix) {"
    },
    {
        "commit_id": "95897ca14b7abd7fe047fdcf150473a8a2cbb024",
        "commit_message": "YARN-1885. Fixed a bug that RM may not send application-clean-up signal to NMs where the completed applications previously ran in case of RM restart. Contributed by Wangda Tan\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1603028 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/95897ca14b7abd7fe047fdcf150473a8a2cbb024",
        "buggy_code": "YarnVersionInfo.getVersion(), null);",
        "fixed_code": "YarnVersionInfo.getVersion(), null, null);",
        "patch": "@@ -60,7 +60,7 @@ public void testResourceTrackerOnHA() throws Exception {\n     // make sure registerNodeManager works when failover happens\n     RegisterNodeManagerRequest request =\n         RegisterNodeManagerRequest.newInstance(nodeId, 0, resource,\n-            YarnVersionInfo.getVersion(), null);\n+            YarnVersionInfo.getVersion(), null, null);\n     resourceTracker.registerNodeManager(request);\n     Assert.assertTrue(waitForNodeManagerToConnect(10000, nodeId));\n "
    },
    {
        "commit_id": "5dff070e5194aae6fd70526cf7607b26d7ee5d89",
        "commit_message": "HDFS-6356. Fix typo in DatanodeLayoutVersion. Contributed by Tulasi G.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1598408 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/5dff070e5194aae6fd70526cf7607b26d7ee5d89",
        "buggy_code": "FIRST_LAYOUT(-55, -53, \"First datenode layout\", false);",
        "fixed_code": "FIRST_LAYOUT(-55, -53, \"First datanode layout\", false);",
        "patch": "@@ -62,7 +62,7 @@ public static boolean supports(final LayoutFeature f, final int lv) {\n    * </ul>\n    */\n   public static enum Feature implements LayoutFeature {\n-    FIRST_LAYOUT(-55, -53, \"First datenode layout\", false);\n+    FIRST_LAYOUT(-55, -53, \"First datanode layout\", false);\n    \n     private final FeatureInfo info;\n "
    },
    {
        "commit_id": "978e3a6813b142a4ab39e2205cc9d8ba37111a81",
        "commit_message": "HDFS-6443. Fix MiniQJMHACluster related test failures. (Contributed by Zesheng Wu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1597238 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/978e3a6813b142a4ab39e2205cc9d8ba37111a81",
        "buggy_code": "MiniDFSNNTopology topology = MiniQJMHACluster.createDefaultTopology();",
        "fixed_code": "MiniDFSNNTopology topology = MiniQJMHACluster.createDefaultTopology(10000);",
        "patch": "@@ -104,7 +104,7 @@ public void setUpCluster() throws Exception {\n     HAUtil.setAllowStandbyReads(conf, true);\n     \n     if (clusterType == TestType.SHARED_DIR_HA) {\n-      MiniDFSNNTopology topology = MiniQJMHACluster.createDefaultTopology();\n+      MiniDFSNNTopology topology = MiniQJMHACluster.createDefaultTopology(10000);\n       cluster = new MiniDFSCluster.Builder(conf)\n         .nnTopology(topology)\n         .numDataNodes(0)"
    },
    {
        "commit_id": "b2f4e53e2bf1808762669628fb9cdbc13beb4790",
        "commit_message": "HDFS-6409. Fix typo in log message about NameNode layout version upgrade. Contributed by Chen He.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1596739 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/b2f4e53e2bf1808762669628fb9cdbc13beb4790",
        "buggy_code": "+ \"\\\" option if a rolling upgraded is already started;\"",
        "fixed_code": "+ \"\\\" option if a rolling upgrade is already started;\"",
        "patch": "@@ -227,7 +227,7 @@ boolean recoverTransitionRead(StartupOption startOpt, FSNamesystem target,\n           + HdfsConstants.NAMENODE_LAYOUT_VERSION + \" is required.\\n\"\n           + \"Please restart NameNode with the \\\"\"\n           + RollingUpgradeStartupOption.STARTED.getOptionString()\n-          + \"\\\" option if a rolling upgraded is already started;\"\n+          + \"\\\" option if a rolling upgrade is already started;\"\n           + \" or restart NameNode with the \\\"\"\n           + StartupOption.UPGRADE.getName() + \"\\\" option to start\"\n           + \" a new upgrade.\");"
    },
    {
        "commit_id": "41344a4a692073252caf0b2ee4767eaacef2f0c1",
        "commit_message": "YARN-1975. Fix yarn application CLI to print the scheme of the tracking url of failed/killed applications. Contributed by Junping Du\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1593874 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/41344a4a692073252caf0b2ee4767eaacef2f0c1",
        "buggy_code": "WebAppUtils.getResolvedRMWebAppURLWithoutScheme(conf),",
        "fixed_code": "WebAppUtils.getResolvedRMWebAppURLWithScheme(conf),",
        "patch": "@@ -526,7 +526,7 @@ private String generateProxyUriWithScheme(\n \n   private void setTrackingUrlToRMAppPage() {\n     originalTrackingUrl = pjoin(\n-        WebAppUtils.getResolvedRMWebAppURLWithoutScheme(conf),\n+        WebAppUtils.getResolvedRMWebAppURLWithScheme(conf),\n         \"cluster\", \"app\", getAppAttemptId().getApplicationId());\n     proxiedTrackingUrl = originalTrackingUrl;\n   }"
    },
    {
        "commit_id": "41344a4a692073252caf0b2ee4767eaacef2f0c1",
        "commit_message": "YARN-1975. Fix yarn application CLI to print the scheme of the tracking url of failed/killed applications. Contributed by Junping Du\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1593874 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/41344a4a692073252caf0b2ee4767eaacef2f0c1",
        "buggy_code": "WebAppUtils.getResolvedRMWebAppURLWithoutScheme(new Configuration());",
        "fixed_code": "WebAppUtils.getResolvedRMWebAppURLWithScheme(new Configuration());",
        "patch": "@@ -113,7 +113,7 @@ public class TestRMAppAttemptTransitions {\n   \n   private static final String EMPTY_DIAGNOSTICS = \"\";\n   private static final String RM_WEBAPP_ADDR =\n-      WebAppUtils.getResolvedRMWebAppURLWithoutScheme(new Configuration());\n+      WebAppUtils.getResolvedRMWebAppURLWithScheme(new Configuration());\n   \n   private boolean isSecurityEnabled;\n   private RMContext rmContext;"
    },
    {
        "commit_id": "7f635b8f53b59bae141177da80590753ff1eef72",
        "commit_message": "HDFS-6275. Fix warnings - type arguments can be inferred and redudant local variable. Contributed by Suresh Srinivas.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1589510 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/7f635b8f53b59bae141177da80590753ff1eef72",
        "buggy_code": "return Collections.<File>emptyList();",
        "fixed_code": "return Collections.emptyList();",
        "patch": "@@ -263,7 +263,7 @@ private List<File> getLatestEditsFiles() {\n       // the image is already current, discard edits\n       LOG.debug(\n           \"Name checkpoint time is newer than edits, not loading edits.\");\n-      return Collections.<File>emptyList();\n+      return Collections.emptyList();\n     }\n     \n     return getEditsInStorageDir(latestEditsSD);"
    },
    {
        "commit_id": "7f635b8f53b59bae141177da80590753ff1eef72",
        "commit_message": "HDFS-6275. Fix warnings - type arguments can be inferred and redudant local variable. Contributed by Suresh Srinivas.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1589510 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/7f635b8f53b59bae141177da80590753ff1eef72",
        "buggy_code": "Collections.<Long>synchronizedSet(new HashSet<Long>());",
        "fixed_code": "Collections.synchronizedSet(new HashSet<Long>());",
        "patch": "@@ -82,7 +82,7 @@ public class ImageServlet extends HttpServlet {\n   private static final String IMAGE_FILE_TYPE = \"imageFile\";\n \n   private static final Set<Long> currentlyDownloadingCheckpoints =\n-    Collections.<Long>synchronizedSet(new HashSet<Long>());\n+    Collections.synchronizedSet(new HashSet<Long>());\n   \n   @Override\n   public void doGet(final HttpServletRequest request,"
    },
    {
        "commit_id": "7f635b8f53b59bae141177da80590753ff1eef72",
        "commit_message": "HDFS-6275. Fix warnings - type arguments can be inferred and redudant local variable. Contributed by Suresh Srinivas.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1589510 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/7f635b8f53b59bae141177da80590753ff1eef72",
        "buggy_code": "SettableFuture<Void> slowLog = SettableFuture.<Void>create();",
        "fixed_code": "SettableFuture<Void> slowLog = SettableFuture.create();",
        "patch": "@@ -208,7 +208,7 @@ public void testWriteEditsOneSlow() throws Exception {\n         anyLong(), eq(1L), eq(1), Mockito.<byte[]>any());\n     \n     // And the third log not respond\n-    SettableFuture<Void> slowLog = SettableFuture.<Void>create();\n+    SettableFuture<Void> slowLog = SettableFuture.create();\n     Mockito.doReturn(slowLog).when(spyLoggers.get(2)).sendEdits(\n         anyLong(), eq(1L), eq(1), Mockito.<byte[]>any());\n     stm.flush();"
    },
    {
        "commit_id": "7f635b8f53b59bae141177da80590753ff1eef72",
        "commit_message": "HDFS-6275. Fix warnings - type arguments can be inferred and redudant local variable. Contributed by Suresh Srinivas.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1589510 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/7f635b8f53b59bae141177da80590753ff1eef72",
        "buggy_code": "List<File> localPath = Collections.<File>singletonList(",
        "fixed_code": "List<File> localPath = Collections.singletonList(",
        "patch": "@@ -65,7 +65,7 @@ public void testClientSideException() throws IOException {\n     MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf)\n       .numDataNodes(0).build();\n     NNStorage mockStorage = Mockito.mock(NNStorage.class);\n-    List<File> localPath = Collections.<File>singletonList(\n+    List<File> localPath = Collections.singletonList(\n         new File(\"/xxxxx-does-not-exist/blah\"));\n        \n     try {"
    },
    {
        "commit_id": "7f635b8f53b59bae141177da80590753ff1eef72",
        "commit_message": "HDFS-6275. Fix warnings - type arguments can be inferred and redudant local variable. Contributed by Suresh Srinivas.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1589510 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/7f635b8f53b59bae141177da80590753ff1eef72",
        "buggy_code": "return Mockito.<StateChangeRequestInfo>any();",
        "fixed_code": "return Mockito.any();",
        "patch": "@@ -414,6 +414,6 @@ private Object runTool(String ... args) throws Exception {\n   }\n   \n   private StateChangeRequestInfo anyReqInfo() {\n-    return Mockito.<StateChangeRequestInfo>any();\n+    return Mockito.any();\n   }\n }"
    },
    {
        "commit_id": "a5b37c6ed14e92f5a7f7dd76a9a82b3f859fb6dd",
        "commit_message": "HDFS-6181. Fix the wrong property names in NFS user guide. Contributed by Brandon Li\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1585563 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/a5b37c6ed14e92f5a7f7dd76a9a82b3f859fb6dd",
        "buggy_code": "serverPrincipal = DFSConfigKeys.DFS_DATANODE_USER_NAME_KEY)",
        "fixed_code": "serverPrincipal = DFSConfigKeys.DFS_DATANODE_KERBEROS_PRINCIPAL_KEY)",
        "patch": "@@ -34,7 +34,7 @@\n @InterfaceAudience.Private\n @InterfaceStability.Evolving\n @KerberosInfo(\n-    serverPrincipal = DFSConfigKeys.DFS_DATANODE_USER_NAME_KEY)\n+    serverPrincipal = DFSConfigKeys.DFS_DATANODE_KERBEROS_PRINCIPAL_KEY)\n @TokenInfo(BlockTokenSelector.class)\n public interface ClientDatanodeProtocol {\n   /**"
    },
    {
        "commit_id": "a5b37c6ed14e92f5a7f7dd76a9a82b3f859fb6dd",
        "commit_message": "HDFS-6181. Fix the wrong property names in NFS user guide. Contributed by Brandon Li\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1585563 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/a5b37c6ed14e92f5a7f7dd76a9a82b3f859fb6dd",
        "buggy_code": "serverPrincipal = DFSConfigKeys.DFS_NAMENODE_USER_NAME_KEY)",
        "fixed_code": "serverPrincipal = DFSConfigKeys.DFS_NAMENODE_KERBEROS_PRINCIPAL_KEY)",
        "patch": "@@ -64,7 +64,7 @@\n @InterfaceAudience.Private\n @InterfaceStability.Evolving\n @KerberosInfo(\n-    serverPrincipal = DFSConfigKeys.DFS_NAMENODE_USER_NAME_KEY)\n+    serverPrincipal = DFSConfigKeys.DFS_NAMENODE_KERBEROS_PRINCIPAL_KEY)\n @TokenInfo(DelegationTokenSelector.class)\n public interface ClientProtocol {\n "
    },
    {
        "commit_id": "a5b37c6ed14e92f5a7f7dd76a9a82b3f859fb6dd",
        "commit_message": "HDFS-6181. Fix the wrong property names in NFS user guide. Contributed by Brandon Li\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1585563 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/a5b37c6ed14e92f5a7f7dd76a9a82b3f859fb6dd",
        "buggy_code": "serverPrincipal = DFSConfigKeys.DFS_DATANODE_USER_NAME_KEY)",
        "fixed_code": "serverPrincipal = DFSConfigKeys.DFS_DATANODE_KERBEROS_PRINCIPAL_KEY)",
        "patch": "@@ -26,7 +26,7 @@\n import org.apache.hadoop.security.token.TokenInfo;\n \n @KerberosInfo(\n-    serverPrincipal = DFSConfigKeys.DFS_DATANODE_USER_NAME_KEY)\n+    serverPrincipal = DFSConfigKeys.DFS_DATANODE_KERBEROS_PRINCIPAL_KEY)\n @TokenInfo(BlockTokenSelector.class)\n @ProtocolInfo(protocolName = \n     \"org.apache.hadoop.hdfs.protocol.ClientDatanodeProtocol\","
    },
    {
        "commit_id": "a5b37c6ed14e92f5a7f7dd76a9a82b3f859fb6dd",
        "commit_message": "HDFS-6181. Fix the wrong property names in NFS user guide. Contributed by Brandon Li\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1585563 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/a5b37c6ed14e92f5a7f7dd76a9a82b3f859fb6dd",
        "buggy_code": "serverPrincipal = DFSConfigKeys.DFS_NAMENODE_USER_NAME_KEY)",
        "fixed_code": "serverPrincipal = DFSConfigKeys.DFS_NAMENODE_KERBEROS_PRINCIPAL_KEY)",
        "patch": "@@ -31,7 +31,7 @@\n @InterfaceAudience.Private\n @InterfaceStability.Stable\n @KerberosInfo(\n-    serverPrincipal = DFSConfigKeys.DFS_NAMENODE_USER_NAME_KEY)\n+    serverPrincipal = DFSConfigKeys.DFS_NAMENODE_KERBEROS_PRINCIPAL_KEY)\n @TokenInfo(DelegationTokenSelector.class)\n @ProtocolInfo(protocolName = HdfsConstants.CLIENT_NAMENODE_PROTOCOL_NAME, \n     protocolVersion = 1)"
    },
    {
        "commit_id": "a5b37c6ed14e92f5a7f7dd76a9a82b3f859fb6dd",
        "commit_message": "HDFS-6181. Fix the wrong property names in NFS user guide. Contributed by Brandon Li\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1585563 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/a5b37c6ed14e92f5a7f7dd76a9a82b3f859fb6dd",
        "buggy_code": ".get(DFSConfigKeys.DFS_SECONDARY_NAMENODE_USER_NAME_KEY),",
        "fixed_code": ".get(DFSConfigKeys.DFS_SECONDARY_NAMENODE_KERBEROS_PRINCIPAL_KEY),",
        "patch": "@@ -93,7 +93,7 @@ protected boolean isValidRequestor(HttpServletRequest request, Configuration con\n     validRequestors.addAll(DFSUtil.getAllNnPrincipals(conf));\n     validRequestors.add(\n         SecurityUtil.getServerPrincipal(conf\n-            .get(DFSConfigKeys.DFS_SECONDARY_NAMENODE_USER_NAME_KEY),\n+            .get(DFSConfigKeys.DFS_SECONDARY_NAMENODE_KERBEROS_PRINCIPAL_KEY),\n             SecondaryNameNode.getHttpAddress(conf).getHostName()));\n \n     // Check the full principal name of all the configured valid requestors."
    },
    {
        "commit_id": "a5b37c6ed14e92f5a7f7dd76a9a82b3f859fb6dd",
        "commit_message": "HDFS-6181. Fix the wrong property names in NFS user guide. Contributed by Brandon Li\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1585563 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/a5b37c6ed14e92f5a7f7dd76a9a82b3f859fb6dd",
        "buggy_code": "DFSConfigKeys.DFS_JOURNALNODE_USER_NAME_KEY, socAddr.getHostName());",
        "fixed_code": "DFSConfigKeys.DFS_JOURNALNODE_KERBEROS_PRINCIPAL_KEY, socAddr.getHostName());",
        "patch": "@@ -140,7 +140,7 @@ public void start() throws IOException {\n \n     InetSocketAddress socAddr = JournalNodeRpcServer.getAddress(conf);\n     SecurityUtil.login(conf, DFSConfigKeys.DFS_JOURNALNODE_KEYTAB_FILE_KEY,\n-        DFSConfigKeys.DFS_JOURNALNODE_USER_NAME_KEY, socAddr.getHostName());\n+        DFSConfigKeys.DFS_JOURNALNODE_KERBEROS_PRINCIPAL_KEY, socAddr.getHostName());\n     \n     registerJNMXBean();\n     "
    },
    {
        "commit_id": "a5b37c6ed14e92f5a7f7dd76a9a82b3f859fb6dd",
        "commit_message": "HDFS-6181. Fix the wrong property names in NFS user guide. Contributed by Brandon Li\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1585563 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/a5b37c6ed14e92f5a7f7dd76a9a82b3f859fb6dd",
        "buggy_code": "DFSConfigKeys.DFS_JOURNALNODE_INTERNAL_SPNEGO_USER_NAME_KEY,",
        "fixed_code": "DFSConfigKeys.DFS_JOURNALNODE_KERBEROS_INTERNAL_SPNEGO_PRINCIPAL_KEY,",
        "patch": "@@ -58,7 +58,7 @@ void start() throws IOException {\n \n     HttpServer2.Builder builder = DFSUtil.httpServerTemplateForNNAndJN(conf,\n         httpAddr, httpsAddr, \"journal\",\n-        DFSConfigKeys.DFS_JOURNALNODE_INTERNAL_SPNEGO_USER_NAME_KEY,\n+        DFSConfigKeys.DFS_JOURNALNODE_KERBEROS_INTERNAL_SPNEGO_PRINCIPAL_KEY,\n         DFSConfigKeys.DFS_JOURNALNODE_KEYTAB_FILE_KEY);\n \n     httpServer = builder.build();"
    },
    {
        "commit_id": "a5b37c6ed14e92f5a7f7dd76a9a82b3f859fb6dd",
        "commit_message": "HDFS-6181. Fix the wrong property names in NFS user guide. Contributed by Brandon Li\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1585563 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/a5b37c6ed14e92f5a7f7dd76a9a82b3f859fb6dd",
        "buggy_code": "DFS_DATANODE_USER_NAME_KEY);",
        "fixed_code": "DFS_DATANODE_KERBEROS_PRINCIPAL_KEY);",
        "patch": "@@ -1761,7 +1761,7 @@ public static DataNode instantiateDataNode(String args [], Configuration conf,\n     Collection<StorageLocation> dataLocations = getStorageLocations(conf);\n     UserGroupInformation.setConfiguration(conf);\n     SecurityUtil.login(conf, DFS_DATANODE_KEYTAB_FILE_KEY,\n-        DFS_DATANODE_USER_NAME_KEY);\n+        DFS_DATANODE_KERBEROS_PRINCIPAL_KEY);\n     return makeInstance(dataLocations, conf, resources);\n   }\n "
    },
    {
        "commit_id": "a5b37c6ed14e92f5a7f7dd76a9a82b3f859fb6dd",
        "commit_message": "HDFS-6181. Fix the wrong property names in NFS user guide. Contributed by Brandon Li\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1585563 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/a5b37c6ed14e92f5a7f7dd76a9a82b3f859fb6dd",
        "buggy_code": "DFSConfigKeys.DFS_NAMENODE_INTERNAL_SPNEGO_USER_NAME_KEY,",
        "fixed_code": "DFSConfigKeys.DFS_NAMENODE_KERBEROS_INTERNAL_SPNEGO_PRINCIPAL_KEY,",
        "patch": "@@ -110,7 +110,7 @@ void start() throws IOException {\n \n     HttpServer2.Builder builder = DFSUtil.httpServerTemplateForNNAndJN(conf,\n         httpAddr, httpsAddr, \"hdfs\",\n-        DFSConfigKeys.DFS_NAMENODE_INTERNAL_SPNEGO_USER_NAME_KEY,\n+        DFSConfigKeys.DFS_NAMENODE_KERBEROS_INTERNAL_SPNEGO_PRINCIPAL_KEY,\n         DFSConfigKeys.DFS_NAMENODE_KEYTAB_FILE_KEY);\n \n     httpServer = builder.build();"
    },
    {
        "commit_id": "a5b37c6ed14e92f5a7f7dd76a9a82b3f859fb6dd",
        "commit_message": "HDFS-6181. Fix the wrong property names in NFS user guide. Contributed by Brandon Li\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1585563 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/a5b37c6ed14e92f5a7f7dd76a9a82b3f859fb6dd",
        "buggy_code": "serverPrincipal = DFSConfigKeys.DFS_NAMENODE_USER_NAME_KEY)",
        "fixed_code": "serverPrincipal = DFSConfigKeys.DFS_NAMENODE_KERBEROS_PRINCIPAL_KEY)",
        "patch": "@@ -34,7 +34,7 @@\n  * It's used to get part of the name node state\n  *****************************************************************************/\n @KerberosInfo(\n-    serverPrincipal = DFSConfigKeys.DFS_NAMENODE_USER_NAME_KEY)\n+    serverPrincipal = DFSConfigKeys.DFS_NAMENODE_KERBEROS_PRINCIPAL_KEY)\n @InterfaceAudience.Private\n public interface NamenodeProtocol {\n   /**"
    },
    {
        "commit_id": "a5b37c6ed14e92f5a7f7dd76a9a82b3f859fb6dd",
        "commit_message": "HDFS-6181. Fix the wrong property names in NFS user guide. Contributed by Brandon Li\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1585563 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/a5b37c6ed14e92f5a7f7dd76a9a82b3f859fb6dd",
        "buggy_code": "DFSConfigKeys.DFS_NAMENODE_USER_NAME_KEY, \"\");",
        "fixed_code": "DFSConfigKeys.DFS_NAMENODE_KERBEROS_PRINCIPAL_KEY, \"\");",
        "patch": "@@ -66,7 +66,7 @@ public static Configuration addSecurityConfiguration(Configuration conf) {\n     // force loading of hdfs-site.xml.\n     conf = new HdfsConfiguration(conf);\n     String nameNodePrincipal = conf.get(\n-        DFSConfigKeys.DFS_NAMENODE_USER_NAME_KEY, \"\");\n+        DFSConfigKeys.DFS_NAMENODE_KERBEROS_PRINCIPAL_KEY, \"\");\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Using NN principal: \" + nameNodePrincipal);\n     }"
    },
    {
        "commit_id": "a5b37c6ed14e92f5a7f7dd76a9a82b3f859fb6dd",
        "commit_message": "HDFS-6181. Fix the wrong property names in NFS user guide. Contributed by Brandon Li\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1585563 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/a5b37c6ed14e92f5a7f7dd76a9a82b3f859fb6dd",
        "buggy_code": "DFSConfigKeys.DFS_NAMENODE_USER_NAME_KEY, \"\");",
        "fixed_code": "DFSConfigKeys.DFS_NAMENODE_KERBEROS_PRINCIPAL_KEY, \"\");",
        "patch": "@@ -70,7 +70,7 @@ protected InetSocketAddress getProtocolAddress(Configuration conf)\n   public void setConf(Configuration conf) {\n     conf = new HdfsConfiguration(conf);\n     String nameNodePrincipal = conf.get(\n-        DFSConfigKeys.DFS_NAMENODE_USER_NAME_KEY, \"\");\n+        DFSConfigKeys.DFS_NAMENODE_KERBEROS_PRINCIPAL_KEY, \"\");\n     \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Using NN principal: \" + nameNodePrincipal);"
    },
    {
        "commit_id": "a5b37c6ed14e92f5a7f7dd76a9a82b3f859fb6dd",
        "commit_message": "HDFS-6181. Fix the wrong property names in NFS user guide. Contributed by Brandon Li\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1585563 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/a5b37c6ed14e92f5a7f7dd76a9a82b3f859fb6dd",
        "buggy_code": "conf.set(DFSConfigKeys.DFS_NAMENODE_USER_NAME_KEY,",
        "fixed_code": "conf.set(DFSConfigKeys.DFS_NAMENODE_KERBEROS_PRINCIPAL_KEY,",
        "patch": "@@ -58,7 +58,7 @@ public void testName() throws IOException, InterruptedException {\n       Configuration conf = new HdfsConfiguration();\n       conf.set(CommonConfigurationKeys.HADOOP_SECURITY_AUTHENTICATION,\n           \"kerberos\");\n-      conf.set(DFSConfigKeys.DFS_NAMENODE_USER_NAME_KEY,\n+      conf.set(DFSConfigKeys.DFS_NAMENODE_KERBEROS_PRINCIPAL_KEY,\n           \"nn1/localhost@EXAMPLE.COM\");\n       conf.set(DFSConfigKeys.DFS_NAMENODE_KEYTAB_FILE_KEY, nn1KeytabPath);\n "
    },
    {
        "commit_id": "05da90ee2f8e58d836af2246a226e728475b23da",
        "commit_message": "YARN-1898. Addendum patch to ensure /jmx and /metrics are re-directed to Active RM.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1584954 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/05da90ee2f8e58d836af2246a226e728475b23da",
        "buggy_code": "\"/conf\", \"/stacks\", \"/logLevel\", \"/metrics\", \"/jmx\", \"/logs\");",
        "fixed_code": "\"/conf\", \"/stacks\", \"/logLevel\", \"/logs\");",
        "patch": "@@ -46,7 +46,7 @@ public class RMWebAppFilter extends GuiceContainer {\n \n   // define a set of URIs which do not need to do redirection\n   private static final Set<String> NON_REDIRECTED_URIS = Sets.newHashSet(\n-      \"/conf\", \"/stacks\", \"/logLevel\", \"/metrics\", \"/jmx\", \"/logs\");\n+      \"/conf\", \"/stacks\", \"/logLevel\", \"/logs\");\n \n   @Inject\n   public RMWebAppFilter(Injector injector) {"
    },
    {
        "commit_id": "14556cc5d8fee8f8a846e4f65572828553be386c",
        "commit_message": "HDFS-6155. Fix Boxing/unboxing to parse a primitive findbugs warnings. Contributed by Suresh Srinivas.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1582068 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/14556cc5d8fee8f8a846e4f65572828553be386c",
        "buggy_code": "super(Long.valueOf(0));",
        "fixed_code": "super(0L);",
        "patch": "@@ -38,7 +38,7 @@ public class CacheDirectiveIterator\n \n   public CacheDirectiveIterator(ClientProtocol namenode,\n       CacheDirectiveInfo filter) {\n-    super(Long.valueOf(0));\n+    super(0L);\n     this.namenode = namenode;\n     this.filter = filter;\n   }"
    },
    {
        "commit_id": "14556cc5d8fee8f8a846e4f65572828553be386c",
        "commit_message": "HDFS-6155. Fix Boxing/unboxing to parse a primitive findbugs warnings. Contributed by Suresh Srinivas.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1582068 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/14556cc5d8fee8f8a846e4f65572828553be386c",
        "buggy_code": "OOB_TIMEOUT[i] = (i < ele.length) ? Long.valueOf(ele[i]) : 0;",
        "fixed_code": "OOB_TIMEOUT[i] = (i < ele.length) ? Long.parseLong(ele[i]) : 0;",
        "patch": "@@ -52,7 +52,7 @@ public class PipelineAck {\n     String[] ele = conf.get(DFS_DATANODE_OOB_TIMEOUT_KEY,\n         DFS_DATANODE_OOB_TIMEOUT_DEFAULT).split(\",\");\n     for (int i = 0; i < NUM_OOB_TYPES; i++) {\n-      OOB_TIMEOUT[i] = (i < ele.length) ? Long.valueOf(ele[i]) : 0;\n+      OOB_TIMEOUT[i] = (i < ele.length) ? Long.parseLong(ele[i]) : 0;\n     }\n   }\n "
    },
    {
        "commit_id": "14556cc5d8fee8f8a846e4f65572828553be386c",
        "commit_message": "HDFS-6155. Fix Boxing/unboxing to parse a primitive findbugs warnings. Contributed by Suresh Srinivas.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1582068 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/14556cc5d8fee8f8a846e4f65572828553be386c",
        "buggy_code": "long txid = Long.valueOf(matcher.group(1));",
        "fixed_code": "long txid = Long.parseLong(matcher.group(1));",
        "patch": "@@ -165,7 +165,7 @@ private static void purgeMatching(File dir, List<Pattern> patterns,\n         if (matcher.matches()) {\n           // This parsing will always succeed since the group(1) is\n           // /\\d+/ in the regex itself.\n-          long txid = Long.valueOf(matcher.group(1));\n+          long txid = Long.parseLong(matcher.group(1));\n           if (txid < minTxIdToKeep) {\n             LOG.info(\"Purging no-longer needed file \" + txid);\n             if (!f.delete()) {"
    },
    {
        "commit_id": "14556cc5d8fee8f8a846e4f65572828553be386c",
        "commit_message": "HDFS-6155. Fix Boxing/unboxing to parse a primitive findbugs warnings. Contributed by Suresh Srinivas.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1582068 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/14556cc5d8fee8f8a846e4f65572828553be386c",
        "buggy_code": "port = Integer.valueOf(hostLine.substring(idx+1));",
        "fixed_code": "port = Integer.parseInt(hostLine.substring(idx+1));",
        "patch": "@@ -1167,7 +1167,7 @@ private DatanodeID parseDNFromHostsEntry(String hostLine) {\n       port = DFSConfigKeys.DFS_DATANODE_DEFAULT_PORT;\n     } else {\n       hostStr = hostLine.substring(0, idx);\n-      port = Integer.valueOf(hostLine.substring(idx+1));\n+      port = Integer.parseInt(hostLine.substring(idx+1));\n     }\n \n     if (InetAddresses.isInetAddress(hostStr)) {"
    },
    {
        "commit_id": "14556cc5d8fee8f8a846e4f65572828553be386c",
        "commit_message": "HDFS-6155. Fix Boxing/unboxing to parse a primitive findbugs warnings. Contributed by Suresh Srinivas.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1582068 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/14556cc5d8fee8f8a846e4f65572828553be386c",
        "buggy_code": "File baseDir = new File(new String(replicaDirInfo.baseDirPath));",
        "fixed_code": "File baseDir = new File(replicaDirInfo.baseDirPath);",
        "patch": "@@ -187,7 +187,7 @@ private void setDirInternal(File dir) {\n       if (!internedBaseDirs.containsKey(replicaDirInfo.baseDirPath)) {\n         // Create a new String path of this file and make a brand new File object\n         // to guarantee we drop the reference to the underlying char[] storage.\n-        File baseDir = new File(new String(replicaDirInfo.baseDirPath));\n+        File baseDir = new File(replicaDirInfo.baseDirPath);\n         internedBaseDirs.put(replicaDirInfo.baseDirPath, baseDir);\n       }\n       this.baseDir = internedBaseDirs.get(replicaDirInfo.baseDirPath);"
    },
    {
        "commit_id": "14556cc5d8fee8f8a846e4f65572828553be386c",
        "commit_message": "HDFS-6155. Fix Boxing/unboxing to parse a primitive findbugs warnings. Contributed by Suresh Srinivas.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1582068 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/14556cc5d8fee8f8a846e4f65572828553be386c",
        "buggy_code": "id = Long.valueOf(inodeId);",
        "fixed_code": "id = Long.parseLong(inodeId);",
        "patch": "@@ -2943,7 +2943,7 @@ static String resolvePath(String src, byte[][] pathComponents, FSDirectory fsd)\n     final String inodeId = DFSUtil.bytes2String(pathComponents[3]);\n     long id = 0;\n     try {\n-      id = Long.valueOf(inodeId);\n+      id = Long.parseLong(inodeId);\n     } catch (NumberFormatException e) {\n       throw new FileNotFoundException(\"Invalid inode path: \" + src);\n     }"
    },
    {
        "commit_id": "14556cc5d8fee8f8a846e4f65572828553be386c",
        "commit_message": "HDFS-6155. Fix Boxing/unboxing to parse a primitive findbugs warnings. Contributed by Suresh Srinivas.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1582068 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/14556cc5d8fee8f8a846e4f65572828553be386c",
        "buggy_code": "long txid = Long.valueOf(imageMatch.group(1));",
        "fixed_code": "long txid = Long.parseLong(imageMatch.group(1));",
        "patch": "@@ -108,7 +108,7 @@ public void inspectDirectory(StorageDirectory sd) throws IOException {\n       if (imageMatch != null) {\n         if (sd.getStorageDirType().isOfType(NameNodeDirType.IMAGE)) {\n           try {\n-            long txid = Long.valueOf(imageMatch.group(1));\n+            long txid = Long.parseLong(imageMatch.group(1));\n             foundImages.add(new FSImageFile(sd, f, txid));\n           } catch (NumberFormatException nfe) {\n             LOG.error(\"Image file \" + f + \" has improperly formatted \" +"
    },
    {
        "commit_id": "14556cc5d8fee8f8a846e4f65572828553be386c",
        "commit_message": "HDFS-6155. Fix Boxing/unboxing to parse a primitive findbugs warnings. Contributed by Suresh Srinivas.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1582068 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/14556cc5d8fee8f8a846e4f65572828553be386c",
        "buggy_code": "port = Integer.valueOf(portStr);",
        "fixed_code": "port = Integer.parseInt(portStr);",
        "patch": "@@ -105,7 +105,7 @@ static Entry parse(String fileName, String entry) throws IOException {\n         prefix = entry.substring(0, idx);\n         String portStr = entry.substring(idx + 1);\n         try {\n-          port = Integer.valueOf(portStr);\n+          port = Integer.parseInt(portStr);\n         } catch (NumberFormatException e) {\n           throw new IOException(\"unable to parse port number for \" +\n               \"'\" + entry + \"'\", e);"
    },
    {
        "commit_id": "14556cc5d8fee8f8a846e4f65572828553be386c",
        "commit_message": "HDFS-6155. Fix Boxing/unboxing to parse a primitive findbugs warnings. Contributed by Suresh Srinivas.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1582068 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/14556cc5d8fee8f8a846e4f65572828553be386c",
        "buggy_code": "id = Long.valueOf(idString);",
        "fixed_code": "id = Long.parseLong(idString);",
        "patch": "@@ -301,7 +301,7 @@ public int run(Configuration conf, List<String> args) throws IOException {\n       }\n       long id;\n       try {\n-        id = Long.valueOf(idString);\n+        id = Long.parseLong(idString);\n       } catch (NumberFormatException e) {\n         System.err.println(\"Invalid directive ID \" + idString + \": expected \" +\n             \"a numeric value.\");"
    },
    {
        "commit_id": "14556cc5d8fee8f8a846e4f65572828553be386c",
        "commit_message": "HDFS-6155. Fix Boxing/unboxing to parse a primitive findbugs warnings. Contributed by Suresh Srinivas.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1582068 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/14556cc5d8fee8f8a846e4f65572828553be386c",
        "buggy_code": "fileSize += Long.valueOf(value);",
        "fixed_code": "fileSize += Long.parseLong(value);",
        "patch": "@@ -144,7 +144,7 @@ void visit(ImageElement element, String value) throws IOException {\n     \n     // Special case of file size, which is sum of the num bytes in each block\n     if(element == ImageElement.NUM_BYTES)\n-      fileSize += Long.valueOf(value);\n+      fileSize += Long.parseLong(value);\n     \n     if(elements.containsKey(element) && element != ImageElement.NUM_BYTES)\n       elements.put(element, value);"
    },
    {
        "commit_id": "14556cc5d8fee8f8a846e4f65572828553be386c",
        "commit_message": "HDFS-6155. Fix Boxing/unboxing to parse a primitive findbugs warnings. Contributed by Suresh Srinivas.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1582068 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/14556cc5d8fee8f8a846e4f65572828553be386c",
        "buggy_code": "long imageTxId = Long.valueOf(imageMatch.group(1));",
        "fixed_code": "long imageTxId = Long.parseLong(imageMatch.group(1));",
        "patch": "@@ -107,7 +107,7 @@ private static File getHighestFsImageOnCluster(MiniDFSCluster cluster) {\n       for (File imageFile : new File(new File(nameDir), \"current\").listFiles()) {\n         Matcher imageMatch = IMAGE_REGEX.matcher(imageFile.getName());\n         if (imageMatch.matches()) {\n-          long imageTxId = Long.valueOf(imageMatch.group(1));\n+          long imageTxId = Long.parseLong(imageMatch.group(1));\n           if (imageTxId > highestImageTxId) {\n             highestImageTxId = imageTxId;\n             highestImageOnNn = imageFile;"
    },
    {
        "commit_id": "14556cc5d8fee8f8a846e4f65572828553be386c",
        "commit_message": "HDFS-6155. Fix Boxing/unboxing to parse a primitive findbugs warnings. Contributed by Suresh Srinivas.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1582068 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/14556cc5d8fee8f8a846e4f65572828553be386c",
        "buggy_code": "int threadCount = Integer.valueOf(args[2]);",
        "fixed_code": "int threadCount = Integer.parseInt(args[2]);",
        "patch": "@@ -518,7 +518,7 @@ public static void main(String[] args) throws Exception {\n     }\n     boolean shortcircuit = Boolean.valueOf(args[0]);\n     boolean checksum = Boolean.valueOf(args[1]);\n-    int threadCount = Integer.valueOf(args[2]);\n+    int threadCount = Integer.parseInt(args[2]);\n \n     // Setup create a file\n     final Configuration conf = new Configuration();"
    },
    {
        "commit_id": "14556cc5d8fee8f8a846e4f65572828553be386c",
        "commit_message": "HDFS-6155. Fix Boxing/unboxing to parse a primitive findbugs warnings. Contributed by Suresh Srinivas.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1582068 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/14556cc5d8fee8f8a846e4f65572828553be386c",
        "buggy_code": "Integer.valueOf(digitLine) == expectedCount);",
        "fixed_code": "Integer.parseInt(digitLine) == expectedCount);",
        "patch": "@@ -316,7 +316,7 @@ private void assertCounts(DataNodeStatus dataNodeStatus, String output,\n       String digitLine = output.substring(matcher.start(), matcher.end())\n           .trim();\n       assertTrue(\"assertCounts error. actual != expected\",\n-          Integer.valueOf(digitLine) == expectedCount);\n+          Integer.parseInt(digitLine) == expectedCount);\n     } else {\n       fail(\"assertCount matcher error\");\n     }"
    },
    {
        "commit_id": "14556cc5d8fee8f8a846e4f65572828553be386c",
        "commit_message": "HDFS-6155. Fix Boxing/unboxing to parse a primitive findbugs warnings. Contributed by Suresh Srinivas.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1582068 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/14556cc5d8fee8f8a846e4f65572828553be386c",
        "buggy_code": "Long.valueOf(EXP_DATE) == DelegationTokenFetcher.renewDelegationToken(",
        "fixed_code": "Long.parseLong(EXP_DATE) == DelegationTokenFetcher.renewDelegationToken(",
        "patch": "@@ -186,7 +186,7 @@ public void testRenewTokenFromHttp() throws IOException,\n       NumberFormatException, AuthenticationException {\n     bootstrap = startHttpServer(httpPort, testToken, serviceUrl);\n     assertTrue(\"testRenewTokenFromHttp error\",\n-        Long.valueOf(EXP_DATE) == DelegationTokenFetcher.renewDelegationToken(\n+        Long.parseLong(EXP_DATE) == DelegationTokenFetcher.renewDelegationToken(\n             connectionFactory, serviceUrl, testToken));\n     if (assertionError != null)\n       throw assertionError;"
    },
    {
        "commit_id": "a126a01fa197beebe955837c8f2efbd3257f7aa5",
        "commit_message": "HADOOP-10437. Fix the javac warnings in the conf and the util package.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1582015 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/a126a01fa197beebe955837c8f2efbd3257f7aa5",
        "buggy_code": "JarEntry entry = (JarEntry)entries.nextElement();",
        "fixed_code": "final JarEntry entry = entries.nextElement();",
        "patch": "@@ -78,7 +78,7 @@ public static void unJar(File jarFile, File toDir, Pattern unpackRegex)\n     try {\n       Enumeration<JarEntry> entries = jar.entries();\n       while (entries.hasMoreElements()) {\n-        JarEntry entry = (JarEntry)entries.nextElement();\n+        final JarEntry entry = entries.nextElement();\n         if (!entry.isDirectory() &&\n             unpackRegex.matcher(entry.getName()).matches()) {\n           InputStream in = jar.getInputStream(entry);"
    },
    {
        "commit_id": "a126a01fa197beebe955837c8f2efbd3257f7aa5",
        "commit_message": "HADOOP-10437. Fix the javac warnings in the conf and the util package.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1582015 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/a126a01fa197beebe955837c8f2efbd3257f7aa5",
        "buggy_code": "while ((nextIndex = str.indexOf((int)separator, startIndex)) != -1) {",
        "fixed_code": "while ((nextIndex = str.indexOf(separator, startIndex)) != -1) {",
        "patch": "@@ -431,7 +431,7 @@ public static String[] split(\n     ArrayList<String> strList = new ArrayList<String>();\n     int startIndex = 0;\n     int nextIndex = 0;\n-    while ((nextIndex = str.indexOf((int)separator, startIndex)) != -1) {\n+    while ((nextIndex = str.indexOf(separator, startIndex)) != -1) {\n       strList.add(str.substring(startIndex, nextIndex));\n       startIndex = nextIndex + 1;\n     }"
    },
    {
        "commit_id": "234edcadd5d811ca38dfc6d04626861aaaa36f28",
        "commit_message": "HADOOP-10221. Add file missed in previous checkin, fix typo.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1579387 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/234edcadd5d811ca38dfc6d04626861aaaa36f28",
        "buggy_code": "return new SaslRpcServer(authMethod).create(this ,saslProps, secretManager);",
        "fixed_code": "return new SaslRpcServer(authMethod).create(this, saslProps, secretManager);",
        "patch": "@@ -1641,7 +1641,7 @@ private SaslServer createSaslServer(AuthMethod authMethod)\n         throws IOException, InterruptedException {\n       final Map<String,?> saslProps =\n                   saslPropsResolver.getServerProperties(addr);\n-      return new SaslRpcServer(authMethod).create(this ,saslProps, secretManager);\n+      return new SaslRpcServer(authMethod).create(this, saslProps, secretManager);\n     }\n     \n     /**"
    },
    {
        "commit_id": "81a456e6386e1241c14b3b6b62dc2e5d445f5ee7",
        "commit_message": "YARN-1591. Fixed AsyncDispatcher to handle interrupts on shutdown in a sane manner and thus fix failure of TestResourceTrackerService. Contributed by Tsuyoshi Ozawa.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1578628 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/81a456e6386e1241c14b3b6b62dc2e5d445f5ee7",
        "buggy_code": "throw new YarnRuntimeException(e);",
        "fixed_code": "LOG.info(\"Interrupted. Trying to exit gracefully.\");",
        "patch": "@@ -630,7 +630,7 @@ public void handle(SchedulerEvent event) {\n         }\n         this.eventQueue.put(event);\n       } catch (InterruptedException e) {\n-        throw new YarnRuntimeException(e);\n+        LOG.info(\"Interrupted. Trying to exit gracefully.\");\n       }\n     }\n   }"
    },
    {
        "commit_id": "37cb314f79f515421cfc2c3605382bf1534dc266",
        "commit_message": "HADOOP-10407. Fix the javac warnings in org.apache.hadoop.ipc package.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1577710 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/37cb314f79f515421cfc2c3605382bf1534dc266",
        "buggy_code": "final OutputStream out2) throws IOException, InterruptedException {",
        "fixed_code": "final OutputStream out2) throws IOException {",
        "patch": "@@ -541,7 +541,7 @@ private synchronized boolean shouldAuthenticateOverKrb() throws IOException {\n     }\n     \n     private synchronized AuthMethod setupSaslConnection(final InputStream in2, \n-        final OutputStream out2) throws IOException, InterruptedException {\n+        final OutputStream out2) throws IOException {\n       // Do not use Client.conf here! We must use ConnectionId.conf, since the\n       // Client object is cached and shared between all RPC clients, even those\n       // for separate services."
    },
    {
        "commit_id": "b7428fe63d80ce150a964fae427f13c161f39164",
        "commit_message": "HADOOP-10393. Fix the javac warnings in hadoop-auth.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1575470 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/b7428fe63d80ce150a964fae427f13c161f39164",
        "buggy_code": "private void sendToken(byte[] outToken) throws IOException, AuthenticationException {",
        "fixed_code": "private void sendToken(byte[] outToken) throws IOException {",
        "patch": "@@ -313,7 +313,7 @@ public Void run() throws Exception {\n   /*\n   * Sends the Kerberos token to the server.\n   */\n-  private void sendToken(byte[] outToken) throws IOException, AuthenticationException {\n+  private void sendToken(byte[] outToken) throws IOException {\n     String token = base64.encodeToString(outToken);\n     conn = (HttpURLConnection) url.openConnection();\n     if (connConfigurator != null) {"
    },
    {
        "commit_id": "1fe2bd55341297db698c2b4d03511cdff07c439c",
        "commit_message": "HDFS-6028. Print clearer error message when user attempts to delete required mask entry from ACL. Contributed by Chris Nauroth.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1572753 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/1fe2bd55341297db698c2b4d03511cdff07c439c",
        "buggy_code": "\"Invalid ACL: mask is required, but it was deleted.\");",
        "fixed_code": "\"Invalid ACL: mask is required and cannot be deleted.\");",
        "patch": "@@ -365,7 +365,7 @@ private static void calculateMasks(List<AclEntry> aclBuilder,\n           maskDirty.contains(scope)) {\n         // Caller explicitly removed mask entry, but it's required.\n         throw new AclException(\n-          \"Invalid ACL: mask is required, but it was deleted.\");\n+          \"Invalid ACL: mask is required and cannot be deleted.\");\n       } else if (providedMask.containsKey(scope) &&\n           (!scopeDirty.contains(scope) || maskDirty.contains(scope))) {\n         // Caller explicitly provided new mask, or we are preserving the existing"
    },
    {
        "commit_id": "c080fec82be6866ab8d69540fe6ee147b2a0108d",
        "commit_message": "HDFS-3969. Small bug fixes and improvements for disk locations API. Contributed by Todd Lipcon and Andrew Wang.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1572284 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/c080fec82be6866ab8d69540fe6ee147b2a0108d",
        "buggy_code": "dataNodes.add(new DataNodeProperties(dn, newconf, dnArgs, secureResources));",
        "fixed_code": "dataNodes.add(new DataNodeProperties(dn, newconf, dnArgs, secureResources, dn.getIpcPort()));",
        "patch": "@@ -178,7 +178,7 @@ public synchronized void startDataNodes(Configuration conf, int numDataNodes,\n         }\n       }\n       dn.runDatanodeDaemon();\n-      dataNodes.add(new DataNodeProperties(dn, newconf, dnArgs, secureResources));\n+      dataNodes.add(new DataNodeProperties(dn, newconf, dnArgs, secureResources, dn.getIpcPort()));\n     }\n     curDatanodesNum += numDataNodes;\n     this.numDataNodes += numDataNodes;"
    },
    {
        "commit_id": "c080fec82be6866ab8d69540fe6ee147b2a0108d",
        "commit_message": "HDFS-3969. Small bug fixes and improvements for disk locations API. Contributed by Todd Lipcon and Andrew Wang.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1572284 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/c080fec82be6866ab8d69540fe6ee147b2a0108d",
        "buggy_code": "public HdfsBlocksMetadata getHdfsBlocksMetadata(List<ExtendedBlock> blocks)",
        "fixed_code": "public HdfsBlocksMetadata getHdfsBlocksMetadata(String bpid, long[] blockIds)",
        "patch": "@@ -1049,7 +1049,7 @@ public BlockLocalPathInfo getBlockLocalPathInfo(ExtendedBlock b) {\n   }\n \n   @Override\n-  public HdfsBlocksMetadata getHdfsBlocksMetadata(List<ExtendedBlock> blocks)\n+  public HdfsBlocksMetadata getHdfsBlocksMetadata(String bpid, long[] blockIds)\n       throws IOException {\n     throw new UnsupportedOperationException();\n   }"
    },
    {
        "commit_id": "c080fec82be6866ab8d69540fe6ee147b2a0108d",
        "commit_message": "HDFS-3969. Small bug fixes and improvements for disk locations API. Contributed by Todd Lipcon and Andrew Wang.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1572284 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/c080fec82be6866ab8d69540fe6ee147b2a0108d",
        "buggy_code": "static void waitForNNToIssueDeletions(final NameNode nn)",
        "fixed_code": "public static void waitForNNToIssueDeletions(final NameNode nn)",
        "patch": "@@ -111,7 +111,7 @@ public Boolean get() {\n    * Wait for the NameNode to issue any deletions that are already\n    * pending (i.e. for the pendingDeletionBlocksCount to go to 0)\n    */\n-  static void waitForNNToIssueDeletions(final NameNode nn)\n+  public static void waitForNNToIssueDeletions(final NameNode nn)\n       throws Exception {\n     GenericTestUtils.waitFor(new Supplier<Boolean>() {\n       @Override"
    },
    {
        "commit_id": "c7142e77617e9074a10d6c5daf9776b7af8ecc50",
        "commit_message": "YARN-1561. Fix a generic type warning in FairScheduler. (Chen He via junping_du)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1571924 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/c7142e77617e9074a10d6c5daf9776b7af8ecc50",
        "buggy_code": "private Comparator nodeAvailableResourceComparator =",
        "fixed_code": "private Comparator<NodeId> nodeAvailableResourceComparator =",
        "patch": "@@ -175,7 +175,7 @@ public class FairScheduler extends AbstractYarnScheduler {\n   protected WeightAdjuster weightAdjuster; // Can be null for no weight adjuster\n   protected boolean continuousSchedulingEnabled; // Continuous Scheduling enabled or not\n   protected int continuousSchedulingSleepMs; // Sleep time for each pass in continuous scheduling\n-  private Comparator nodeAvailableResourceComparator =\n+  private Comparator<NodeId> nodeAvailableResourceComparator =\n           new NodeAvailableResourceComparator(); // Node available resource comparator\n   protected double nodeLocalityThreshold; // Cluster threshold for node locality\n   protected double rackLocalityThreshold; // Cluster threshold for rack locality"
    },
    {
        "commit_id": "329c7051817c956bfc64661f4e1349b7009a2747",
        "commit_message": "HDFS-5987. Fix findbugs warnings in Rolling Upgrade branch. (Contributed by szetszwo)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-5535@1570389 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/329c7051817c956bfc64661f4e1349b7009a2747",
        "buggy_code": "public void discardSegments(long startTxId) throws IOException {",
        "fixed_code": "synchronized void discardSegments(long startTxId) throws IOException {",
        "patch": "@@ -1037,7 +1037,7 @@ public void doRollback() throws IOException {\n     storage.getJournalManager().doRollback();\n   }\n \n-  public void discardSegments(long startTxId) throws IOException {\n+  synchronized void discardSegments(long startTxId) throws IOException {\n     storage.getJournalManager().discardSegments(startTxId);\n     // we delete all the segments after the startTxId. let's reset committedTxnId \n     committedTxnId.set(startTxId - 1);"
    },
    {
        "commit_id": "077adb25b7fb3d8d0dbd93096a3f72418ea6521c",
        "commit_message": "HDFS-5979. Typo and logger fix for fsimage PB code. (wang)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1570070 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/077adb25b7fb3d8d0dbd93096a3f72418ea6521c",
        "buggy_code": "private static final Log LOG = LogFactory.getLog(FSImageFormatProtobuf.class);",
        "fixed_code": "private static final Log LOG = LogFactory.getLog(FSImageFormatPBINode.class);",
        "patch": "@@ -75,7 +75,7 @@ public final class FSImageFormatPBINode {\n   private static final AclEntryType[] ACL_ENTRY_TYPE_VALUES = AclEntryType\n       .values();\n \n-  private static final Log LOG = LogFactory.getLog(FSImageFormatProtobuf.class);\n+  private static final Log LOG = LogFactory.getLog(FSImageFormatPBINode.class);\n \n   public final static class Loader {\n     public static PermissionStatus loadPermission(long id,"
    },
    {
        "commit_id": "077adb25b7fb3d8d0dbd93096a3f72418ea6521c",
        "commit_message": "HDFS-5979. Typo and logger fix for fsimage PB code. (wang)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1570070 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/077adb25b7fb3d8d0dbd93096a3f72418ea6521c",
        "buggy_code": "LOG.warn(\"Unregconized section \" + n);",
        "fixed_code": "LOG.warn(\"Unrecognized section \" + n);",
        "patch": "@@ -268,7 +268,7 @@ public int compare(FileSummary.Section s1, FileSummary.Section s2) {\n         }\n           break;\n         default:\n-          LOG.warn(\"Unregconized section \" + n);\n+          LOG.warn(\"Unrecognized section \" + n);\n           break;\n         }\n       }"
    },
    {
        "commit_id": "377424e36a25ab34bba9aaed5feaae9d293eb57f",
        "commit_message": "HDFS-5966. Fix rollback of rolling upgrade in NameNode HA setup.  Contributed by jing9\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-5535@1569885 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/377424e36a25ab34bba9aaed5feaae9d293eb57f",
        "buggy_code": "private static String NAMESERVICE = \"ns1\";",
        "fixed_code": "public static String NAMESERVICE = \"ns1\";",
        "patch": "@@ -38,7 +38,7 @@ public class MiniQJMHACluster {\n   private MiniJournalCluster journalCluster;\n   private final Configuration conf;\n   \n-  private static String NAMESERVICE = \"ns1\";\n+  public static String NAMESERVICE = \"ns1\";\n   private static final String NN1 = \"nn1\";\n   private static final String NN2 = \"nn2\";\n   private static final int NN1_IPC_PORT = 10000;"
    },
    {
        "commit_id": "bf5971b86a042076ff50add2ec8f90ae6198d3ca",
        "commit_message": "HDFS-5959. Fix typo at section name in FSImageFormatProtobuf.java. Contributed by Akira Ajisaka.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1569156 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/bf5971b86a042076ff50add2ec8f90ae6198d3ca",
        "buggy_code": "parent.commitSection(headers, SectionName.INODE_REFRENCE);",
        "fixed_code": "parent.commitSection(headers, SectionName.INODE_REFERENCE);",
        "patch": "@@ -383,7 +383,7 @@ public void serializeINodeReferenceSection(OutputStream out)\n         INodeReferenceSection.INodeReference.Builder rb = buildINodeReference(ref);\n         rb.build().writeDelimitedTo(out);\n       }\n-      parent.commitSection(headers, SectionName.INODE_REFRENCE);\n+      parent.commitSection(headers, SectionName.INODE_REFERENCE);\n     }\n \n     private INodeReferenceSection.INodeReference.Builder buildINodeReference("
    },
    {
        "commit_id": "bf5971b86a042076ff50add2ec8f90ae6198d3ca",
        "commit_message": "HDFS-5959. Fix typo at section name in FSImageFormatProtobuf.java. Contributed by Akira Ajisaka.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1569156 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/bf5971b86a042076ff50add2ec8f90ae6198d3ca",
        "buggy_code": "case INODE_REFRENCE:",
        "fixed_code": "case INODE_REFERENCE:",
        "patch": "@@ -115,7 +115,7 @@ public int compare(FileSummary.Section s1, FileSummary.Section s2) {\n         case INODE:\n           loadINodeSection(is);\n           break;\n-        case INODE_REFRENCE:\n+        case INODE_REFERENCE:\n           loadINodeReferenceSection(is);\n           break;\n         case INODE_DIR:"
    },
    {
        "commit_id": "bf5971b86a042076ff50add2ec8f90ae6198d3ca",
        "commit_message": "HDFS-5959. Fix typo at section name in FSImageFormatProtobuf.java. Contributed by Akira Ajisaka.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1569156 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/bf5971b86a042076ff50add2ec8f90ae6198d3ca",
        "buggy_code": "case INODE_REFRENCE:",
        "fixed_code": "case INODE_REFERENCE:",
        "patch": "@@ -113,7 +113,7 @@ public int compare(FileSummary.Section s1, FileSummary.Section s2) {\n         case INODE:\n           dumpINodeSection(is);\n           break;\n-        case INODE_REFRENCE:\n+        case INODE_REFERENCE:\n           dumpINodeReferenceSection(is);\n           break;\n         case INODE_DIR:"
    },
    {
        "commit_id": "a795bc42d012bf75872ae412cb2644c2d80177e3",
        "commit_message": "HDFS-5494. Merge Protobuf-based-FSImage code from trunk - fix build break after merge. (Contributed by Jing Zhao)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-5535@1568517 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/a795bc42d012bf75872ae412cb2644c2d80177e3",
        "buggy_code": ".setLayoutVersion(LayoutVersion.getCurrentLayoutVersion());",
        "fixed_code": ".setLayoutVersion(NameNodeLayoutVersion.CURRENT_LAYOUT_VERSION);",
        "patch": "@@ -428,7 +428,7 @@ private void saveInternal(FileOutputStream fout,\n \n       FileSummary.Builder b = FileSummary.newBuilder()\n           .setOndiskVersion(FSImageUtil.FILE_VERSION)\n-          .setLayoutVersion(LayoutVersion.getCurrentLayoutVersion());\n+          .setLayoutVersion(NameNodeLayoutVersion.CURRENT_LAYOUT_VERSION);\n \n       codec = compression.getImageCodec();\n       if (codec != null) {"
    },
    {
        "commit_id": "a795bc42d012bf75872ae412cb2644c2d80177e3",
        "commit_message": "HDFS-5494. Merge Protobuf-based-FSImage code from trunk - fix build break after merge. (Contributed by Jing Zhao)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-5535@1568517 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/a795bc42d012bf75872ae412cb2644c2d80177e3",
        "buggy_code": "if (!LayoutVersion.supports(Feature.PROTOBUF_FORMAT,",
        "fixed_code": "if (!NameNodeLayoutVersion.supports(Feature.PROTOBUF_FORMAT,",
        "patch": "@@ -71,7 +71,7 @@ public static FileSummary loadSummary(RandomAccessFile file)\n           + summary.getOndiskVersion());\n     }\n \n-    if (!LayoutVersion.supports(Feature.PROTOBUF_FORMAT,\n+    if (!NameNodeLayoutVersion.supports(Feature.PROTOBUF_FORMAT,\n         summary.getLayoutVersion())) {\n       throw new IOException(\"Unsupported layout version \"\n           + summary.getLayoutVersion());"
    },
    {
        "commit_id": "990cffdcfa9349fff0cee144b1d0e5267c40f63d",
        "commit_message": "YARN-1553. Modified YARN and MR to stop using HttpConfig.isSecure() and\ninstead rely on the http policy framework. And also fix some bugs related\nto https handling in YARN web-apps. Contributed by Haohui Mai.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1568501 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/990cffdcfa9349fff0cee144b1d0e5267c40f63d",
        "buggy_code": "nodeLink == null ? \"#\" : url(HttpConfig.getSchemePrefix(), nodeLink))",
        "fixed_code": "nodeLink == null ? \"#\" : url(\"//\", nodeLink))",
        "patch": "@@ -163,7 +163,7 @@ protected void render(Block html) {\n         .append(startTime)\n         .append(\"\\\",\\\"<a href='\")\n         .append(\n-          nodeLink == null ? \"#\" : url(HttpConfig.getSchemePrefix(), nodeLink))\n+          nodeLink == null ? \"#\" : url(\"//\", nodeLink))\n         .append(\"'>\")\n         .append(\n           nodeLink == null ? \"N/A\" : StringEscapeUtils"
    },
    {
        "commit_id": "990cffdcfa9349fff0cee144b1d0e5267c40f63d",
        "commit_message": "YARN-1553. Modified YARN and MR to stop using HttpConfig.isSecure() and\ninstead rely on the http policy framework. And also fix some bugs related\nto https handling in YARN web-apps. Contributed by Haohui Mai.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1568501 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/990cffdcfa9349fff0cee144b1d0e5267c40f63d",
        "buggy_code": "for (String confKey : YarnConfiguration.RM_SERVICES_ADDRESS_CONF_KEYS) {",
        "fixed_code": "for (String confKey : YarnConfiguration.getServiceAddressConfKeys(conf)) {",
        "patch": "@@ -253,7 +253,7 @@ private void setNonHARMConfiguration(Configuration conf) {\n \n   private void setHARMConfiguration(final int index, Configuration conf) {\n     String hostname = MiniYARNCluster.getHostname();\n-    for (String confKey : YarnConfiguration.RM_SERVICES_ADDRESS_CONF_KEYS) {\n+    for (String confKey : YarnConfiguration.getServiceAddressConfKeys(conf)) {\n       conf.set(HAUtil.addSuffix(confKey, rmIds[index]), hostname + \":0\");\n     }\n   }"
    },
    {
        "commit_id": "a8c780d378df86aafba09751c0c43dd4e0d54c0a",
        "commit_message": "YARN-1673. Fix option parsing in YARN's application CLI after it is broken by YARN-967. Contributed by Mayank Bansal.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1564188 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/a8c780d378df86aafba09751c0c43dd4e0d54c0a",
        "buggy_code": "if (args.length != 2) {",
        "fixed_code": "if (args.length != 3) {",
        "patch": "@@ -197,7 +197,7 @@ public int run(String[] args) throws Exception {\n         listApplications(appTypes, appStates);\n       }\n     } else if (cliParser.hasOption(KILL_CMD)) {\n-      if (args.length != 2) {\n+      if (args.length != 3) {\n         printUsage(opts);\n         return exitCode;\n       }"
    },
    {
        "commit_id": "cb5e0787a6fc0b0748753b7e7c4c3fdbfd2714b2",
        "commit_message": "YARN-1498 addendum to fix findbugs warning\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1564018 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/cb5e0787a6fc0b0748753b7e7c4c3fdbfd2714b2",
        "buggy_code": "public void move(Queue newQueue) {",
        "fixed_code": "public synchronized void move(Queue newQueue) {",
        "patch": "@@ -432,7 +432,7 @@ public synchronized void transferStateFromPreviousAttempt(\n       .transferStateFromPreviousAppSchedulingInfo(appAttempt.appSchedulingInfo);\n   }\n   \n-  public void move(Queue newQueue) {\n+  public synchronized void move(Queue newQueue) {\n     QueueMetrics oldMetrics = queue.getMetrics();\n     QueueMetrics newMetrics = newQueue.getMetrics();\n     String user = getUser();"
    },
    {
        "commit_id": "52372eba8e06150185661d74d7c903391c542190",
        "commit_message": "HADOOP-10274 Lower the logging level from ERROR to WARN for UGI.doAs method (Takeshi Miao via stack)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1561934 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/52372eba8e06150185661d74d7c903391c542190",
        "buggy_code": "LOG.error(\"PriviledgedActionException as:\"+this+\" cause:\"+cause);",
        "fixed_code": "LOG.warn(\"PriviledgedActionException as:\"+this+\" cause:\"+cause);",
        "patch": "@@ -1560,7 +1560,7 @@ public <T> T doAs(PrivilegedExceptionAction<T> action\n       return Subject.doAs(subject, action);\n     } catch (PrivilegedActionException pae) {\n       Throwable cause = pae.getCause();\n-      LOG.error(\"PriviledgedActionException as:\"+this+\" cause:\"+cause);\n+      LOG.warn(\"PriviledgedActionException as:\"+this+\" cause:\"+cause);\n       if (cause instanceof IOException) {\n         throw (IOException) cause;\n       } else if (cause instanceof Error) {"
    },
    {
        "commit_id": "602f71a8daa0dc98d0183028935e3b10460e28a5",
        "commit_message": "HDFS-5800. Fix a typo in DFSClient.renewLease().  Contributed by Kousuke Saruta\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1559701 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/602f71a8daa0dc98d0183028935e3b10460e28a5",
        "buggy_code": "+ (elapsed/1000) + \" seconds (>= soft-limit =\"",
        "fixed_code": "+ (elapsed/1000) + \" seconds (>= hard-limit =\"",
        "patch": "@@ -770,7 +770,7 @@ boolean renewLease() throws IOException {\n         final long elapsed = Time.now() - getLastLeaseRenewal();\n         if (elapsed > HdfsConstants.LEASE_HARDLIMIT_PERIOD) {\n           LOG.warn(\"Failed to renew lease for \" + clientName + \" for \"\n-              + (elapsed/1000) + \" seconds (>= soft-limit =\"\n+              + (elapsed/1000) + \" seconds (>= hard-limit =\"\n               + (HdfsConstants.LEASE_HARDLIMIT_PERIOD/1000) + \" seconds.) \"\n               + \"Closing all files being written ...\", e);\n           closeAllFilesBeingWritten(true);"
    },
    {
        "commit_id": "349f25a13225d6a240b577d988a8b1ac6a722578",
        "commit_message": "HADOOP-10236. Fix typo in o.a.h.ipc.Client#checkResponse. Contributed by Akira Ajisaka.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1558498 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/349f25a13225d6a240b577d988a8b1ac6a722578",
        "buggy_code": "+ StringUtils.byteToHexString(clientId) + \", ID in reponse=\"",
        "fixed_code": "+ StringUtils.byteToHexString(clientId) + \", ID in response=\"",
        "patch": "@@ -286,7 +286,7 @@ void checkResponse(RpcResponseHeaderProto header) throws IOException {\n       if (!Arrays.equals(id, RpcConstants.DUMMY_CLIENT_ID)) {\n         if (!Arrays.equals(id, clientId)) {\n           throw new IOException(\"Client IDs not matched: local ID=\"\n-              + StringUtils.byteToHexString(clientId) + \", ID in reponse=\"\n+              + StringUtils.byteToHexString(clientId) + \", ID in response=\"\n               + StringUtils.byteToHexString(header.getClientId().toByteArray()));\n         }\n       }"
    },
    {
        "commit_id": "519d5d3014c2a4c77d4b3b575bc34807e7c0ec50",
        "commit_message": "HADOOP-10214. Fix multithreaded correctness warnings in ActiveStandbyElector (Liang Xie via kasha)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1556734 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/519d5d3014c2a4c77d4b3b575bc34807e7c0ec50",
        "buggy_code": "public void terminateConnection() {",
        "fixed_code": "public synchronized void terminateConnection() {",
        "patch": "@@ -768,7 +768,7 @@ private void createConnection() throws IOException, KeeperException {\n   }\n \n   @InterfaceAudience.Private\n-  public void terminateConnection() {\n+  public synchronized void terminateConnection() {\n     if (zkClient == null) {\n       return;\n     }"
    },
    {
        "commit_id": "b524501d4f4b48edeb02901114087f3b5f57691f",
        "commit_message": "MAPREDUCE-5685. Fixed a bug with JobContext getCacheFiles API inside the WrappedReducer class. Contributed by Yi Song.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1554320 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/b524501d4f4b48edeb02901114087f3b5f57691f",
        "buggy_code": "return reduceContext.getCacheArchives();",
        "fixed_code": "return reduceContext.getCacheFiles();",
        "patch": "@@ -137,7 +137,7 @@ public URI[] getCacheArchives() throws IOException {\n \n     @Override\n     public URI[] getCacheFiles() throws IOException {\n-      return reduceContext.getCacheArchives();\n+      return reduceContext.getCacheFiles();\n     }\n \n     @Override"
    },
    {
        "commit_id": "7f86c8114ec98f8a38a690bc1304c2cfc41d093e",
        "commit_message": "HDFS-5701. Fix the CacheAdmin -addPool -maxTtl option name. Contributed by Stephen Chu.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1554305 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/7f86c8114ec98f8a38a690bc1304c2cfc41d093e",
        "buggy_code": "\"[-maxttl <maxTtl>]\\n\";",
        "fixed_code": "\"[-maxTtl <maxTtl>]\\n\";",
        "patch": "@@ -578,7 +578,7 @@ public String getName() {\n     public String getShortUsage() {\n       return \"[\" + NAME + \" <name> [-owner <owner>] \" +\n           \"[-group <group>] [-mode <mode>] [-limit <limit>] \" +\n-          \"[-maxttl <maxTtl>]\\n\";\n+          \"[-maxTtl <maxTtl>]\\n\";\n     }\n \n     @Override"
    },
    {
        "commit_id": "e7120079bd1dd8c87267e126dac58cbc70b8d827",
        "commit_message": "YARN-1481. Reverting addendum patch\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1553994 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/e7120079bd1dd8c87267e126dac58cbc70b8d827",
        "buggy_code": "private boolean isRMActive() {",
        "fixed_code": "private synchronized boolean isRMActive() {",
        "patch": "@@ -174,7 +174,7 @@ private UserGroupInformation checkAcls(String method) throws YarnException {\n     }\n   }\n \n-  private boolean isRMActive() {\n+  private synchronized boolean isRMActive() {\n     return HAServiceState.ACTIVE == rmContext.getHAServiceState();\n   }\n "
    },
    {
        "commit_id": "defeef6fe43de476fc3ff08660feaa17a16931cd",
        "commit_message": "YARN-1481. Addendum patch to fix synchronization in AdminService\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1553738 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/defeef6fe43de476fc3ff08660feaa17a16931cd",
        "buggy_code": "private synchronized boolean isRMActive() {",
        "fixed_code": "private boolean isRMActive() {",
        "patch": "@@ -174,7 +174,7 @@ private UserGroupInformation checkAcls(String method) throws YarnException {\n     }\n   }\n \n-  private synchronized boolean isRMActive() {\n+  private boolean isRMActive() {\n     return HAServiceState.ACTIVE == rmContext.getHAServiceState();\n   }\n "
    },
    {
        "commit_id": "9184c4d1794c9a2d02a7ae7807a00626ac35f8ec",
        "commit_message": "MAPREDUCE-5687. Correcting the previous commit by ushing the right patch.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1552069 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/9184c4d1794c9a2d02a7ae7807a00626ac35f8ec",
        "buggy_code": "return recordFactory.newRecordInstance(KillApplicationResponse.class);",
        "fixed_code": "return KillApplicationResponse.newInstance(true);",
        "patch": "@@ -304,7 +304,7 @@ public SubmitApplicationResponse submitApplication(\n     @Override\n     public KillApplicationResponse forceKillApplication(\n         KillApplicationRequest request) throws IOException {\n-      return recordFactory.newRecordInstance(KillApplicationResponse.class);\n+      return KillApplicationResponse.newInstance(true);\n     }\n \n     @Override"
    },
    {
        "commit_id": "97acde2d33967f7f870f7dfe96c6b558e6fe324b",
        "commit_message": "HDFS-5542. Fix TODO and clean up the code in HDFS-2832. (Contributed by szetszwo)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1544664 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/97acde2d33967f7f870f7dfe96c6b558e6fe324b",
        "buggy_code": "datanode.getStorageInfos()[0].setUtilization(100L, 100L, 0, 100L);",
        "fixed_code": "datanode.getStorageInfos()[0].setUtilizationForTesting(100L, 100L, 0, 100L);",
        "patch": "@@ -103,7 +103,7 @@ public void testProcesOverReplicateBlock() throws Exception {\n           String corruptMachineName = corruptDataNode.getXferAddr();\n           for (DatanodeDescriptor datanode : hm.getDatanodes()) {\n             if (!corruptMachineName.equals(datanode.getXferAddr())) {\n-              datanode.getStorageInfos()[0].setUtilization(100L, 100L, 0, 100L);\n+              datanode.getStorageInfos()[0].setUtilizationForTesting(100L, 100L, 0, 100L);\n               datanode.updateHeartbeat(\n                   BlockManagerTestUtil.getStorageReportsForDatanode(datanode),\n                   0L, 0L, 0, 0);"
    },
    {
        "commit_id": "97acde2d33967f7f870f7dfe96c6b558e6fe324b",
        "commit_message": "HDFS-5542. Fix TODO and clean up the code in HDFS-2832. (Contributed by szetszwo)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1544664 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/97acde2d33967f7f870f7dfe96c6b558e6fe324b",
        "buggy_code": "dn.getStorageInfos()[0].setUtilization(",
        "fixed_code": "dn.getStorageInfos()[0].setUtilizationForTesting(",
        "patch": "@@ -93,7 +93,7 @@ public class TestReplicationPolicy {\n   private static void updateHeartbeatWithUsage(DatanodeDescriptor dn,\n     long capacity, long dfsUsed, long remaining, long blockPoolUsed,\n     long dnCacheCapacity, long dnCacheUsed, int xceiverCount, int volFailures) {\n-    dn.getStorageInfos()[0].setUtilization(\n+    dn.getStorageInfos()[0].setUtilizationForTesting(\n         capacity, dfsUsed, remaining, blockPoolUsed);\n     dn.updateHeartbeat(\n         BlockManagerTestUtil.getStorageReportsForDatanode(dn),"
    },
    {
        "commit_id": "97acde2d33967f7f870f7dfe96c6b558e6fe324b",
        "commit_message": "HDFS-5542. Fix TODO and clean up the code in HDFS-2832. (Contributed by szetszwo)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1544664 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/97acde2d33967f7f870f7dfe96c6b558e6fe324b",
        "buggy_code": "dn.getStorageInfos()[0].setUtilization(",
        "fixed_code": "dn.getStorageInfos()[0].setUtilizationForTesting(",
        "patch": "@@ -150,7 +150,7 @@ private static void updateHeartbeatWithUsage(DatanodeDescriptor dn,\n       long capacity, long dfsUsed, long remaining, long blockPoolUsed,\n       long dnCacheCapacity, long dnCacheUsed, int xceiverCount,\n       int volFailures) {\n-    dn.getStorageInfos()[0].setUtilization(\n+    dn.getStorageInfos()[0].setUtilizationForTesting(\n         capacity, dfsUsed, remaining, blockPoolUsed);\n     dn.updateHeartbeat(\n         BlockManagerTestUtil.getStorageReportsForDatanode(dn),"
    },
    {
        "commit_id": "cd768489f373ca101a0bfd285ec9b8695087fc42",
        "commit_message": "HDFS-5515. Fix TestDFSStartupVersions for HDFS-2832.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1542176 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/cd768489f373ca101a0bfd285ec9b8695087fc42",
        "buggy_code": "@Test",
        "fixed_code": "@Test (timeout=300000)",
        "patch": "@@ -237,7 +237,7 @@ boolean isVersionCompatible(StorageData namenodeSd, StorageData datanodeSd) {\n    *         this iterations version 3-tuple\n    * </pre>\n    */\n-  @Test\n+  @Test (timeout=300000)\n   public void testVersions() throws Exception {\n     UpgradeUtilities.initialize();\n     Configuration conf = UpgradeUtilities.initializeStorageStateConf(1, "
    },
    {
        "commit_id": "46cbce9af1272ce0eb6e300f96a1a8d4b08e23e3",
        "commit_message": "HDFS-5508. Fix compilation error after merge. (Contributed by szetszwo)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1541352 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/46cbce9af1272ce0eb6e300f96a1a8d4b08e23e3",
        "buggy_code": "startOffset, isCorrupt, cachedLocations);",
        "fixed_code": "null, null, startOffset, isCorrupt, cachedLocations);",
        "patch": "@@ -414,7 +414,7 @@ private static LocatedBlock toLocatedBlock(final Map<?, ?> m) throws IOException\n         (Object[])m.get(\"cachedLocations\"));\n \n     final LocatedBlock locatedblock = new LocatedBlock(b, locations,\n-        startOffset, isCorrupt, cachedLocations);\n+        null, null, startOffset, isCorrupt, cachedLocations);\n     locatedblock.setBlockToken(toBlockToken((Map<?, ?>)m.get(\"blockToken\")));\n     return locatedblock;\n   }"
    },
    {
        "commit_id": "fa5ba6d977520f1faaa97c55a50a22c98b3ee109",
        "commit_message": "HDFS-5439. Fix TestPendingReplication. (Contributed by Junping Du, Arpit Agarwal)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1539247 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/fa5ba6d977520f1faaa97c55a50a22c98b3ee109",
        "buggy_code": "+ \";nsInfo=\" + nsInfo);",
        "fixed_code": "+ \";nsInfo=\" + nsInfo + \";dnuuid=\" + storage.getDatanodeUuid());",
        "patch": "@@ -867,7 +867,7 @@ private void initStorage(final NamespaceInfo nsInfo) throws IOException {\n       final StorageInfo bpStorage = storage.getBPStorage(bpid);\n       LOG.info(\"Setting up storage: nsid=\" + bpStorage.getNamespaceID()\n           + \";bpid=\" + bpid + \";lv=\" + storage.getLayoutVersion()\n-          + \";nsInfo=\" + nsInfo);\n+          + \";nsInfo=\" + nsInfo + \";dnuuid=\" + storage.getDatanodeUuid());\n     }\n \n     synchronized(this)  {"
    },
    {
        "commit_id": "fa5ba6d977520f1faaa97c55a50a22c98b3ee109",
        "commit_message": "HDFS-5439. Fix TestPendingReplication. (Contributed by Junping Du, Arpit Agarwal)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1539247 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/fa5ba6d977520f1faaa97c55a50a22c98b3ee109",
        "buggy_code": "+ \", storageID=\" + getDatanodeUuid()",
        "fixed_code": "+ \", datanodeUuid=\" + getDatanodeUuid()",
        "patch": "@@ -82,7 +82,7 @@ public String getAddress() {\n   public String toString() {\n     return getClass().getSimpleName()\n       + \"(\" + getIpAddr()\n-      + \", storageID=\" + getDatanodeUuid()\n+      + \", datanodeUuid=\" + getDatanodeUuid()\n       + \", infoPort=\" + getInfoPort()\n       + \", ipcPort=\" + getIpcPort()\n       + \", storageInfo=\" + storageInfo"
    },
    {
        "commit_id": "d2b7b6589d30f50f86dbef25aeb322f817f037d8",
        "commit_message": "HDFS-5457. Fix TestDatanodeRegistration, TestFsck and TestAddBlockRetry. (Contributed by szetszwo)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1538794 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/d2b7b6589d30f50f86dbef25aeb322f817f037d8",
        "buggy_code": "return iterators.get(index).hasNext();",
        "fixed_code": "return !iterators.isEmpty() && iterators.get(index).hasNext();",
        "patch": "@@ -385,7 +385,7 @@ private BlockIterator(final DatanodeStorageInfo... storages) {\n     @Override\n     public boolean hasNext() {\n       update();\n-      return iterators.get(index).hasNext();\n+      return !iterators.isEmpty() && iterators.get(index).hasNext();\n     }\n \n     @Override"
    },
    {
        "commit_id": "26a1fda51e325377734c90399850ff3aa44b5bc1",
        "commit_message": "HDFS-5452. Fix TestReplicationPolicy and TestBlocksScheduledCounter.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1538407 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/26a1fda51e325377734c90399850ff3aa44b5bc1",
        "buggy_code": "assertEquals(lastLogEntry.getLevel(), Level.WARN);",
        "fixed_code": "assertTrue(Level.WARN.isGreaterOrEqual(lastLogEntry.getLevel()));",
        "patch": "@@ -474,7 +474,7 @@ public void testChooseTargetWithMoreThanAvailableNodes() throws Exception {\n     assertFalse(log.size() == 0);\n     final LoggingEvent lastLogEntry = log.get(log.size() - 1);\n     \n-    assertEquals(lastLogEntry.getLevel(), Level.WARN);\n+    assertTrue(Level.WARN.isGreaterOrEqual(lastLogEntry.getLevel()));\n     // Suppose to place replicas on each node but two data nodes are not\n     // available for placing replica, so here we expect a short of 2\n     assertTrue(((String)lastLogEntry.getMessage()).contains(\"in need of 2\"));"
    },
    {
        "commit_id": "2af5083a35037fde1cfd34a8bfa6c8958cb65ff7",
        "commit_message": "HDFS-5437. Fix TestBlockReport and TestBPOfferService failures.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1537365 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/2af5083a35037fde1cfd34a8bfa6c8958cb65ff7",
        "buggy_code": "new LocatedBlock(block, dnArr, uuids, types, -1, false, null) };",
        "fixed_code": "new LocatedBlock(block, dnArr, uuids, types) };",
        "patch": "@@ -252,7 +252,7 @@ void reportBadBlocks(ExtendedBlock block,\n     // TODO: Corrupt flag is set to false for compatibility. We can probably\n     // set it to true here.\n     LocatedBlock[] blocks = {\n-        new LocatedBlock(block, dnArr, uuids, types, -1, false, null) };\n+        new LocatedBlock(block, dnArr, uuids, types) };\n     \n     try {\n       bpNamenode.reportBadBlocks(blocks);  "
    },
    {
        "commit_id": "2af5083a35037fde1cfd34a8bfa6c8958cb65ff7",
        "commit_message": "HDFS-5437. Fix TestBlockReport and TestBPOfferService failures.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1537365 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/2af5083a35037fde1cfd34a8bfa6c8958cb65ff7",
        "buggy_code": "throw new UnsupportedOperationException();",
        "fixed_code": "return new StorageReport[0];",
        "patch": "@@ -1029,7 +1029,7 @@ public List<FsVolumeSpi> getVolumes() {\n \n   @Override\n   public StorageReport[] getStorageReports(String bpid) {\n-    throw new UnsupportedOperationException();\n+    return new StorageReport[0];\n   }\n \n   @Override"
    },
    {
        "commit_id": "7dd201c541c811069a898403cf28a50152a38737",
        "commit_message": "Fix inadvertent file changes made via r1536888 (bikas)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1536894 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/7dd201c541c811069a898403cf28a50152a38737",
        "buggy_code": "@Test//(timeout=10000)",
        "fixed_code": "@Test(timeout=10000)",
        "patch": "@@ -72,7 +72,7 @@ public class TestCombineTextInputFormat {\n     new Path(new Path(System.getProperty(\"test.build.data\", \".\"), \"data\"),\n              \"TestCombineTextInputFormat\");\n \n-  @Test//(timeout=10000)\n+  @Test(timeout=10000)\n   public void testFormat() throws Exception {\n     Job job = Job.getInstance(new Configuration(defaultConf));\n "
    },
    {
        "commit_id": "b9d561c548c26d0db4994e6c13c7ebf43705d794",
        "commit_message": "HDFS-5214. Fix NPEs in BlockManager and DirectoryScanner.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1536179 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/b9d561c548c26d0db4994e6c13c7ebf43705d794",
        "buggy_code": "List<Block> arr = dataset.getFinalizedBlocks(blockPoolId);",
        "fixed_code": "List<FinalizedReplica> arr = dataset.getFinalizedBlocks(blockPoolId);",
        "patch": "@@ -187,7 +187,7 @@ public LinkedElement getNext() {\n         + hours + \" hours for block pool \" + bpid);\n \n     // get the list of blocks and arrange them in random order\n-    List<Block> arr = dataset.getFinalizedBlocks(blockPoolId);\n+    List<FinalizedReplica> arr = dataset.getFinalizedBlocks(blockPoolId);\n     Collections.shuffle(arr);\n     \n     long scanTime = -1;"
    },
    {
        "commit_id": "b9d561c548c26d0db4994e6c13c7ebf43705d794",
        "commit_message": "HDFS-5214. Fix NPEs in BlockManager and DirectoryScanner.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1536179 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/b9d561c548c26d0db4994e6c13c7ebf43705d794",
        "buggy_code": "public List<Block> getFinalizedBlocks(String bpid) {",
        "fixed_code": "public List<FinalizedReplica> getFinalizedBlocks(String bpid) {",
        "patch": "@@ -1006,7 +1006,7 @@ public StorageReport[] getStorageReports(String bpid) {\n   }\n \n   @Override\n-  public List<Block> getFinalizedBlocks(String bpid) {\n+  public List<FinalizedReplica> getFinalizedBlocks(String bpid) {\n     throw new UnsupportedOperationException();\n   }\n "
    },
    {
        "commit_id": "b9d561c548c26d0db4994e6c13c7ebf43705d794",
        "commit_message": "HDFS-5214. Fix NPEs in BlockManager and DirectoryScanner.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1536179 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/b9d561c548c26d0db4994e6c13c7ebf43705d794",
        "buggy_code": "new DirectoryScanner.ScanInfo(blockId);",
        "fixed_code": "new DirectoryScanner.ScanInfo(blockId, null, null, null);",
        "patch": "@@ -447,7 +447,7 @@ void testScanInfoObject(long blockId, File blockFile, File metaFile)\n   \n   void testScanInfoObject(long blockId) throws Exception {\n     DirectoryScanner.ScanInfo scanInfo =\n-        new DirectoryScanner.ScanInfo(blockId);\n+        new DirectoryScanner.ScanInfo(blockId, null, null, null);\n     assertEquals(blockId, scanInfo.getBlockId());\n     assertNull(scanInfo.getBlockFile());\n     assertNull(scanInfo.getMetaFile());"
    },
    {
        "commit_id": "dc2ee20aec7b3fe1d13c846926ba1b0f02c5adef",
        "commit_message": "HDFS-5419. Fixup test-patch.sh warnings on HDFS-4949 branch. (wang)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1535607 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/dc2ee20aec7b3fe1d13c846926ba1b0f02c5adef",
        "buggy_code": "commandName.replaceAll(\"^[-]*\", \"\");",
        "fixed_code": "commandName = commandName.replaceAll(\"^[-]*\", \"\");",
        "patch": "@@ -720,7 +720,7 @@ public int run(Configuration conf, List<String> args) throws IOException {\n         return 0;\n       }\n       String commandName = args.get(0);\n-      commandName.replaceAll(\"^[-]*\", \"\");\n+      commandName = commandName.replaceAll(\"^[-]*\", \"\");\n       Command command = determineCommand(commandName);\n       if (command == null) {\n         System.err.print(\"Sorry, I don't know the command '\" +"
    },
    {
        "commit_id": "f28f5ed62861a4c87256eddb8a8ab64e05696192",
        "commit_message": "HDFS-5370. Typo in Error Message: different between range in condition and range in error message. Contributed by Kousuke Saruta.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1532899 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/f28f5ed62861a4c87256eddb8a8ab64e05696192",
        "buggy_code": "throw new IOException(\"offset < 0 || offset > getFileLength(), offset=\"",
        "fixed_code": "throw new IOException(\"offset < 0 || offset >= getFileLength(), offset=\"",
        "patch": "@@ -403,7 +403,7 @@ private synchronized LocatedBlock getBlockAt(long offset,\n \n     //check offset\n     if (offset < 0 || offset >= getFileLength()) {\n-      throw new IOException(\"offset < 0 || offset > getFileLength(), offset=\"\n+      throw new IOException(\"offset < 0 || offset >= getFileLength(), offset=\"\n           + offset\n           + \", updatePosition=\" + updatePosition\n           + \", locatedBlocks=\" + locatedBlocks);"
    },
    {
        "commit_id": "09e9e57a0bdc4ccc963af717d71c352030e6eed9",
        "commit_message": "HDFS-5348. Fix error message when dfs.datanode.max.locked.memory is improperly configured. (Contributed by Colin Patrick McCabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1531460 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/09e9e57a0bdc4ccc963af717d71c352030e6eed9",
        "buggy_code": "\" size (%s) of %d bytes is less than the datanode's available\" +",
        "fixed_code": "\" size (%s) of %d bytes is more than the datanode's available\" +",
        "patch": "@@ -753,7 +753,7 @@ void startDataNode(Configuration conf,\n       if (dnConf.maxLockedMemory > ulimit) {\n       throw new RuntimeException(String.format(\n           \"Cannot start datanode because the configured max locked memory\" +\n-          \" size (%s) of %d bytes is less than the datanode's available\" +\n+          \" size (%s) of %d bytes is more than the datanode's available\" +\n           \" RLIMIT_MEMLOCK ulimit of %d bytes.\",\n           DFS_DATANODE_MAX_LOCKED_MEMORY_KEY,\n           dnConf.maxLockedMemory,"
    },
    {
        "commit_id": "d2e73b27757fc72a21829e58bac1ce04029e7011",
        "commit_message": "HADOOP-9964. Fix deadlocks in TestHttpServer by synchronize ReflectionUtils.printThreadInfo. (Junping Du via llu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1527650 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/d2e73b27757fc72a21829e58bac1ce04029e7011",
        "buggy_code": "public static void printThreadInfo(PrintWriter stream,",
        "fixed_code": "public synchronized static void printThreadInfo(PrintWriter stream,",
        "patch": "@@ -154,7 +154,7 @@ private static String getTaskName(long id, String name) {\n    * @param stream the stream to\n    * @param title a string title for the stack trace\n    */\n-  public static void printThreadInfo(PrintWriter stream,\n+  public synchronized static void printThreadInfo(PrintWriter stream,\n                                      String title) {\n     final int STACK_DEPTH = 20;\n     boolean contention = threadBean.isThreadContentionMonitoringEnabled();"
    },
    {
        "commit_id": "fb48b6cdc9d9314bc8af388a42a4a22755c72651",
        "commit_message": "MAPREDUCE-5503. Fixed a test issue in TestMRJobClient. Contributed by Jian He.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1526362 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/fb48b6cdc9d9314bc8af388a42a4a22755c72651",
        "buggy_code": "testJobList(jobId, conf);",
        "fixed_code": "testAllJobList(jobId, conf);",
        "patch": "@@ -79,7 +79,7 @@ public void testJobClient() throws Exception {\n     Configuration conf = createJobConf();\n     String jobId = runJob();\n     testGetCounter(jobId, conf);\n-    testJobList(jobId, conf);\n+    testAllJobList(jobId, conf);\n     testChangingJobPriority(jobId, conf);\n   }\n   "
    },
    {
        "commit_id": "85c203602993a946fb5f41eadf1cf1484a0ce686",
        "commit_message": "HDFS-5210. Fix some failing unit tests on HDFS-4949 branch. (Contributed by Andrew Wang)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1523754 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/85c203602993a946fb5f41eadf1cf1484a0ce686",
        "buggy_code": "targets = new ArrayList<DatanodeDescriptor>(storedNodes);",
        "fixed_code": "targets = new ArrayList<DatanodeDescriptor>(storedNodes.size());",
        "patch": "@@ -156,7 +156,7 @@ private void computeCachingWorkForBlocks(List<Block> blocksToCache) {\n           }\n           // Choose some replicas to cache if needed\n           additionalRepl = requiredRepl - effectiveRepl;\n-          targets = new ArrayList<DatanodeDescriptor>(storedNodes);\n+          targets = new ArrayList<DatanodeDescriptor>(storedNodes.size());\n           // Only target replicas that aren't already cached.\n           for (DatanodeDescriptor dn: storedNodes) {\n             if (!cachedNodes.contains(dn)) {"
    },
    {
        "commit_id": "85c203602993a946fb5f41eadf1cf1484a0ce686",
        "commit_message": "HDFS-5210. Fix some failing unit tests on HDFS-4949 branch. (Contributed by Andrew Wang)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1523754 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/85c203602993a946fb5f41eadf1cf1484a0ce686",
        "buggy_code": "checksumBuf.limit(chunks*bytesPerChecksum);",
        "fixed_code": "checksumBuf.limit(chunks*checksumSize);",
        "patch": "@@ -225,7 +225,7 @@ public void verifyChecksum() throws IOException, ChecksumException {\n       blockBuf.flip();\n       // Number of read chunks, including partial chunk at end\n       int chunks = (bytesRead+bytesPerChecksum-1) / bytesPerChecksum;\n-      checksumBuf.limit(chunks*bytesPerChecksum);\n+      checksumBuf.limit(chunks*checksumSize);\n       fillBuffer(metaChannel, checksumBuf);\n       checksumBuf.flip();\n       checksum.verifyChunkedSums(blockBuf, checksumBuf, block.getBlockName(),"
    },
    {
        "commit_id": "5adba5597ce071c2e84d0c9834e1d9e5e76f9bdb",
        "commit_message": "YARN-1085. Addendum patch to address issues with the earlier patch.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1517721 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/5adba5597ce071c2e84d0c9834e1d9e5e76f9bdb",
        "buggy_code": "YarnConfiguration.RM_WEBAPP_SPENGO_KEYTAB_FILE_KEY)",
        "fixed_code": "YarnConfiguration.RM_WEBAPP_SPNEGO_KEYTAB_FILE_KEY)",
        "patch": "@@ -584,7 +584,7 @@ protected void startWepApp() {\n             .withHttpSpnegoPrincipalKey(\n                 YarnConfiguration.RM_WEBAPP_SPNEGO_USER_NAME_KEY)\n             .withHttpSpnegoKeytabKey(\n-                YarnConfiguration.RM_WEBAPP_SPENGO_KEYTAB_FILE_KEY)\n+                YarnConfiguration.RM_WEBAPP_SPNEGO_KEYTAB_FILE_KEY)\n             .at(this.conf.get(YarnConfiguration.RM_WEBAPP_ADDRESS,\n         YarnConfiguration.DEFAULT_RM_WEBAPP_ADDRESS)); \n     String proxyHostAndPort = YarnConfiguration.getProxyHostAndPort(conf);"
    },
    {
        "commit_id": "487ce6c7bc87819659602a4d930bc50d31f5d022",
        "commit_message": "YARN-1082. Addendum patch.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1516352 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/487ce6c7bc87819659602a4d930bc50d31f5d022",
        "buggy_code": "protected void startInternal() throws Exception {",
        "fixed_code": "protected synchronized void startInternal() throws Exception {",
        "patch": "@@ -91,7 +91,7 @@ public synchronized void initInternal(Configuration conf)\n   }\n \n   @Override\n-  protected void startInternal() throws Exception {\n+  protected synchronized void startInternal() throws Exception {\n     // create filesystem only now, as part of service-start. By this time, RM is\n     // authenticated with kerberos so we are good to create a file-system\n     // handle."
    },
    {
        "commit_id": "1d915238a6a06d09e1789532994f00f496bd969c",
        "commit_message": "MAPREDUCE-5385. Fixed a bug with JobContext getCacheFiles API. Contributed by Omkar Vinit Joshi.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1508595 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/1d915238a6a06d09e1789532994f00f496bd969c",
        "buggy_code": "return mapContext.getCacheArchives();",
        "fixed_code": "return mapContext.getCacheFiles();",
        "patch": "@@ -144,7 +144,7 @@ public URI[] getCacheArchives() throws IOException {\n \n     @Override\n     public URI[] getCacheFiles() throws IOException {\n-      return mapContext.getCacheArchives();\n+      return mapContext.getCacheFiles();\n     }\n \n     @Override"
    },
    {
        "commit_id": "f179afc68d863ea35f3ce5c06f1690fb7a4e8f02",
        "commit_message": "YARN-937. Fix unmanaged AM in non-secure/secure setup post YARN-701. (tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1507706 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/f179afc68d863ea35f3ce5c06f1690fb7a4e8f02",
        "buggy_code": "YarnConfiguration.DEFAULT_APPLICATION_TYPE);",
        "fixed_code": "YarnConfiguration.DEFAULT_APPLICATION_TYPE, null);",
        "patch": "@@ -90,7 +90,7 @@ private ApplicationReport getUnknownApplicationReport() {\n     return ApplicationReport.newInstance(unknownAppId, unknownAttemptId,\n       \"N/A\", \"N/A\", \"N/A\", \"N/A\", 0, null, YarnApplicationState.NEW, \"N/A\",\n       \"N/A\", 0, 0, FinalApplicationStatus.UNDEFINED, null, \"N/A\", 0.0f,\n-      YarnConfiguration.DEFAULT_APPLICATION_TYPE);\n+      YarnConfiguration.DEFAULT_APPLICATION_TYPE, null);\n   }\n \n   NotRunningJob(ApplicationReport applicationReport, JobState jobState) {"
    },
    {
        "commit_id": "3eb61be352589491117ac2781bb18f55988a8084",
        "commit_message": "HADOOP-9754. Remove unnecessary \"throws IOException/InterruptedException\", and fix generic and other javac warnings.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1505610 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/3eb61be352589491117ac2781bb18f55988a8084",
        "buggy_code": "RpcResponseWrapper val = null;",
        "fixed_code": "final RpcResponseWrapper val;",
        "patch": "@@ -192,7 +192,6 @@ public Object invoke(Object proxy, Method method, Object[] args)\n       }\n \n       RequestHeaderProto rpcRequestHeader = constructRpcRequestHeader(method);\n-      RpcResponseWrapper val = null;\n       \n       if (LOG.isTraceEnabled()) {\n         LOG.trace(Thread.currentThread().getId() + \": Call -> \" +\n@@ -202,6 +201,7 @@ public Object invoke(Object proxy, Method method, Object[] args)\n \n \n       Message theRequest = (Message) args[1];\n+      final RpcResponseWrapper val;\n       try {\n         val = (RpcResponseWrapper) client.call(RPC.RpcKind.RPC_PROTOCOL_BUFFER,\n             new RpcRequestWrapper(rpcRequestHeader, theRequest), remoteId);"
    },
    {
        "commit_id": "3eb61be352589491117ac2781bb18f55988a8084",
        "commit_message": "HADOOP-9754. Remove unnecessary \"throws IOException/InterruptedException\", and fix generic and other javac warnings.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1505610 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/3eb61be352589491117ac2781bb18f55988a8084",
        "buggy_code": "interface Bar extends Mixin, VersionedProtocol {",
        "fixed_code": "interface Bar extends Mixin {",
        "patch": "@@ -64,7 +64,7 @@ interface Mixin extends VersionedProtocol{\n     public static final long versionID = 0L;\n     void hello() throws IOException;\n   }\n-  interface Bar extends Mixin, VersionedProtocol {\n+  interface Bar extends Mixin {\n     public static final long versionID = 0L;\n     int echo(int i) throws IOException;\n   }"
    },
    {
        "commit_id": "3eb61be352589491117ac2781bb18f55988a8084",
        "commit_message": "HADOOP-9754. Remove unnecessary \"throws IOException/InterruptedException\", and fix generic and other javac warnings.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1505610 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/3eb61be352589491117ac2781bb18f55988a8084",
        "buggy_code": "public void testExceptionsHandler() throws IOException {",
        "fixed_code": "public void testExceptionsHandler() {",
        "patch": "@@ -118,7 +118,7 @@ public void testBindError() throws Exception {\n   }\n   \n   @Test\n-  public void testExceptionsHandler() throws IOException {\n+  public void testExceptionsHandler() {\n     Server.ExceptionsHandler handler = new Server.ExceptionsHandler();\n     handler.addTerseExceptions(IOException.class);\n     handler.addTerseExceptions(RpcServerException.class, IpcException.class);"
    },
    {
        "commit_id": "8f8be404a7f476bd376307170f8cff656bf19474",
        "commit_message": "Trivial fix for minor refactor error for YARN-521\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1503543 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/8f8be404a7f476bd376307170f8cff656bf19474",
        "buggy_code": "(!relaxLocality && (racks == null || racks.length == 0)",
        "fixed_code": "!(!relaxLocality && (racks == null || racks.length == 0)",
        "patch": "@@ -159,7 +159,7 @@ public ContainerRequest(Resource capability, String[] nodes,\n       Preconditions.checkArgument(containerCount > 0,\n           \"The number of containers to request should larger than 0\");\n       Preconditions.checkArgument(\n-              (!relaxLocality && (racks == null || racks.length == 0) \n+              !(!relaxLocality && (racks == null || racks.length == 0) \n                   && (nodes == null || nodes.length == 0)),\n               \"Can't turn off locality relaxation on a \" + \n               \"request with no location constraints\");"
    },
    {
        "commit_id": "3ee5949912a4fcd51e19ba758b6eff276543a74a",
        "commit_message": "YARN-368. Fixed a typo in error message in Auxiliary services. Contributed by Albert Chu.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1501852 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/3ee5949912a4fcd51e19ba758b6eff276543a74a",
        "buggy_code": "throw new RuntimeException(\"No class defiend for \" + sName);",
        "fixed_code": "throw new RuntimeException(\"No class defined for \" + sName);",
        "patch": "@@ -92,7 +92,7 @@ public void serviceInit(Configuration conf) throws Exception {\n               String.format(YarnConfiguration.NM_AUX_SERVICE_FMT, sName), null,\n               AuxiliaryService.class);\n         if (null == sClass) {\n-          throw new RuntimeException(\"No class defiend for \" + sName);\n+          throw new RuntimeException(\"No class defined for \" + sName);\n         }\n         AuxiliaryService s = ReflectionUtils.newInstance(sClass, conf);\n         // TODO better use s.getName()?"
    },
    {
        "commit_id": "b3a8d99817dcceb4d1125dec0c3ecbb0f15f6c76",
        "commit_message": "YARN-874. Making common RPC to switch to not switch to simple when other mechanisms are enabled and thus fix YARN/MR test failures after HADOOP-9421. Contributed by Daryn Sharp and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1496692 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/b3a8d99817dcceb4d1125dec0c3ecbb0f15f6c76",
        "buggy_code": "+ \"Available:[KERBEROS, DIGEST]\"));",
        "fixed_code": "+ \"Available:[TOKEN, KERBEROS]\"));",
        "patch": "@@ -231,7 +231,7 @@ public ApplicationMasterProtocol run() {\n       // server side will assume we are trying simple auth.\n       Assert.assertTrue(e.getCause().getMessage().contains(\n         \"SIMPLE authentication is not enabled.  \"\n-            + \"Available:[KERBEROS, DIGEST]\"));\n+            + \"Available:[TOKEN, KERBEROS]\"));\n     }\n \n     // Now try to validate invalid authorization."
    },
    {
        "commit_id": "ca35235b04b7eb60635a62a0f517d214adc32ba7",
        "commit_message": "HADOOP-9439.  JniBasedUnixGroupsMapping: fix some crash bugs (Colin Patrick McCabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1496112 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/ca35235b04b7eb60635a62a0f517d214adc32ba7",
        "buggy_code": "static final boolean WORKAROUND_NON_THREADSAFE_CALLS_DEFAULT = false;",
        "fixed_code": "static final boolean WORKAROUND_NON_THREADSAFE_CALLS_DEFAULT = true;",
        "patch": "@@ -98,7 +98,7 @@ public static class POSIX {\n \n     static final String WORKAROUND_NON_THREADSAFE_CALLS_KEY =\n       \"hadoop.workaround.non.threadsafe.getpwuid\";\n-    static final boolean WORKAROUND_NON_THREADSAFE_CALLS_DEFAULT = false;\n+    static final boolean WORKAROUND_NON_THREADSAFE_CALLS_DEFAULT = true;\n \n     private static long cacheTimeout = -1;\n "
    },
    {
        "commit_id": "6451288704922576e75dd1597f5fd0ef09ab4f26",
        "commit_message": "YARN-799. Fix CgroupsLCEResourcesHandler to use /tasks instead of /cgroup.procs. Contributed by Chris Riccomini.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1494035 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/6451288704922576e75dd1597f5fd0ef09ab4f26",
        "buggy_code": "sb.append(pathForCgroup(CONTROLLER_CPU, containerName) + \"/cgroup.procs\");",
        "fixed_code": "sb.append(pathForCgroup(CONTROLLER_CPU, containerName) + \"/tasks\");",
        "patch": "@@ -222,7 +222,7 @@ public String getResourcesOption(ContainerId containerId) {\n     StringBuilder sb = new StringBuilder(\"cgroups=\");\n \n     if (isCpuWeightEnabled()) {\n-      sb.append(pathForCgroup(CONTROLLER_CPU, containerName) + \"/cgroup.procs\");\n+      sb.append(pathForCgroup(CONTROLLER_CPU, containerName) + \"/tasks\");\n       sb.append(\",\");\n     }\n "
    },
    {
        "commit_id": "2b14656ab5050dd75935b64681cdc25fb49db94f",
        "commit_message": "YARN-805. Fix javadoc and annotations on classes in the yarn-api package. Contributed by Jian He.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1493992 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/2b14656ab5050dd75935b64681cdc25fb49db94f",
        "buggy_code": "req.getNumContainers());",
        "fixed_code": "req.getNumContainers(), req.getRelaxLocality());",
        "patch": "@@ -241,7 +241,7 @@ protected void containerFailedOnHost(String hostName) {\n               ResourceRequest zeroedRequest =\n                   ResourceRequest.newInstance(req.getPriority(),\n                     req.getResourceName(), req.getCapability(),\n-                    req.getNumContainers());\n+                    req.getNumContainers(), req.getRelaxLocality());\n \n               zeroedRequest.setNumContainers(0);\n               // to be sent to RM on next heartbeat"
    },
    {
        "commit_id": "2b14656ab5050dd75935b64681cdc25fb49db94f",
        "commit_message": "YARN-805. Fix javadoc and annotations on classes in the yarn-api package. Contributed by Jian He.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1493992 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/2b14656ab5050dd75935b64681cdc25fb49db94f",
        "buggy_code": ".getNumContainers());",
        "fixed_code": ".getNumContainers(), req.getRelaxLocality());",
        "patch": "@@ -1219,7 +1219,7 @@ public synchronized Allocation allocate(\n       for (ResourceRequest req : ask) {\n         ResourceRequest reqCopy = ResourceRequest.newInstance(req\n             .getPriority(), req.getResourceName(), req.getCapability(), req\n-            .getNumContainers());\n+            .getNumContainers(), req.getRelaxLocality());\n         askCopy.add(reqCopy);\n       }\n       lastAsk = ask;"
    },
    {
        "commit_id": "5e72bfc521b1525b6232f2e685d57bb2dee95823",
        "commit_message": "revert HADOOP-9646 to fix hadoop-streaming compile issue\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1493252 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/5e72bfc521b1525b6232f2e685d57bb2dee95823",
        "buggy_code": ") throws IOException {",
        "fixed_code": ") throws IOException, InterruptedException {",
        "patch": "@@ -800,7 +800,7 @@ public static int symLink(String target, String linkname) throws IOException{\n    * @throws InterruptedException\n    */\n   public static int chmod(String filename, String perm\n-                          ) throws IOException {\n+                          ) throws IOException, InterruptedException {\n     return chmod(filename, perm, false);\n   }\n "
    },
    {
        "commit_id": "496b80b28c35dbd52d3d919d16f4c75983f81a79",
        "commit_message": "HDFS-4850. Fix OfflineImageViewer to work on fsimages with empty files or snapshots. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1490080 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/496b80b28c35dbd52d3d919d16f4c75983f81a79",
        "buggy_code": "&& isDir && dir instanceof INodeDirectoryWithSnapshot) {",
        "fixed_code": "&& isDir && dir instanceof INodeDirectorySnapshottable) {",
        "patch": "@@ -186,7 +186,7 @@ static INodesInPath resolve(final INodeDirectory startingDir,\n       \n       // check if the next byte[] in components is for \".snapshot\"\n       if (isDotSnapshotDir(childName)\n-          && isDir && dir instanceof INodeDirectoryWithSnapshot) {\n+          && isDir && dir instanceof INodeDirectorySnapshottable) {\n         // skip the \".snapshot\" in components\n         count++;\n         index++;"
    },
    {
        "commit_id": "496b80b28c35dbd52d3d919d16f4c75983f81a79",
        "commit_message": "HDFS-4850. Fix OfflineImageViewer to work on fsimages with empty files or snapshots. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1490080 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/496b80b28c35dbd52d3d919d16f4c75983f81a79",
        "buggy_code": "SNAPSHOT_DIFF_SNAPSHOTROOT,",
        "fixed_code": "SNAPSHOT_DIFF_SNAPSHOTID,",
        "patch": "@@ -95,7 +95,7 @@ public enum ImageElement {\n     NUM_SNAPSHOT_DIR_DIFF,\n     SNAPSHOT_DIR_DIFFS,\n     SNAPSHOT_DIR_DIFF,\n-    SNAPSHOT_DIFF_SNAPSHOTROOT,\n+    SNAPSHOT_DIFF_SNAPSHOTID,\n     SNAPSHOT_DIR_DIFF_CHILDREN_SIZE,\n     SNAPSHOT_DIFF_SNAPSHOTINODE,\n     SNAPSHOT_DIR_DIFF_CREATEDLIST,"
    },
    {
        "commit_id": "496b80b28c35dbd52d3d919d16f4c75983f81a79",
        "commit_message": "HDFS-4850. Fix OfflineImageViewer to work on fsimages with empty files or snapshots. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1490080 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/496b80b28c35dbd52d3d919d16f4c75983f81a79",
        "buggy_code": "class XmlImageVisitor extends TextWriterImageVisitor {",
        "fixed_code": "public class XmlImageVisitor extends TextWriterImageVisitor {",
        "patch": "@@ -24,7 +24,7 @@\n  * An XmlImageVisitor walks over an fsimage structure and writes out\n  * an equivalent XML document that contains the fsimage's components.\n  */\n-class XmlImageVisitor extends TextWriterImageVisitor {\n+public class XmlImageVisitor extends TextWriterImageVisitor {\n   final private LinkedList<ImageElement> tagQ =\n                                           new LinkedList<ImageElement>();\n "
    },
    {
        "commit_id": "e00f828b119b6a271b6319b6c4885228cd4cb3ed",
        "commit_message": "HDFS-4876. Fix the javadoc of FileWithSnapshot and move FileDiffList to FileWithSnapshot.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1489708 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/e00f828b119b6a271b6319b6c4885228cd4cb3ed",
        "buggy_code": "import org.apache.hadoop.hdfs.server.namenode.snapshot.FileDiffList;",
        "fixed_code": "import org.apache.hadoop.hdfs.server.namenode.snapshot.FileWithSnapshot.FileDiffList;",
        "patch": "@@ -50,7 +50,7 @@\n import org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo;\n import org.apache.hadoop.hdfs.server.blockmanagement.BlockManager;\n import org.apache.hadoop.hdfs.server.common.InconsistentFSStateException;\n-import org.apache.hadoop.hdfs.server.namenode.snapshot.FileDiffList;\n+import org.apache.hadoop.hdfs.server.namenode.snapshot.FileWithSnapshot.FileDiffList;\n import org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectorySnapshottable;\n import org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectoryWithSnapshot;\n import org.apache.hadoop.hdfs.server.namenode.snapshot.INodeFileUnderConstructionWithSnapshot;"
    },
    {
        "commit_id": "e00f828b119b6a271b6319b6c4885228cd4cb3ed",
        "commit_message": "HDFS-4876. Fix the javadoc of FileWithSnapshot and move FileDiffList to FileWithSnapshot.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1489708 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/e00f828b119b6a271b6319b6c4885228cd4cb3ed",
        "buggy_code": "private GSet<INode, INodeWithAdditionalFields> map;",
        "fixed_code": "private final GSet<INode, INodeWithAdditionalFields> map;",
        "patch": "@@ -45,7 +45,7 @@ static INodeMap newInstance(INodeDirectory rootDir) {\n   }\n   \n   /** Synchronized by external lock. */\n-  private GSet<INode, INodeWithAdditionalFields> map;\n+  private final GSet<INode, INodeWithAdditionalFields> map;\n   \n   private INodeMap(GSet<INode, INodeWithAdditionalFields> map) {\n     Preconditions.checkArgument(map != null);"
    },
    {
        "commit_id": "e00f828b119b6a271b6319b6c4885228cd4cb3ed",
        "commit_message": "HDFS-4876. Fix the javadoc of FileWithSnapshot and move FileDiffList to FileWithSnapshot.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1489708 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/e00f828b119b6a271b6319b6c4885228cd4cb3ed",
        "buggy_code": "import org.apache.hadoop.hdfs.server.namenode.snapshot.FileDiffList;",
        "fixed_code": "import org.apache.hadoop.hdfs.server.namenode.snapshot.FileWithSnapshot.FileDiffList;",
        "patch": "@@ -34,7 +34,7 @@\n import org.apache.hadoop.hdfs.server.namenode.INodeFile;\n import org.apache.hadoop.hdfs.server.namenode.INodeReference;\n import org.apache.hadoop.hdfs.server.namenode.snapshot.FileWithSnapshot.FileDiff;\n-import org.apache.hadoop.hdfs.server.namenode.snapshot.FileDiffList;\n+import org.apache.hadoop.hdfs.server.namenode.snapshot.FileWithSnapshot.FileDiffList;\n import org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectoryWithSnapshot.DirectoryDiff;\n import org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectoryWithSnapshot.DirectoryDiffList;\n import org.apache.hadoop.hdfs.tools.snapshot.SnapshotDiff;"
    },
    {
        "commit_id": "131bfc91a60481b95fc0eb89ad6b4bcd1c841c3b",
        "commit_message": "MAPREDUCE-5261. Reverting the patch as it is no longer needed.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1488032 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/131bfc91a60481b95fc0eb89ad6b4bcd1c841c3b",
        "buggy_code": "public static void setTokenServiceUseIp(boolean flag) {",
        "fixed_code": "static void setTokenServiceUseIp(boolean flag) {",
        "patch": "@@ -95,7 +95,7 @@ public class SecurityUtil {\n    * For use only by tests and initialization\n    */\n   @InterfaceAudience.Private\n-  public static void setTokenServiceUseIp(boolean flag) {\n+  static void setTokenServiceUseIp(boolean flag) {\n     useIpForTokenService = flag;\n     hostResolver = !useIpForTokenService\n         ? new QualifiedHostResolver()"
    },
    {
        "commit_id": "4a4e42d91984379b240b0522e0ffb612b2fc1790",
        "commit_message": "MAPREDUCE-5261. Fix issues in TestRMContainerAllocator after YARN-617. Contributed by Omkar Vinit Joshi.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1485079 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/4a4e42d91984379b240b0522e0ffb612b2fc1790",
        "buggy_code": "static void setTokenServiceUseIp(boolean flag) {",
        "fixed_code": "public static void setTokenServiceUseIp(boolean flag) {",
        "patch": "@@ -95,7 +95,7 @@ public class SecurityUtil {\n    * For use only by tests and initialization\n    */\n   @InterfaceAudience.Private\n-  static void setTokenServiceUseIp(boolean flag) {\n+  public static void setTokenServiceUseIp(boolean flag) {\n     useIpForTokenService = flag;\n     hostResolver = !useIpForTokenService\n         ? new QualifiedHostResolver()"
    },
    {
        "commit_id": "67d7d8f4d31780b00b7b579741502fd00bf43da9",
        "commit_message": "MAPREDUCE-5209. Fix units in a ShuffleScheduler log message.\nContributed by Tsuyoshi OZAWA\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1480464 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/67d7d8f4d31780b00b7b579741502fd00bf43da9",
        "buggy_code": "(System.currentTimeMillis()-shuffleStart.get()) + \"s\");",
        "fixed_code": "(System.currentTimeMillis()-shuffleStart.get()) + \"ms\");",
        "patch": "@@ -359,7 +359,7 @@ public synchronized void freeHost(MapHost host) {\n       }\n     }\n     LOG.info(host + \" freed by \" + Thread.currentThread().getName() + \" in \" + \n-             (System.currentTimeMillis()-shuffleStart.get()) + \"s\");\n+             (System.currentTimeMillis()-shuffleStart.get()) + \"ms\");\n   }\n     \n   public synchronized void resetKnownMaps() {"
    },
    {
        "commit_id": "e2091275dc26745c4e919cd767283d32608a1817",
        "commit_message": "HDFS-4610. Reverting the patch Jenkins build is not run.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1477396 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/e2091275dc26745c4e919cd767283d32608a1817",
        "buggy_code": "if (!FileUtil.canWrite(root)) {",
        "fixed_code": "if (!root.canWrite()) {",
        "patch": "@@ -448,7 +448,7 @@ public StorageState analyzeStorage(StartupOption startOpt, Storage storage)\n           LOG.warn(rootPath + \"is not a directory\");\n           return StorageState.NON_EXISTENT;\n         }\n-        if (!FileUtil.canWrite(root)) {\n+        if (!root.canWrite()) {\n           LOG.warn(\"Cannot access storage directory \" + rootPath);\n           return StorageState.NON_EXISTENT;\n         }"
    },
    {
        "commit_id": "ae393ab26e13bb9780227507359635790816bf47",
        "commit_message": "MAPREDUCE-5179. Fix unit test in TestHSWebServices which fails when versionInfo has parantheses like when running on a git checkout. Contributed by Hitesh Shah.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1476835 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/ae393ab26e13bb9780227507359635790816bf47",
        "buggy_code": "WebServicesTestUtils.checkStringMatch(\"hadoopBuildVersion\",",
        "fixed_code": "WebServicesTestUtils.checkStringEqual(\"hadoopBuildVersion\",",
        "patch": "@@ -348,7 +348,7 @@ public void verifyHsInfoGeneric(String hadoopVersionBuiltOn,\n       String hadoopBuildVersion, String hadoopVersion, long startedon) {\n     WebServicesTestUtils.checkStringMatch(\"hadoopVersionBuiltOn\",\n         VersionInfo.getDate(), hadoopVersionBuiltOn);\n-    WebServicesTestUtils.checkStringMatch(\"hadoopBuildVersion\",\n+    WebServicesTestUtils.checkStringEqual(\"hadoopBuildVersion\",\n         VersionInfo.getBuildVersion(), hadoopBuildVersion);\n     WebServicesTestUtils.checkStringMatch(\"hadoopVersion\",\n         VersionInfo.getVersion(), hadoopVersion);"
    },
    {
        "commit_id": "43bdc22e9207a74678665de5f109dd7e56fe979a",
        "commit_message": "HDFS-4726. Fix test failures after merging the INodeId-INode mapping from trunk.  Contributed by Jing Zhao\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1470735 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/43bdc22e9207a74678665de5f109dd7e56fe979a",
        "buggy_code": "if (cnode.equals(dnode)) {",
        "fixed_code": "if (cnode.compareTo(dnode.getLocalNameBytes()) == 0) {",
        "patch": "@@ -185,7 +185,7 @@ public List<DiffReportEntry> generateReport(byte[][] parentPath,\n       for (; c < created.size() && d < deleted.size(); ) {\n         INode cnode = created.get(c);\n         INode dnode = deleted.get(d);\n-        if (cnode.equals(dnode)) {\n+        if (cnode.compareTo(dnode.getLocalNameBytes()) == 0) {\n           fullPath[fullPath.length - 1] = cnode.getLocalNameBytes();\n           if (cnode.isSymlink() && dnode.isSymlink()) {\n             dList.add(new DiffReportEntry(DiffType.MODIFY, fullPath));"
    },
    {
        "commit_id": "43bdc22e9207a74678665de5f109dd7e56fe979a",
        "commit_message": "HDFS-4726. Fix test failures after merging the INodeId-INode mapping from trunk.  Contributed by Jing Zhao\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1470735 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/43bdc22e9207a74678665de5f109dd7e56fe979a",
        "buggy_code": "Preconditions.checkArgument(oldElement.equals(newElement),",
        "fixed_code": "Preconditions.checkArgument(oldElement.compareTo(newElement.getKey()) == 0,",
        "patch": "@@ -261,7 +261,7 @@ public void undoDelete(final E element, final UndoInfo<E> undoInfo) {\n   public UndoInfo<E> modify(final E oldElement, final E newElement) {\n     Preconditions.checkArgument(oldElement != newElement,\n         \"They are the same object: oldElement == newElement = %s\", newElement);\n-    Preconditions.checkArgument(oldElement.equals(newElement),\n+    Preconditions.checkArgument(oldElement.compareTo(newElement.getKey()) == 0,\n         \"The names do not match: oldElement=%s, newElement=%s\",\n         oldElement, newElement);\n     final int c = search(created, newElement.getKey());"
    },
    {
        "commit_id": "43bdc22e9207a74678665de5f109dd7e56fe979a",
        "commit_message": "HDFS-4726. Fix test failures after merging the INodeId-INode mapping from trunk.  Contributed by Jing Zhao\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1470735 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/43bdc22e9207a74678665de5f109dd7e56fe979a",
        "buggy_code": "@Test (timeout=60000)",
        "fixed_code": "@Test",
        "patch": "@@ -192,7 +192,7 @@ public void testRenameFileNotInSnapshot() throws Exception {\n    * Rename a file under a snapshottable directory, file exists\n    * in a snapshot.\n    */\n-  @Test (timeout=60000)\n+  @Test\n   public void testRenameFileInSnapshot() throws Exception {\n     hdfs.mkdirs(sub1);\n     hdfs.allowSnapshot(sub1);"
    },
    {
        "commit_id": "0ad27ad3e3f8de83cb4bb7330d69ea1cb6ea1663",
        "commit_message": "HDFS-4707. Add snapshot methods to FilterFileSystem and fix findbugs warnings.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1469119 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/0ad27ad3e3f8de83cb4bb7330d69ea1cb6ea1663",
        "buggy_code": "if (file.isUnderConstruction()) {",
        "fixed_code": "if (file instanceof INodeFileUnderConstruction) {",
        "patch": "@@ -192,7 +192,7 @@ public static void writeINodeFile(INodeFile file, DataOutput out,\n     SnapshotFSImageFormat.saveFileDiffList(file, out);\n \n     if (writeUnderConstruction) {\n-      if (file.isUnderConstruction()) {\n+      if (file instanceof INodeFileUnderConstruction) {\n         out.writeBoolean(true);\n         final INodeFileUnderConstruction uc = (INodeFileUnderConstruction)file;\n         writeString(uc.getClientName(), out);"
    },
    {
        "commit_id": "0ad27ad3e3f8de83cb4bb7330d69ea1cb6ea1663",
        "commit_message": "HDFS-4707. Add snapshot methods to FilterFileSystem and fix findbugs warnings.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1469119 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/0ad27ad3e3f8de83cb4bb7330d69ea1cb6ea1663",
        "buggy_code": "public class Root extends INodeDirectory {",
        "fixed_code": "static public class Root extends INodeDirectory {",
        "patch": "@@ -113,7 +113,7 @@ public static Snapshot findLatestSnapshot(INode inode, Snapshot anchor) {\n   }\n \n   /** The root directory of the snapshot. */\n-  public class Root extends INodeDirectory {\n+  static public class Root extends INodeDirectory {\n     Root(INodeDirectory other) {\n       super(other, false);\n     }"
    },
    {
        "commit_id": "0ad27ad3e3f8de83cb4bb7330d69ea1cb6ea1663",
        "commit_message": "HDFS-4707. Add snapshot methods to FilterFileSystem and fix findbugs warnings.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1469119 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/0ad27ad3e3f8de83cb4bb7330d69ea1cb6ea1663",
        "buggy_code": "if (size != -1) {",
        "fixed_code": "if (dir instanceof INodeDirectoryWithSnapshot) {",
        "patch": "@@ -238,7 +238,7 @@ public static void loadSnapshotList(\n   public static void loadDirectoryDiffList(INodeDirectory dir,\n       DataInput in, FSImageFormat.Loader loader) throws IOException {\n     final int size = in.readInt();\n-    if (size != -1) {\n+    if (dir instanceof INodeDirectoryWithSnapshot) {\n       INodeDirectoryWithSnapshot withSnapshot = (INodeDirectoryWithSnapshot)dir;\n       DirectoryDiffList diffs = withSnapshot.getDiffs();\n       for (int i = 0; i < size; i++) {"
    },
    {
        "commit_id": "6bda1f20ad396918edde211f709f5819a361b51e",
        "commit_message": "HDFS-4700. Fix the undo section of rename with snapshots.  Contributed by Jing Zhao\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1468632 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/6bda1f20ad396918edde211f709f5819a361b51e",
        "buggy_code": "currentINode.addSpaceConsumed(1, 0);",
        "fixed_code": "currentINode.addSpaceConsumed(1, 0, true);",
        "patch": "@@ -108,7 +108,7 @@ final Quota.Counts deleteSnapshotDiff(final Snapshot snapshot,\n   /** Add an {@link AbstractINodeDiff} for the given snapshot. */\n   final D addDiff(Snapshot latest, N currentINode)\n       throws QuotaExceededException {\n-    currentINode.addSpaceConsumed(1, 0);\n+    currentINode.addSpaceConsumed(1, 0, true);\n     return addLast(factory.createDiff(latest, currentINode));\n   }\n "
    },
    {
        "commit_id": "6bda1f20ad396918edde211f709f5819a361b51e",
        "commit_message": "HDFS-4700. Fix the undo section of rename with snapshots.  Contributed by Jing Zhao\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1468632 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/6bda1f20ad396918edde211f709f5819a361b51e",
        "buggy_code": "-counts.get(Quota.DISKSPACE));",
        "fixed_code": "-counts.get(Quota.DISKSPACE), true);",
        "patch": "@@ -325,7 +325,7 @@ Snapshot removeSnapshot(String snapshotName,\n         INodeDirectory parent = getParent();\n         if (parent != null) {\n           parent.addSpaceConsumed(-counts.get(Quota.NAMESPACE),\n-              -counts.get(Quota.DISKSPACE));\n+              -counts.get(Quota.DISKSPACE), true);\n         }\n       } catch(QuotaExceededException e) {\n         LOG.error(\"BUG: removeSnapshot increases namespace usage.\", e);"
    },
    {
        "commit_id": "3a54a5653bf1ea0b5b98e223c7500a9606abf04d",
        "commit_message": "YARN-112. Fixed a race condition during localization that fails containers. Contributed by Omkar Vinit Joshi.\nMAPREDUCE-5138. Fix LocalDistributedCacheManager after YARN-112. Contributed by Omkar Vinit Joshi.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1466196 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/3a54a5653bf1ea0b5b98e223c7500a9606abf04d",
        "buggy_code": "return new FSDownload(lfs, ugi, conf, path, rsrc, new Random());",
        "fixed_code": "return new FSDownload(lfs, ugi, conf, path, rsrc);",
        "patch": "@@ -198,7 +198,7 @@ CompletionService<Path> createCompletionService(ExecutorService exec) {\n   Callable<Path> download(Path path, LocalResource rsrc,\n       UserGroupInformation ugi) throws IOException {\n     DiskChecker.checkDir(new File(path.toUri().getRawPath()));\n-    return new FSDownload(lfs, ugi, conf, path, rsrc, new Random());\n+    return new FSDownload(lfs, ugi, conf, path, rsrc);\n   }\n \n   static long getEstimatedSize(LocalResource rsrc) {"
    },
    {
        "commit_id": "55865f42c43c8e8c6282952722a06f2a58f0c264",
        "commit_message": "YARN-557. Fix TestUnmanagedAMLauncher failure on Windows. Contributed by Chris Nauroth.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1465869 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/55865f42c43c8e8c6282952722a06f2a58f0c264",
        "buggy_code": "USER(\"USER\"),",
        "fixed_code": "USER(Shell.WINDOWS ? \"USERNAME\": \"USER\"),",
        "patch": "@@ -103,7 +103,7 @@ public enum Environment {\n      * $USER\n      * Final, non-modifiable.\n      */\n-    USER(\"USER\"),\n+    USER(Shell.WINDOWS ? \"USERNAME\": \"USER\"),\n     \n     /**\n      * $LOGNAME"
    },
    {
        "commit_id": "55865f42c43c8e8c6282952722a06f2a58f0c264",
        "commit_message": "YARN-557. Fix TestUnmanagedAMLauncher failure on Windows. Contributed by Chris Nauroth.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1465869 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/55865f42c43c8e8c6282952722a06f2a58f0c264",
        "buggy_code": ".name());",
        "fixed_code": ".key());",
        "patch": "@@ -683,7 +683,7 @@ public void run() {\n       ctx.setResource(container.getResource());\n \n       String jobUserName = System.getenv(ApplicationConstants.Environment.USER\n-          .name());\n+          .key());\n       ctx.setUser(jobUserName);\n       LOG.info(\"Setting user in ContainerLaunchContext to: \" + jobUserName);\n "
    },
    {
        "commit_id": "46315a2d914058969c7234272420c063ce268bf5",
        "commit_message": "MAPREDUCE-5062. Fix MR AM to read max-retries from the RM. Contributed by *Zhijie Shen.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1460923 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/46315a2d914058969c7234272420c063ce268bf5",
        "buggy_code": ".currentTimeMillis());",
        "fixed_code": ".currentTimeMillis(), MRJobConfig.DEFAULT_MR_AM_MAX_ATTEMPTS);",
        "patch": "@@ -192,7 +192,7 @@ public MRApp(ApplicationAttemptId appAttemptId, ContainerId amContainerId,\n       int maps, int reduces, boolean autoComplete, String testName,\n       boolean cleanOnStart, int startCount, Clock clock) {\n     super(appAttemptId, amContainerId, NM_HOST, NM_PORT, NM_HTTP_PORT, clock, System\n-        .currentTimeMillis());\n+        .currentTimeMillis(), MRJobConfig.DEFAULT_MR_AM_MAX_ATTEMPTS);\n     this.testWorkDir = new File(\"target\", testName);\n     testAbsPath = new Path(testWorkDir.getAbsolutePath());\n     LOG.info(\"PathUsed: \" + testAbsPath);"
    },
    {
        "commit_id": "8d95784bf1a7fcf473fd5a9fed0140521a92a968",
        "commit_message": "HDFS-4557. Fix FSDirectory#delete when INode#cleanSubtree returns 0.  Contributed by Jing Zhao\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1454138 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/8d95784bf1a7fcf473fd5a9fed0140521a92a968",
        "buggy_code": "int clear() {",
        "fixed_code": "public int clear() {",
        "patch": "@@ -50,7 +50,7 @@ public final List<D> asList() {\n   }\n   \n   /** Get the size of the list and then clear it. */\n-  int clear() {\n+  public int clear() {\n     final int n = diffs.size();\n     diffs.clear();\n     return n;"
    },
    {
        "commit_id": "8d95784bf1a7fcf473fd5a9fed0140521a92a968",
        "commit_message": "HDFS-4557. Fix FSDirectory#delete when INode#cleanSubtree returns 0.  Contributed by Jing Zhao\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1454138 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/8d95784bf1a7fcf473fd5a9fed0140521a92a968",
        "buggy_code": "return 1;",
        "fixed_code": "return prior == null ? 1 : 0;",
        "patch": "@@ -121,7 +121,7 @@ public int cleanSubtree(final Snapshot snapshot, Snapshot prior,\n     } else { // delete a snapshot\n       return diffs.deleteSnapshotDiff(snapshot, prior, this, collectedBlocks);\n     }\n-    return 1;\n+    return prior == null ? 1 : 0;\n   }\n \n   @Override"
    },
    {
        "commit_id": "df68c56267ca7dfbfee4b241bc84325d1760d12d",
        "commit_message": "MAPREDUCE-3685. Fix bugs in MergeManager to ensure compression codec is appropriately used and that on-disk segments are correctly sorted on file-size. Contributed by Anty Rao and Ravi Prakash.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1453365 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/df68c56267ca7dfbfee4b241bc84325d1760d12d",
        "buggy_code": "static <K extends Object, V extends Object>",
        "fixed_code": "public static <K extends Object, V extends Object>",
        "patch": "@@ -169,7 +169,7 @@ RawKeyValueIterator merge(Configuration conf, FileSystem fs,\n   }\n \n \n-  static <K extends Object, V extends Object>\n+  public static <K extends Object, V extends Object>\n   RawKeyValueIterator merge(Configuration conf, FileSystem fs,\n                           Class<K> keyClass, Class<V> valueClass,\n                           CompressionCodec codec,"
    },
    {
        "commit_id": "4840775e3d1485af3983f63ece2fc394b89563ef",
        "commit_message": "HADOOP-9323. Fix typos in API documentation. Contributed by Suresh Srinivas.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1449977 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/4840775e3d1485af3983f63ece2fc394b89563ef",
        "buggy_code": "hash = (31 * hash) + (int)b[i];",
        "fixed_code": "hash = (31 * hash) + b[i];",
        "patch": "@@ -192,7 +192,7 @@ public int hashCode() {\n     int hash = 1;\n     byte[] b = this.get();\n     for (int i = 0; i < count; i++)\n-      hash = (31 * hash) + (int)b[i];\n+      hash = (31 * hash) + b[i];\n     return hash;\n   }\n   "
    },
    {
        "commit_id": "f29fa9e820e25730d00a1a00c51c6f11028fb5a7",
        "commit_message": "HDFS-4499. Fix file/directory/snapshot deletion for file diff.  Contributed by Jing Zhao\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1448504 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/f29fa9e820e25730d00a1a00c51c6f11028fb5a7",
        "buggy_code": "Snapshot.findLatestSnapshot(pendingFile));",
        "fixed_code": "Snapshot.findLatestSnapshot(pendingFile, null));",
        "patch": "@@ -3446,7 +3446,7 @@ void commitBlockSynchronization(ExtendedBlock lastblock,\n \n         //remove lease, close file\n         finalizeINodeFileUnderConstruction(src, pendingFile,\n-            Snapshot.findLatestSnapshot(pendingFile));\n+            Snapshot.findLatestSnapshot(pendingFile, null));\n       } else {\n         // If this commit does not want to close the file, persist blocks\n         dir.persistBlocks(src, pendingFile);"
    },
    {
        "commit_id": "d9e2514d21c2ae356ee7fe8d4a857748b5defa4c",
        "commit_message": "HDFS-4487. Fix snapshot diff report for HDFS-4446.  Contributed by Jing Zhao\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1446385 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/d9e2514d21c2ae356ee7fe8d4a857748b5defa4c",
        "buggy_code": "final INode deleted = loader.loadINodeWithLocalName(false, in);",
        "fixed_code": "final INode deleted = loader.loadINodeWithLocalName(true, in);",
        "patch": "@@ -193,7 +193,7 @@ private static List<INode> loadDeletedList(INodeDirectoryWithSnapshot parent,\n     int deletedSize = in.readInt();\n     List<INode> deletedList = new ArrayList<INode>(deletedSize);\n     for (int i = 0; i < deletedSize; i++) {\n-      final INode deleted = loader.loadINodeWithLocalName(false, in);\n+      final INode deleted = loader.loadINodeWithLocalName(true, in);\n       deletedList.add(deleted);\n       // set parent: the parent field of an INode in the deleted list is not \n       // useful, but set the parent here to be consistent with the original "
    },
    {
        "commit_id": "2372e394dd99d69d396327d5a5e172953a8b8c6a",
        "commit_message": "HDFS-4189. Renames the getMutableXxx methods to getXxx4Write and fix a bug that some getExistingPathINodes calls should be getINodesInPath4Write.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1441193 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/2372e394dd99d69d396327d5a5e172953a8b8c6a",
        "buggy_code": "final INodesInPath iip = fsDir.getINodesInPath(path);",
        "fixed_code": "final INodesInPath iip = fsDir.getLastINodeInPath(path);",
        "patch": "@@ -638,7 +638,7 @@ private void loadFilesUnderConstruction(DataInputStream in,\n \n         // verify that file exists in namespace\n         String path = cons.getLocalName();\n-        final INodesInPath iip = fsDir.getINodesInPath(path);\n+        final INodesInPath iip = fsDir.getLastINodeInPath(path);\n         INodeFile oldnode = INodeFile.valueOf(iip.getINode(0), path);\n         cons.setLocalName(oldnode.getLocalNameBytes());\n         if (oldnode instanceof FileWithSnapshot"
    },
    {
        "commit_id": "2372e394dd99d69d396327d5a5e172953a8b8c6a",
        "commit_message": "HDFS-4189. Renames the getMutableXxx methods to getXxx4Write and fix a bug that some getExistingPathINodes calls should be getINodesInPath4Write.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1441193 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/2372e394dd99d69d396327d5a5e172953a8b8c6a",
        "buggy_code": "final INodesInPath inodesInPath = root.getExistingPathINodes(path, true);",
        "fixed_code": "final INodesInPath inodesInPath = root.getINodesInPath(path, true);",
        "patch": "@@ -122,7 +122,7 @@ void checkPermission(String path, INodeDirectory root, boolean doCheckOwner,\n     }\n     // check if (parentAccess != null) && file exists, then check sb\n       // Resolve symlinks, the check is performed on the link target.\n-      final INodesInPath inodesInPath = root.getExistingPathINodes(path, true); \n+      final INodesInPath inodesInPath = root.getINodesInPath(path, true); \n       final Snapshot snapshot = inodesInPath.getPathSnapshot();\n       final INode[] inodes = inodesInPath.getINodes();\n       int ancestorIndex = inodes.length - 2;"
    },
    {
        "commit_id": "d12f465c674b3bb5102671b6d6c2746261602d7e",
        "commit_message": "HDFS-4417. Fix case where local reads get disabled incorrectly. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-347@1437616 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/d12f465c674b3bb5102671b6d6c2746261602d7e",
        "buggy_code": "Peer peer = dfsClient.peerCache.get(dn.getDatanodeId());",
        "fixed_code": "Peer peer = dfsClient.peerCache.get(dn.getDatanodeId(), false);",
        "patch": "@@ -114,7 +114,7 @@ public void testKeepaliveTimeouts() throws Exception {\n     \n     // Take it out of the cache - reading should\n     // give an EOF.\n-    Peer peer = dfsClient.peerCache.get(dn.getDatanodeId());\n+    Peer peer = dfsClient.peerCache.get(dn.getDatanodeId(), false);\n     assertNotNull(peer);\n     assertEquals(-1, peer.getInputStream().read());\n   }"
    },
    {
        "commit_id": "00d318378e4b43d36be91b29ae3ef8a879a81e1e",
        "commit_message": "HDFS-4397. Fix a bug in INodeDirectoryWithSnapshot.Diff.combinePostDiff(..) that it may put the wrong node into the deleted list.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1433293 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/00d318378e4b43d36be91b29ae3ef8a879a81e1e",
        "buggy_code": "protected int collectSubtreeBlocksAndClear(BlocksMapUpdateInfo info) {",
        "fixed_code": "public int collectSubtreeBlocksAndClear(BlocksMapUpdateInfo info) {",
        "patch": "@@ -211,7 +211,7 @@ public void setBlocks(BlockInfo[] blocks) {\n   }\n \n   @Override\n-  protected int collectSubtreeBlocksAndClear(BlocksMapUpdateInfo info) {\n+  public int collectSubtreeBlocksAndClear(BlocksMapUpdateInfo info) {\n     parent = null;\n     if(blocks != null && info != null) {\n       for (BlockInfo blk : blocks) {"
    },
    {
        "commit_id": "35a145d92f6b8513924616cca2a08e569333cfb2",
        "commit_message": "HDFS-4401. Fix bug in DomainSocket path validation. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-347@1433229 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/35a145d92f6b8513924616cca2a08e569333cfb2",
        "buggy_code": "DomainSocket.validateSocketPathSecurity0(\"/foo\", 0);",
        "fixed_code": "DomainSocket.validateSocketPathSecurity0(\"/foo\", 1);",
        "patch": "@@ -698,7 +698,7 @@ public void testFdPassingPathSecurity() throws Exception {\n             \"component: \", e);\n       }\n       // Root should be secure\n-      DomainSocket.validateSocketPathSecurity0(\"/foo\", 0);\n+      DomainSocket.validateSocketPathSecurity0(\"/foo\", 1);\n     } finally {\n       tmp.close();\n     }"
    },
    {
        "commit_id": "b16dfc125dfd172900e34de1b46d3a06fe9aceb6",
        "commit_message": "MAPREDUCE-4848. TaskAttemptContext cast error during AM recovery. Contributed by Jerry Chen\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1431131 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/b16dfc125dfd172900e34de1b46d3a06fe9aceb6",
        "buggy_code": "appContext.getClock(), getCommitter());",
        "fixed_code": "appContext.getClock(), getCommitter(), isNewApiCommitter());",
        "patch": "@@ -579,7 +579,7 @@ protected EventHandler<JobFinishEvent> createJobFinishEventHandler() {\n    */\n   protected Recovery createRecoveryService(AppContext appContext) {\n     return new RecoveryService(appContext.getApplicationAttemptId(),\n-        appContext.getClock(), getCommitter());\n+        appContext.getClock(), getCommitter(), isNewApiCommitter());\n   }\n \n   /** Create and initialize (but don't start) a single job. "
    },
    {
        "commit_id": "81222adc11b93ab97fa91bdff2e39569dfa1909c",
        "commit_message": "YARN-217. Fix RMAdmin protocol description to make it work in secure mode also. Contributed by Devaraj K.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1429683 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/81222adc11b93ab97fa91bdff2e39569dfa1909c",
        "buggy_code": "protocolName = \"org.apache.hadoop.yarn.server.nodemanager.api.RMAdminProtocolPB\",",
        "fixed_code": "protocolName = \"org.apache.hadoop.yarn.api.RMAdminProtocolPB\",",
        "patch": "@@ -21,7 +21,7 @@\n import org.apache.hadoop.yarn.proto.RMAdminProtocol.RMAdminProtocolService;\n \n @ProtocolInfo(\n-    protocolName = \"org.apache.hadoop.yarn.server.nodemanager.api.RMAdminProtocolPB\",\n+    protocolName = \"org.apache.hadoop.yarn.api.RMAdminProtocolPB\",\n     protocolVersion = 1)\n public interface RMAdminProtocolPB extends RMAdminProtocolService.BlockingInterface {\n "
    },
    {
        "commit_id": "a7d444d002c664208669ec6ddf3bcb1db71e3741",
        "commit_message": "MAPREDUCE-4902. Fix typo \"receievd\" should be \"received\" in log output. Contributed by Albert Chu\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1426018 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/a7d444d002c664208669ec6ddf3bcb1db71e3741",
        "buggy_code": "LOG.info(\"for url=\"+msgToEncode+\" sent hash and receievd reply\");",
        "fixed_code": "LOG.info(\"for url=\"+msgToEncode+\" sent hash and received reply\");",
        "patch": "@@ -282,7 +282,7 @@ protected void copyFromHost(MapHost host) throws IOException {\n       LOG.debug(\"url=\"+msgToEncode+\";encHash=\"+encHash+\";replyHash=\"+replyHash);\n       // verify that replyHash is HMac of encHash\n       SecureShuffleUtils.verifyReply(replyHash, encHash, jobTokenSecret);\n-      LOG.info(\"for url=\"+msgToEncode+\" sent hash and receievd reply\");\n+      LOG.info(\"for url=\"+msgToEncode+\" sent hash and received reply\");\n     } catch (IOException ie) {\n       boolean connectExcpt = ie instanceof ConnectException;\n       ioErrs.increment(1);"
    },
    {
        "commit_id": "39d25fbac331ede57196f7a2d2d5e26e2fbc1c9f",
        "commit_message": "HDFS-4293. Fix TestSnapshot failure. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1419882 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/39d25fbac331ede57196f7a2d2d5e26e2fbc1c9f",
        "buggy_code": "INodeDirectory newNode = new INodeDirectory(dirNode);",
        "fixed_code": "INodeDirectory newNode = new INodeDirectory(dirNode, true);",
        "patch": "@@ -2008,7 +2008,7 @@ INodeDirectory unprotectedSetQuota(String src, long nsQuota, long dsQuota)\n         ((INodeDirectoryWithQuota)dirNode).setQuota(nsQuota, dsQuota);\n         if (!dirNode.isQuotaSet()) {\n           // will not come here for root because root's nsQuota is always set\n-          INodeDirectory newNode = new INodeDirectory(dirNode);\n+          INodeDirectory newNode = new INodeDirectory(dirNode, true);\n           INodeDirectory parent = (INodeDirectory)inodes[inodes.length-2];\n           dirNode = newNode;\n           parent.replaceChild(newNode);"
    },
    {
        "commit_id": "39d25fbac331ede57196f7a2d2d5e26e2fbc1c9f",
        "commit_message": "HDFS-4293. Fix TestSnapshot failure. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1419882 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/39d25fbac331ede57196f7a2d2d5e26e2fbc1c9f",
        "buggy_code": "super(other);",
        "fixed_code": "super(other, true);",
        "patch": "@@ -44,7 +44,7 @@ public class INodeDirectoryWithQuota extends INodeDirectory {\n    */\n   protected INodeDirectoryWithQuota(long nsQuota, long dsQuota,\n       INodeDirectory other) {\n-    super(other);\n+    super(other, true);\n     INode.DirCounts counts = new INode.DirCounts();\n     other.spaceConsumedInTree(counts);\n     this.nsCount = counts.getNsCount();"
    },
    {
        "commit_id": "39d25fbac331ede57196f7a2d2d5e26e2fbc1c9f",
        "commit_message": "HDFS-4293. Fix TestSnapshot failure. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1419882 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/39d25fbac331ede57196f7a2d2d5e26e2fbc1c9f",
        "buggy_code": "final INodeDirectory newinode = new INodeDirectory(oldinode);",
        "fixed_code": "final INodeDirectory newinode = new INodeDirectory(oldinode, true);",
        "patch": "@@ -220,7 +220,7 @@ static void modify(INode inode, final List<INode> current, Diff diff) {\n     final int i = Diff.search(current, inode);\n     Assert.assertTrue(i >= 0);\n     final INodeDirectory oldinode = (INodeDirectory)current.get(i);\n-    final INodeDirectory newinode = new INodeDirectory(oldinode);\n+    final INodeDirectory newinode = new INodeDirectory(oldinode, true);\n     newinode.setModificationTime(oldinode.getModificationTime() + 1);\n \n     current.set(i, newinode);"
    },
    {
        "commit_id": "ad619d34d044d7353922dbdce5121461f4887548",
        "commit_message": "HDFS-4260 Fix HDFS tests to set test dir to a valid HDFS path as opposed to the local build path (Chris Nauroth via Sanjay)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1418424 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/ad619d34d044d7353922dbdce5121461f4887548",
        "buggy_code": "public static final String TEST_ROOT_DIR =",
        "fixed_code": "public static String TEST_ROOT_DIR =",
        "patch": "@@ -32,7 +32,7 @@\n  */\n public final class FileContextTestHelper {\n   // The test root is relative to the <wd>/build/test/data by default\n-  public static final String TEST_ROOT_DIR = \n+  public static String TEST_ROOT_DIR = \n     System.getProperty(\"test.build.data\", \"build/test/data\") + \"/test\";\n   private static final int DEFAULT_BLOCK_SIZE = 1024;\n   private static final int DEFAULT_NUM_BLOCKS = 2;"
    },
    {
        "commit_id": "ad619d34d044d7353922dbdce5121461f4887548",
        "commit_message": "HDFS-4260 Fix HDFS tests to set test dir to a valid HDFS path as opposed to the local build path (Chris Nauroth via Sanjay)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1418424 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/ad619d34d044d7353922dbdce5121461f4887548",
        "buggy_code": "public static final String TEST_ROOT_DIR =",
        "fixed_code": "public static String TEST_ROOT_DIR =",
        "patch": "@@ -34,7 +34,7 @@\n  */\n public final class FileSystemTestHelper {\n   // The test root is relative to the <wd>/build/test/data by default\n-  public static final String TEST_ROOT_DIR = \n+  public static String TEST_ROOT_DIR = \n     System.getProperty(\"test.build.data\", \"target/test/data\") + \"/test\";\n   private static final int DEFAULT_BLOCK_SIZE = 1024;\n   private static final int DEFAULT_NUM_BLOCKS = 2;"
    },
    {
        "commit_id": "c271f3cded8636724673882eac3cd2229c157f31",
        "commit_message": "MAPREDUCE-4723 amendment. Fix JobHistory Event handling. (Contributed by Sandy Ryza)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1411292 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/c271f3cded8636724673882eac3cd2229c157f31",
        "buggy_code": "throw new YarnException(\"Invalid event type\");",
        "fixed_code": "break;",
        "patch": "@@ -601,7 +601,7 @@ public void processEventForJobSummary(HistoryEvent event, JobSummary summary,\n       setSummarySlotSeconds(summary, context.getJob(jobId).getAllCounters());\n       break;\n     default:\n-      throw new YarnException(\"Invalid event type\");\n+      break;\n     }\n   }\n "
    },
    {
        "commit_id": "8ca8687fb2fc74ab7c5199a93c70661996ad9e72",
        "commit_message": "HDFS-4188. Add Snapshot.ID_COMPARATOR for comparing IDs and fix a bug in ReadOnlyList.Util.binarySearch(..).\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1410027 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/8ca8687fb2fc74ab7c5199a93c70661996ad9e72",
        "buggy_code": "if (snapshot == null || snapshot.compareTo(s) < 0) {",
        "fixed_code": "if (Snapshot.ID_COMPARATOR.compare(snapshot, s) < 0) {",
        "patch": "@@ -586,7 +586,7 @@ private void setSnapshot(Snapshot s) {\n     }\n     \n     private void updateLatestSnapshot(Snapshot s) {\n-      if (snapshot == null || snapshot.compareTo(s) < 0) {\n+      if (Snapshot.ID_COMPARATOR.compare(snapshot, s) < 0) {\n         snapshot = s;\n       }\n     }"
    },
    {
        "commit_id": "905b17876c44634545a68300ff2f2d73fb86d3b7",
        "commit_message": "MAPREDUCE-4723. Fix warnings found by findbugs 2. Contributed by Sandy Ryza\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1409601 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/905b17876c44634545a68300ff2f2d73fb86d3b7",
        "buggy_code": "failures = failures == null ? 0 : failures;",
        "fixed_code": "failures = failures == null ? Integer.valueOf(0) : failures;",
        "patch": "@@ -210,7 +210,7 @@ protected void containerFailedOnHost(String hostName) {\n       return; //already blacklisted\n     }\n     Integer failures = nodeFailures.remove(hostName);\n-    failures = failures == null ? 0 : failures;\n+    failures = failures == null ? Integer.valueOf(0) : failures;\n     failures++;\n     LOG.info(failures + \" failures on node \" + hostName);\n     if (failures >= maxTaskFailuresPerNode) {"
    },
    {
        "commit_id": "905b17876c44634545a68300ff2f2d73fb86d3b7",
        "commit_message": "MAPREDUCE-4723. Fix warnings found by findbugs 2. Contributed by Sandy Ryza\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1409601 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/905b17876c44634545a68300ff2f2d73fb86d3b7",
        "buggy_code": "String.format(\"\\n|%1$-30s|%2$-30s|%3$-10s|%4$-10s|%5$-10s\",",
        "fixed_code": "String.format(\"%n|%1$-30s|%2$-30s|%3$-10s|%4$-10s|%5$-10s\",",
        "patch": "@@ -188,7 +188,7 @@ private void printCounters(StringBuffer buff, Counters totalCounters,\n              decimal.format(counter.getValue());\n \n            buff.append(\n-               String.format(\"\\n|%1$-30s|%2$-30s|%3$-10s|%4$-10s|%5$-10s\", \n+               String.format(\"%n|%1$-30s|%2$-30s|%3$-10s|%4$-10s|%5$-10s\", \n                    totalGroup.getDisplayName(),\n                    counter.getDisplayName(),\n                    mapValue, reduceValue, totalValue));"
    },
    {
        "commit_id": "905b17876c44634545a68300ff2f2d73fb86d3b7",
        "commit_message": "MAPREDUCE-4723. Fix warnings found by findbugs 2. Contributed by Sandy Ryza\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1409601 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/905b17876c44634545a68300ff2f2d73fb86d3b7",
        "buggy_code": "static final SimpleDateFormat dateFormat =",
        "fixed_code": "final SimpleDateFormat dateFormat =",
        "patch": "@@ -38,7 +38,7 @@\n  */\n public class HsJobsBlock extends HtmlBlock {\n   final AppContext appContext;\n-  static final SimpleDateFormat dateFormat =\n+  final SimpleDateFormat dateFormat =\n     new SimpleDateFormat(\"yyyy.MM.dd HH:mm:ss z\");\n \n   @Inject HsJobsBlock(AppContext appCtx) {"
    },
    {
        "commit_id": "312eb235981166b905a668c82c146ce373ecaffa",
        "commit_message": "HDFS-4048. Use ERROR instead of INFO for volume failure logs. Contributed by Stephen Chu\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1407345 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/312eb235981166b905a668c82c146ce373ecaffa",
        "buggy_code": "FsDatasetImpl.LOG.info(\"Completed checkDirs. Removed \" + removedVols.size()",
        "fixed_code": "FsDatasetImpl.LOG.warn(\"Completed checkDirs. Removed \" + removedVols.size()",
        "patch": "@@ -137,7 +137,7 @@ synchronized List<FsVolumeImpl> checkDirs() {\n     if (removedVols != null && removedVols.size() > 0) {\n       // Replace volume list\n       volumes = Collections.unmodifiableList(volumeList);\n-      FsDatasetImpl.LOG.info(\"Completed checkDirs. Removed \" + removedVols.size()\n+      FsDatasetImpl.LOG.warn(\"Completed checkDirs. Removed \" + removedVols.size()\n           + \" volumes. Current volumes: \" + this);\n     }\n "
    },
    {
        "commit_id": "88d326f0a411442b75d1a95425f150621b51da59",
        "commit_message": "YARN-179. Fix some unit test failures. (Contributed by Vinod Kumar Vavilapalli)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1401481 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/88d326f0a411442b75d1a95425f150621b51da59",
        "buggy_code": "amQueue = cliParser.getOptionValue(\"queue\", \"\");",
        "fixed_code": "amQueue = cliParser.getOptionValue(\"queue\", \"default\");",
        "patch": "@@ -143,7 +143,7 @@ public boolean init(String[] args) throws ParseException {\n \n     appName = cliParser.getOptionValue(\"appname\", \"UnmanagedAM\");\n     amPriority = Integer.parseInt(cliParser.getOptionValue(\"priority\", \"0\"));\n-    amQueue = cliParser.getOptionValue(\"queue\", \"\");\n+    amQueue = cliParser.getOptionValue(\"queue\", \"default\");\n     classpath = cliParser.getOptionValue(\"classpath\", null);\n \n     amCmd = cliParser.getOptionValue(\"cmd\");"
    },
    {
        "commit_id": "21b8d7b1fdb2284cbc079f2d4411cd1a004629f1",
        "commit_message": "YARN-161. Fix multiple compiler warnings for unchecked operations in YARN common. Contributed by Chris Nauroth.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1399056 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/21b8d7b1fdb2284cbc079f2d4411cd1a004629f1",
        "buggy_code": "Class clazz = Class.forName(factoryClassName);",
        "fixed_code": "Class<?> clazz = Class.forName(factoryClassName);",
        "patch": "@@ -58,7 +58,7 @@ public static RecordFactory getRecordFactory(Configuration conf) {\n   \n   private static Object getFactoryClassInstance(String factoryClassName) {\n     try {\n-      Class clazz = Class.forName(factoryClassName);\n+      Class<?> clazz = Class.forName(factoryClassName);\n       Method method = clazz.getMethod(\"get\", null);\n       method.setAccessible(true);\n       return method.invoke(null, null);"
    },
    {
        "commit_id": "21b8d7b1fdb2284cbc079f2d4411cd1a004629f1",
        "commit_message": "YARN-161. Fix multiple compiler warnings for unchecked operations in YARN common. Contributed by Chris Nauroth.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1399056 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/21b8d7b1fdb2284cbc079f2d4411cd1a004629f1",
        "buggy_code": "Class clazz = Class.forName(factoryClassName);",
        "fixed_code": "Class<?> clazz = Class.forName(factoryClassName);",
        "patch": "@@ -73,7 +73,7 @@ public static RpcClientFactory getClientFactory(Configuration conf) {\n \n   private static Object getFactoryClassInstance(String factoryClassName) {\n     try {\n-      Class clazz = Class.forName(factoryClassName);\n+      Class<?> clazz = Class.forName(factoryClassName);\n       Method method = clazz.getMethod(\"get\", null);\n       method.setAccessible(true);\n       return method.invoke(null, null);"
    },
    {
        "commit_id": "21b8d7b1fdb2284cbc079f2d4411cd1a004629f1",
        "commit_message": "YARN-161. Fix multiple compiler warnings for unchecked operations in YARN common. Contributed by Chris Nauroth.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1399056 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/21b8d7b1fdb2284cbc079f2d4411cd1a004629f1",
        "buggy_code": "Class clazz = Class.forName(factoryClassName);",
        "fixed_code": "Class<?> clazz = Class.forName(factoryClassName);",
        "patch": "@@ -51,7 +51,7 @@ public static YarnRemoteExceptionFactory getYarnRemoteExceptionFactory(Configura\n   \n   private static Object getFactoryClassInstance(String factoryClassName) {\n     try {\n-      Class clazz = Class.forName(factoryClassName);\n+      Class<?> clazz = Class.forName(factoryClassName);\n       Method method = clazz.getMethod(\"get\", null);\n       method.setAccessible(true);\n       return method.invoke(null, null);"
    },
    {
        "commit_id": "5c3a3310402c37320c6b28eb3dd72cfb79c39971",
        "commit_message": "MAPREDUCE-4574. Fix TotalOrderParitioner to work with non-WritableComparable key types. Contributed by Harsh J. (harsh)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1395936 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/5c3a3310402c37320c6b28eb3dd72cfb79c39971",
        "buggy_code": "public class TotalOrderPartitioner<K extends WritableComparable<?>,V>",
        "fixed_code": "public class TotalOrderPartitioner<K ,V>",
        "patch": "@@ -31,7 +31,7 @@\n  */\n @InterfaceAudience.Public\n @InterfaceStability.Stable\n-public class TotalOrderPartitioner<K extends WritableComparable<?>,V>\n+public class TotalOrderPartitioner<K ,V>\n     extends org.apache.hadoop.mapreduce.lib.partition.TotalOrderPartitioner<K, V>\n     implements Partitioner<K,V> {\n "
    },
    {
        "commit_id": "57807d50bf0fe84444eb0df8f2dbcefcbb39a493",
        "commit_message": "MAPREDUCE-4681. Fix unit tests broken by HDFS-3910. Contributed by Arun C. Murthy.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1392075 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/57807d50bf0fe84444eb0df8f2dbcefcbb39a493",
        "buggy_code": "public void testJobQueues() throws IOException {",
        "fixed_code": "public void testJobQueues() throws Exception {",
        "patch": "@@ -98,7 +98,7 @@ protected void tearDown() throws Exception {\n     dfsCluster.shutdown();\n   }\n \n-  public void testJobQueues() throws IOException {\n+  public void testJobQueues() throws Exception {\n     JobClient jc = new JobClient(mrCluster.createJobConf());\n     String expectedQueueInfo = \"Maximum Tasks Per Job :: 10\";\n     JobQueueInfo[] queueInfos = jc.getQueues();"
    },
    {
        "commit_id": "40062e1aaa09628c6f45d20298fd66d799fd1f3f",
        "commit_message": "Fix NodeManager to verify the application's user-name.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1390825 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/40062e1aaa09628c6f45d20298fd66d799fd1f3f",
        "buggy_code": "capability);",
        "fixed_code": "application.getUser(), capability);",
        "patch": "@@ -1197,7 +1197,7 @@ public Container createContainer(FiCaSchedulerApp application, FiCaSchedulerNode\n     if (UserGroupInformation.isSecurityEnabled()) {\n       containerToken =\n           containerTokenSecretManager.createContainerToken(containerId, nodeId,\n-            capability);\n+            application.getUser(), capability);\n       if (containerToken == null) {\n         return null; // Try again later.\n       }"
    },
    {
        "commit_id": "40062e1aaa09628c6f45d20298fd66d799fd1f3f",
        "commit_message": "Fix NodeManager to verify the application's user-name.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1390825 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/40062e1aaa09628c6f45d20298fd66d799fd1f3f",
        "buggy_code": "capability);",
        "fixed_code": "application.getUser(), capability);",
        "patch": "@@ -161,7 +161,7 @@ public Container createContainer(\n     if (UserGroupInformation.isSecurityEnabled()) {\n       containerToken =\n           containerTokenSecretManager.createContainerToken(containerId, nodeId,\n-            capability);\n+            application.getUser(), capability);\n       if (containerToken == null) {\n         return null; // Try again later.\n       }"
    },
    {
        "commit_id": "0187553d9ac2113cad439dfce70874dee5235768",
        "commit_message": "Fix the length of the secret key.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1389194 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/0187553d9ac2113cad439dfce70874dee5235768",
        "buggy_code": "private static final int KEY_LENGTH = 20;",
        "fixed_code": "private static final int KEY_LENGTH = 64;",
        "patch": "@@ -93,7 +93,7 @@ public void checkAvailableForRead() throws StandbyException {\n   /**\n    * The length of the random keys to use.\n    */\n-  private static final int KEY_LENGTH = 20;\n+  private static final int KEY_LENGTH = 64;\n \n   /**\n    * A thread local store for the Macs."
    },
    {
        "commit_id": "aa049397f1c32ea7821de95ed76e62fbebdfc429",
        "commit_message": "Fix MAPREDUCE-4580 build breakage.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1381315 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/aa049397f1c32ea7821de95ed76e62fbebdfc429",
        "buggy_code": "import org.hadoop.yarn.client.YarnClientImpl;",
        "fixed_code": "import org.apache.hadoop.yarn.client.YarnClientImpl;",
        "patch": "@@ -43,7 +43,7 @@\n import org.apache.hadoop.yarn.api.records.YarnClusterMetrics;\n import org.apache.hadoop.yarn.conf.YarnConfiguration;\n import org.apache.hadoop.yarn.util.ProtoUtils;\n-import org.hadoop.yarn.client.YarnClientImpl;\n+import org.apache.hadoop.yarn.client.YarnClientImpl;\n \n public class ResourceMgrDelegate extends YarnClientImpl {\n   private static final Log LOG = LogFactory.getLog(ResourceMgrDelegate.class);"
    },
    {
        "commit_id": "45a8e8c5a46535287de97fd6609c0743eef888ee",
        "commit_message": "YARN-60. Fixed a bug in ResourceManager which causes all NMs to get NPEs and thus causes all containers to be rejected. Contributed by Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1379550 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/45a8e8c5a46535287de97fd6609c0743eef888ee",
        "buggy_code": "protected boolean isSecurityEnabled() {",
        "fixed_code": "protected boolean isTokenKeepAliveEnabled(Configuration conf) {",
        "patch": "@@ -261,7 +261,7 @@ protected ResourceTracker getRMClient() {\n     }\n     \n     @Override\n-    protected boolean isSecurityEnabled() {\n+    protected boolean isTokenKeepAliveEnabled(Configuration conf) {\n       return true;\n     }\n   }"
    },
    {
        "commit_id": "45a8e8c5a46535287de97fd6609c0743eef888ee",
        "commit_message": "YARN-60. Fixed a bug in ResourceManager which causes all NMs to get NPEs and thus causes all containers to be rejected. Contributed by Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1379550 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/45a8e8c5a46535287de97fd6609c0743eef888ee",
        "buggy_code": "node = new RMNodeImpl(nodeId, rmContext, null, 0, 0, null, null, null);",
        "fixed_code": "node = new RMNodeImpl(nodeId, rmContext, null, 0, 0, null, null);",
        "patch": "@@ -105,7 +105,7 @@ public Void answer(InvocationOnMock invocation) throws Throwable {\n         new TestSchedulerEventDispatcher());\n     \n     NodeId nodeId = BuilderUtils.newNodeId(\"localhost\", 0);\n-    node = new RMNodeImpl(nodeId, rmContext, null, 0, 0, null, null, null);\n+    node = new RMNodeImpl(nodeId, rmContext, null, 0, 0, null, null);\n \n   }\n   "
    },
    {
        "commit_id": "3b5ea8750202ad9ed0e297d92a90d6dc772ce12a",
        "commit_message": "HDFS-3629. Fix the typo in the error message about inconsistent storage layout version. Contributed by Brandon Li. (harsh)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1359905 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/3b5ea8750202ad9ed0e297d92a90d6dc772ce12a",
        "buggy_code": "\"Storage directories containe multiple layout versions: \"",
        "fixed_code": "\"Storage directories contain multiple layout versions: \"",
        "patch": "@@ -1076,7 +1076,7 @@ FSImageStorageInspector readAndInspectDirs()\n     }\n     if (multipleLV) {            \n       throw new IOException(\n-          \"Storage directories containe multiple layout versions: \"\n+          \"Storage directories contain multiple layout versions: \"\n               + layoutVersions);\n     }\n     // If the storage directories are with the new layout version"
    },
    {
        "commit_id": "d6467bb3b1fd45833fbad7fbb54c407f89b68e7b",
        "commit_message": "MAPREDUCE-4313. TestTokenCache doesn't compile due TokenCache.getDelegationToken compilation error (bobby)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1346406 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/d6467bb3b1fd45833fbad7fbb54c407f89b68e7b",
        "buggy_code": "Token<DelegationTokenIdentifier> nnt = TokenCache.getDelegationToken(",
        "fixed_code": "Token<DelegationTokenIdentifier> nnt = (Token<DelegationTokenIdentifier>)TokenCache.getDelegationToken(",
        "patch": "@@ -289,7 +289,7 @@ public void testGetTokensForNamenodes() throws IOException {\n     // this token is keyed by hostname:port key.\n     String fs_addr = \n       SecurityUtil.buildDTServiceName(p1.toUri(), NameNode.DEFAULT_PORT);\n-    Token<DelegationTokenIdentifier> nnt = TokenCache.getDelegationToken(\n+    Token<DelegationTokenIdentifier> nnt = (Token<DelegationTokenIdentifier>)TokenCache.getDelegationToken(\n         credentials, fs_addr);\n     System.out.println(\"dt for \" + p1 + \"(\" + fs_addr + \")\" + \" = \" +  nnt);\n     assertNotNull(\"Token for nn is null\", nnt);"
    },
    {
        "commit_id": "e576bac2f7fe0452007a394f3909507d17eaa6b3",
        "commit_message": "HADOOP-8444. Fix the tests FSMainOperationsBaseTest.java and FileContextMainOperationsBaseTest.java to avoid potential test failure. Contributed by Madhukara Phatak. (harsh)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1343732 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/e576bac2f7fe0452007a394f3909507d17eaa6b3",
        "buggy_code": "if(file.getName().contains(\"x\") || file.toString().contains(\"X\"))",
        "fixed_code": "if(file.getName().contains(\"x\") || file.getName().contains(\"X\"))",
        "patch": "@@ -75,7 +75,7 @@ public boolean accept(final Path file) {\n   //A test filter with returns any path containing a \"b\" \n   final private static PathFilter TEST_X_FILTER = new PathFilter() {\n     public boolean accept(Path file) {\n-      if(file.getName().contains(\"x\") || file.toString().contains(\"X\"))\n+      if(file.getName().contains(\"x\") || file.getName().contains(\"X\"))\n         return true;\n       else\n         return false;"
    },
    {
        "commit_id": "e576bac2f7fe0452007a394f3909507d17eaa6b3",
        "commit_message": "HADOOP-8444. Fix the tests FSMainOperationsBaseTest.java and FileContextMainOperationsBaseTest.java to avoid potential test failure. Contributed by Madhukara Phatak. (harsh)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1343732 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/e576bac2f7fe0452007a394f3909507d17eaa6b3",
        "buggy_code": "if(file.getName().contains(\"x\") || file.toString().contains(\"X\"))",
        "fixed_code": "if(file.getName().contains(\"x\") || file.getName().contains(\"X\"))",
        "patch": "@@ -75,7 +75,7 @@ public boolean accept(final Path file) {\n   //A test filter with returns any path containing a \"b\" \n   final private static PathFilter TEST_X_FILTER = new PathFilter() {\n     public boolean accept(Path file) {\n-      if(file.getName().contains(\"x\") || file.toString().contains(\"X\"))\n+      if(file.getName().contains(\"x\") || file.getName().contains(\"X\"))\n         return true;\n       else\n         return false;"
    },
    {
        "commit_id": "5258d6bf3fb8090739cf96f5089f96cee87393c4",
        "commit_message": "HDFS-3391. Fix InvalidateBlocks to compare blocks including their generation stamps. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1339897 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/5258d6bf3fb8090739cf96f5089f96cee87393c4",
        "buggy_code": "if (containsElem(index, element, hashCode)) {",
        "fixed_code": "if (getContainedElem(index, element, hashCode) != null) {",
        "patch": "@@ -88,7 +88,7 @@ protected boolean addElem(final T element) {\n     final int hashCode = element.hashCode();\n     final int index = getIndex(hashCode);\n     // return false if already present\n-    if (containsElem(index, element, hashCode)) {\n+    if (getContainedElem(index, element, hashCode) != null) {\n       return false;\n     }\n "
    },
    {
        "commit_id": "a9808de0d9a73a99c10a3e4290ec20778fed4f24",
        "commit_message": "HADOOP-8341. Fix or filter findbugs issues in hadoop-tools (bobby)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1335505 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/a9808de0d9a73a99c10a3e4290ec20778fed4f24",
        "buggy_code": "((JobConf)conf).setJar(testJar);",
        "fixed_code": "this.conf.setJar(testJar);",
        "patch": "@@ -117,7 +117,7 @@ public void setConf(Configuration conf) {\n     // will when running the mapreduce job.\n     String testJar = System.getProperty(TEST_HADOOP_ARCHIVES_JAR_PATH, null);\n     if (testJar != null) {\n-      ((JobConf)conf).setJar(testJar);\n+      this.conf.setJar(testJar);\n     }\n   }\n "
    },
    {
        "commit_id": "a9808de0d9a73a99c10a3e4290ec20778fed4f24",
        "commit_message": "HADOOP-8341. Fix or filter findbugs issues in hadoop-tools (bobby)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1335505 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/a9808de0d9a73a99c10a3e4290ec20778fed4f24",
        "buggy_code": "public static final String[] KNOWN_WORDS =",
        "fixed_code": "static final String[] KNOWN_WORDS =",
        "patch": "@@ -27,7 +27,7 @@\n  * //TODO There is no caching for saving memory.\n  */\n public class WordListAnonymizerUtility {\n-  public static final String[] KNOWN_WORDS = \n+  static final String[] KNOWN_WORDS = \n     new String[] {\"job\", \"tmp\", \"temp\", \"home\", \"homes\", \"usr\", \"user\", \"test\"};\n   \n   /**"
    },
    {
        "commit_id": "2584779166f0095bc26cb0bdb3fb0701c943dafe",
        "commit_message": "HDFS-3321. Fix safe mode turn off tip message.  Contributed by Ravi Prakash\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1330506 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/2584779166f0095bc26cb0bdb3fb0701c943dafe",
        "buggy_code": "numLive, (datanodeThreshold - numLive) + 1 , datanodeThreshold);",
        "fixed_code": "numLive, (datanodeThreshold - numLive), datanodeThreshold);",
        "patch": "@@ -3723,7 +3723,7 @@ String getTurnOffTip() {\n           msg += String.format(\n             \"The number of live datanodes %d needs an additional %d live \"\n             + \"datanodes to reach the minimum number %d.\",\n-            numLive, (datanodeThreshold - numLive) + 1 , datanodeThreshold);\n+            numLive, (datanodeThreshold - numLive), datanodeThreshold);\n         }\n         msg += \" \" + leaveMsg;\n       } else {"
    },
    {
        "commit_id": "2584779166f0095bc26cb0bdb3fb0701c943dafe",
        "commit_message": "HDFS-3321. Fix safe mode turn off tip message.  Contributed by Ravi Prakash\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1330506 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/2584779166f0095bc26cb0bdb3fb0701c943dafe",
        "buggy_code": "\"2 live datanodes to reach the minimum number 1. \" +",
        "fixed_code": "\"1 live datanodes to reach the minimum number 1. \" +",
        "patch": "@@ -342,7 +342,7 @@ public void testDatanodeThreshold() throws IOException {\n     String tipMsg = cluster.getNamesystem().getSafemode();\n     assertTrue(\"Safemode tip message looks right: \" + tipMsg,\n                tipMsg.contains(\"The number of live datanodes 0 needs an additional \" +\n-                               \"2 live datanodes to reach the minimum number 1. \" +\n+                               \"1 live datanodes to reach the minimum number 1. \" +\n                                \"Safe mode will be turned off automatically.\"));\n \n     // Start a datanode"
    },
    {
        "commit_id": "2bf19979b3725f4a4483afd766ffb9ef03be0c4c",
        "commit_message": "Merge trunk into auto-failover branch.\n\nNeeds a few tweaks to fix compilation - will do in followup commit. This is just a straight merge\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-3042@1324567 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/2bf19979b3725f4a4483afd766ffb9ef03be0c4c",
        "buggy_code": "hostDetails.append(\"destination host is: \\\"\").append(quoteHost(destHost))",
        "fixed_code": "hostDetails.append(\"destination host is: \").append(quoteHost(destHost))",
        "patch": "@@ -782,7 +782,7 @@ private static String getHostDetailsAsString(final String destHost,\n     hostDetails.append(\"local host is: \")\n         .append(quoteHost(localHost))\n         .append(\"; \");\n-    hostDetails.append(\"destination host is: \\\"\").append(quoteHost(destHost))\n+    hostDetails.append(\"destination host is: \").append(quoteHost(destHost))\n         .append(\":\")\n         .append(destPort).append(\"; \");\n     return hostDetails.toString();"
    },
    {
        "commit_id": "2bf19979b3725f4a4483afd766ffb9ef03be0c4c",
        "commit_message": "Merge trunk into auto-failover branch.\n\nNeeds a few tweaks to fix compilation - will do in followup commit. This is just a straight merge\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-3042@1324567 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/2bf19979b3725f4a4483afd766ffb9ef03be0c4c",
        "buggy_code": "dstImage.loadEdits(editsStreams, dstNamesystem);",
        "fixed_code": "dstImage.loadEdits(editsStreams, dstNamesystem, null);",
        "patch": "@@ -292,6 +292,6 @@ static void rollForwardByApplyingLogs(\n     }\n     LOG.info(\"Checkpointer about to load edits from \" +\n         editsStreams.size() + \" stream(s).\");\n-    dstImage.loadEdits(editsStreams, dstNamesystem);\n+    dstImage.loadEdits(editsStreams, dstNamesystem, null);\n   }\n }"
    },
    {
        "commit_id": "2bf19979b3725f4a4483afd766ffb9ef03be0c4c",
        "commit_message": "Merge trunk into auto-failover branch.\n\nNeeds a few tweaks to fix compilation - will do in followup commit. This is just a straight merge\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-3042@1324567 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/2bf19979b3725f4a4483afd766ffb9ef03be0c4c",
        "buggy_code": "editsLoaded = image.loadEdits(streams, namesystem);",
        "fixed_code": "editsLoaded = image.loadEdits(streams, namesystem, null);",
        "patch": "@@ -219,7 +219,7 @@ private void doTailEdits() throws IOException, InterruptedException {\n       // disk are ignored.\n       long editsLoaded = 0;\n       try {\n-        editsLoaded = image.loadEdits(streams, namesystem);\n+        editsLoaded = image.loadEdits(streams, namesystem, null);\n       } catch (EditLogInputException elie) {\n         editsLoaded = elie.getNumEditsLoaded();\n         throw elie;"
    },
    {
        "commit_id": "2bf19979b3725f4a4483afd766ffb9ef03be0c4c",
        "commit_message": "Merge trunk into auto-failover branch.\n\nNeeds a few tweaks to fix compilation - will do in followup commit. This is just a straight merge\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-3042@1324567 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/2bf19979b3725f4a4483afd766ffb9ef03be0c4c",
        "buggy_code": "DatanodeInfo d = new DatanodeInfo();",
        "fixed_code": "DatanodeInfo d = DFSTestUtil.getLocalDatanodeInfo();",
        "patch": "@@ -62,7 +62,7 @@ public void resetUGI() {\n    */\n   @Test\n   public void testLocatedBlocks2Locations() {\n-    DatanodeInfo d = new DatanodeInfo();\n+    DatanodeInfo d = DFSTestUtil.getLocalDatanodeInfo();\n     DatanodeInfo[] ds = new DatanodeInfo[1];\n     ds[0] = d;\n "
    },
    {
        "commit_id": "2bf19979b3725f4a4483afd766ffb9ef03be0c4c",
        "commit_message": "Merge trunk into auto-failover branch.\n\nNeeds a few tweaks to fix compilation - will do in followup commit. This is just a straight merge\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-3042@1324567 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/2bf19979b3725f4a4483afd766ffb9ef03be0c4c",
        "buggy_code": "static public final int N_ITERATIONS = 1024 * 4;",
        "fixed_code": "static public final int N_ITERATIONS = 1024;",
        "patch": "@@ -179,7 +179,7 @@ public int pRead(DFSInputStream dis, byte[] target, int startOff, int len)\n    */\n   static class ReadWorker extends Thread {\n \n-    static public final int N_ITERATIONS = 1024 * 4;\n+    static public final int N_ITERATIONS = 1024;\n \n     private static final double PROPORTION_NON_POSITIONAL_READ = 0.10;\n "
    },
    {
        "commit_id": "04cc1d614d0783ba3302f9de239d5c3b41f2b2db",
        "commit_message": "HADOOP-8251. Fix SecurityUtil.fetchServiceTicket after HADOOP-6941. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1310168 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/04cc1d614d0783ba3302f9de239d5c3b41f2b2db",
        "buggy_code": "krb5utilClass = Class.forName(\"sun.security.jgss.krb5\");",
        "fixed_code": "krb5utilClass = Class.forName(\"sun.security.jgss.krb5.Krb5Util\");",
        "patch": "@@ -171,7 +171,7 @@ public static void fetchServiceTicket(URL remoteHost) throws IOException {\n       } else {\n         principalClass = Class.forName(\"sun.security.krb5.PrincipalName\");\n         credentialsClass = Class.forName(\"sun.security.krb5.Credentials\");\n-        krb5utilClass = Class.forName(\"sun.security.jgss.krb5\");\n+        krb5utilClass = Class.forName(\"sun.security.jgss.krb5.Krb5Util\");\n       }\n       @SuppressWarnings(\"rawtypes\")\n       Constructor principalConstructor = principalClass.getConstructor(String.class, "
    },
    {
        "commit_id": "ea868d3d8b6c5e018eb104a560890c60d30fa269",
        "commit_message": "HDFS-3116. Typo in fetchdt error message. Contributed by AOE Takashi.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1304996 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/ea868d3d8b6c5e018eb104a560890c60d30fa269",
        "buggy_code": "System.err.println(\"ERROR: Must specify exacltly one token file\");",
        "fixed_code": "System.err.println(\"ERROR: Must specify exactly one token file\");",
        "patch": "@@ -132,7 +132,7 @@ public static void main(final String[] args) throws Exception {\n       printUsage(System.err);\n     }\n     if (remaining.length != 1 || remaining[0].charAt(0) == '-') {\n-      System.err.println(\"ERROR: Must specify exacltly one token file\");\n+      System.err.println(\"ERROR: Must specify exactly one token file\");\n       printUsage(System.err);\n     }\n     // default to using the local file system"
    },
    {
        "commit_id": "4cb809bf0499b78b554363417576e53c6a3eaa5b",
        "commit_message": "HDFS-3132. Fix findbugs warning on HDFS trunk. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1304681 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/4cb809bf0499b78b554363417576e53c6a3eaa5b",
        "buggy_code": "private HAState state;",
        "fixed_code": "private volatile HAState state;",
        "patch": "@@ -187,7 +187,7 @@ public long getProtocolVersion(String protocol,\n   protected FSNamesystem namesystem; \n   protected final Configuration conf;\n   protected NamenodeRole role;\n-  private HAState state;\n+  private volatile HAState state;\n   private final boolean haEnabled;\n   private final HAContext haContext;\n   protected boolean allowStaleStandbyReads;"
    },
    {
        "commit_id": "fab57a144de0cd515e1de9107e4d3ac58037d846",
        "commit_message": "MAPREDUCE-3792. Fix \"bin/mapred job -list\" to display all jobs instead of only the jobs owned by the user. Contributed by Jason Lowe.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1296721 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/fab57a144de0cd515e1de9107e4d3ac58037d846",
        "buggy_code": "public ApplicationReport createAndGetApplicationReport() {",
        "fixed_code": "public ApplicationReport createAndGetApplicationReport(boolean allowAccess) {",
        "patch": "@@ -207,7 +207,7 @@ public String getTrackingUrl() {\n       throw new UnsupportedOperationException(\"Not supported yet.\");\n     }\n     @Override\n-    public ApplicationReport createAndGetApplicationReport() {\n+    public ApplicationReport createAndGetApplicationReport(boolean allowAccess) {\n       throw new UnsupportedOperationException(\"Not supported yet.\");\n     }\n     @Override"
    },
    {
        "commit_id": "fab57a144de0cd515e1de9107e4d3ac58037d846",
        "commit_message": "MAPREDUCE-3792. Fix \"bin/mapred job -list\" to display all jobs instead of only the jobs owned by the user. Contributed by Jason Lowe.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1296721 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/fab57a144de0cd515e1de9107e4d3ac58037d846",
        "buggy_code": "public ApplicationReport createAndGetApplicationReport() {",
        "fixed_code": "public ApplicationReport createAndGetApplicationReport(boolean allowAccess) {",
        "patch": "@@ -119,7 +119,7 @@ public void setCurrentAppAttempt(RMAppAttempt attempt) {\n   }\n \n   @Override\n-  public ApplicationReport createAndGetApplicationReport() {\n+  public ApplicationReport createAndGetApplicationReport(boolean allowAccess) {\n     throw new UnsupportedOperationException(\"Not supported yet.\");\n   }\n "
    },
    {
        "commit_id": "28de3628f5c80c7ad642199d720d049fcf392f8b",
        "commit_message": "HDFS-3020. Fix editlog to automatically sync when buffer is full. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1295239 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/28de3628f5c80c7ad642199d720d049fcf392f8b",
        "buggy_code": "return bufReady.size() >= initBufferSize;",
        "fixed_code": "return bufCurrent.size() >= initBufferSize;",
        "patch": "@@ -86,7 +86,7 @@ void flushTo(OutputStream out) throws IOException {\n   }\n   \n   boolean shouldForceSync() {\n-    return bufReady.size() >= initBufferSize;\n+    return bufCurrent.size() >= initBufferSize;\n   }\n \n   DataOutputBuffer getCurrentBuf() {"
    },
    {
        "commit_id": "978a8050e28b2afb193a3e00d82a8475fa4d2428",
        "commit_message": "HDFS-2920. fix remaining TODO items. Contributed by Aaron T. Myers and Todd Lipcon.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1294923 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/978a8050e28b2afb193a3e00d82a8475fa4d2428",
        "buggy_code": "DatanodeProtocol nn = dataNode.getBPNamenode(bpid);",
        "fixed_code": "DatanodeProtocol nn = dataNode.getActiveNamenodeForBP(bpid);",
        "patch": "@@ -92,7 +92,7 @@ public synchronized boolean startUpgrade() throws IOException {\n           \"UpgradeManagerDatanode.currentUpgrades is not null.\";\n         assert upgradeDaemon == null : \n           \"UpgradeManagerDatanode.upgradeDaemon is not null.\";\n-        DatanodeProtocol nn = dataNode.getBPNamenode(bpid);\n+        DatanodeProtocol nn = dataNode.getActiveNamenodeForBP(bpid);\n         nn.processUpgradeCommand(broadcastCommand);\n         return true;\n       }"
    },
    {
        "commit_id": "dd732d5a24fff32f71c2fdc2da1aaf7d7c491a7b",
        "commit_message": "Fix TestViewFsFileStatusHdfs per previous commit.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1245762 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/dd732d5a24fff32f71c2fdc2da1aaf7d7c491a7b",
        "buggy_code": "Configuration conf = ViewFileSystemTestSetup.configWithViewfsScheme();",
        "fixed_code": "Configuration conf = ViewFileSystemTestSetup.createConfig();",
        "patch": "@@ -73,7 +73,7 @@ public void testFileStatusSerialziation()\n \n    long len = FileSystemTestHelper.createFile(fHdfs, testfilename);\n \n-    Configuration conf = ViewFileSystemTestSetup.configWithViewfsScheme();\n+    Configuration conf = ViewFileSystemTestSetup.createConfig();\n     ConfigUtil.addLink(conf, \"/tmp\", new URI(fHdfs.getUri().toString() + \"/tmp\"));\n     FileSystem vfs = FileSystem.get(FsConstants.VIEWFS_URI, conf);\n     assertEquals(ViewFileSystem.class, vfs.getClass());"
    },
    {
        "commit_id": "3c145d3492331959d21f6d0c3b8c7e71d35de69f",
        "commit_message": "HDFS-2909. HA: Inaccessible shared edits dir not getting removed from FSImage storage dirs upon error. Contributed by Bikas Saha.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1244753 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/3c145d3492331959d21f6d0c3b8c7e71d35de69f",
        "buggy_code": "journalSet.add(new FileJournalManager(sd), required);",
        "fixed_code": "journalSet.add(new FileJournalManager(sd, storage), required);",
        "patch": "@@ -221,7 +221,7 @@ private void initJournals(List<URI> dirs) {\n       if (u.getScheme().equals(NNStorage.LOCAL_URI_SCHEME)) {\n         StorageDirectory sd = storage.getStorageDirectory(u);\n         if (sd != null) {\n-          journalSet.add(new FileJournalManager(sd), required);\n+          journalSet.add(new FileJournalManager(sd, storage), required);\n         }\n       } else {\n         journalSet.add(createJournal(u), required);"
    },
    {
        "commit_id": "11db1b855fa08a5ef7cdf2d40bd18ee33eba92ee",
        "commit_message": "HDFS-2786. Fix host-based token incompatibilities in DFSUtil. Contributed by Kihwal Lee.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1241766 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/11db1b855fa08a5ef7cdf2d40bd18ee33eba92ee",
        "buggy_code": "namenodeAddress = DFSUtil.getSocketAddress(namenodeAddressInUrl);",
        "fixed_code": "namenodeAddress = NetUtils.createSocketAddr(namenodeAddressInUrl);",
        "patch": "@@ -498,7 +498,7 @@ private static InetSocketAddress getNNServiceAddress(ServletContext context,\n     String namenodeAddressInUrl = request.getParameter(NAMENODE_ADDRESS);\n     InetSocketAddress namenodeAddress = null;\n     if (namenodeAddressInUrl != null) {\n-      namenodeAddress = DFSUtil.getSocketAddress(namenodeAddressInUrl);\n+      namenodeAddress = NetUtils.createSocketAddr(namenodeAddressInUrl);\n     } else if (context != null) {\n       namenodeAddress = NameNodeHttpServer.getNameNodeAddressFromContext(\n           context); "
    },
    {
        "commit_id": "11db1b855fa08a5ef7cdf2d40bd18ee33eba92ee",
        "commit_message": "HDFS-2786. Fix host-based token incompatibilities in DFSUtil. Contributed by Kihwal Lee.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1241766 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/11db1b855fa08a5ef7cdf2d40bd18ee33eba92ee",
        "buggy_code": "InetSocketAddress datanodeAddr = DFSUtil.getSocketAddress(datanode);",
        "fixed_code": "InetSocketAddress datanodeAddr = NetUtils.createSocketAddr(datanode);",
        "patch": "@@ -1127,7 +1127,7 @@ public int run(String[] argv) throws Exception {\n \n   private ClientDatanodeProtocol getDataNodeProxy(String datanode)\n       throws IOException {\n-    InetSocketAddress datanodeAddr = DFSUtil.getSocketAddress(datanode);\n+    InetSocketAddress datanodeAddr = NetUtils.createSocketAddr(datanode);\n     // Get the current configuration\n     Configuration conf = getConf();\n "
    },
    {
        "commit_id": "7dfe20d5a8a16120ac8ea58f4bc530dc9a8ee79d",
        "commit_message": "MAPREDUCE-3823. Fixed a bug in RM web-ui which broke sorting. Contributed by Jonathan Eagles.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1241685 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/7dfe20d5a8a16120ac8ea58f4bc530dc9a8ee79d",
        "buggy_code": "br().$title(startTime)._()._(finishTime)._().",
        "fixed_code": "br().$title(finishTime)._()._(finishTime)._().",
        "patch": "@@ -81,7 +81,7 @@ class AppsBlock extends HtmlBlock {\n           td().\n             br().$title(startTime)._()._(startTime)._().\n           td().\n-          br().$title(startTime)._()._(finishTime)._().\n+          br().$title(finishTime)._()._(finishTime)._().\n           td(appInfo.getState()).\n           td(appInfo.getFinalStatus()).\n           td()."
    },
    {
        "commit_id": "1de317a945ac99287aa5128b5eede7464b3b0ece",
        "commit_message": "MAPREDUCE-3803. Fix broken build of raid contrib due to HDFS-2864. Contributed by Ravi Prakash.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1240441 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/1de317a945ac99287aa5128b5eede7464b3b0ece",
        "buggy_code": "if (version != FSDataset.METADATA_VERSION) {",
        "fixed_code": "if (version != BlockMetadataHeader.VERSION) {",
        "patch": "@@ -108,7 +108,7 @@ public RaidBlockSender(ExtendedBlock block, long blockLength, long startOffset,\n        BlockMetadataHeader header = BlockMetadataHeader.readHeader(checksumIn);\n        short version = header.getVersion();\n \n-        if (version != FSDataset.METADATA_VERSION) {\n+        if (version != BlockMetadataHeader.VERSION) {\n           LOG.warn(\"Wrong version (\" + version + \") for metadata file for \"\n               + block + \" ignoring ...\");\n         }"
    },
    {
        "commit_id": "6ba0375b21c4ce07d2b6b592c4963f705c35222b",
        "commit_message": "MAPREDUCE-3744. Fix the yarn logs command line. Improve error messages for mapred job -logs. (Contributed by Jason Lowe)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1239433 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/6ba0375b21c4ce07d2b6b592c4963f705c35222b",
        "buggy_code": "logDumper.dumpAContainersLogs(logParams.getApplicationId(),",
        "fixed_code": "exitCode = logDumper.dumpAContainersLogs(logParams.getApplicationId(),",
        "patch": "@@ -345,7 +345,7 @@ public int run(String[] argv) throws Exception {\n         LogParams logParams = cluster.getLogParams(jobID, taskAttemptID);\n         LogDumper logDumper = new LogDumper();\n         logDumper.setConf(getConf());\n-        logDumper.dumpAContainersLogs(logParams.getApplicationId(),\n+        exitCode = logDumper.dumpAContainersLogs(logParams.getApplicationId(),\n             logParams.getContainerId(), logParams.getNodeId(),\n             logParams.getOwner());\n         } catch (IOException e) {"
    },
    {
        "commit_id": "641f79a325bad571b11b5700a42efb844eabc5af",
        "commit_message": "HDFS-2824. Fix failover when prior NN died just after creating an edit log segment. Contributed by Aaron T. Myers.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1238069 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/641f79a325bad571b11b5700a42efb844eabc5af",
        "buggy_code": "if (editsLoaded > 0) {",
        "fixed_code": "if (editsLoaded > 0 || LOG.isDebugEnabled()) {",
        "patch": "@@ -224,7 +224,7 @@ private void doTailEdits() throws IOException, InterruptedException {\n         editsLoaded = elie.getNumEditsLoaded();\n         throw elie;\n       } finally {\n-        if (editsLoaded > 0) {\n+        if (editsLoaded > 0 || LOG.isDebugEnabled()) {\n           LOG.info(String.format(\"Loaded %d edits starting from txid %d \",\n               editsLoaded, lastTxnId));\n         }"
    },
    {
        "commit_id": "3e76f00baa6a5edb87761d69bbb8320d245c0621",
        "commit_message": "Fix expected error text in assertion.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1230254 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/3e76f00baa6a5edb87761d69bbb8320d245c0621",
        "buggy_code": "\"No non-corrupt logs for txid \");",
        "fixed_code": "\"Gap in transactions. Expected to be able to read up until at least txid \");",
        "patch": "@@ -248,7 +248,7 @@ public void testRollback() throws Exception {\n       baseDirs = UpgradeUtilities.createNameNodeStorageDirs(nameNodeDirs, \"previous\");\n       deleteMatchingFiles(baseDirs, \"edits.*\");\n       startNameNodeShouldFail(StartupOption.ROLLBACK,\n-          \"No non-corrupt logs for txid \");\n+          \"Gap in transactions. Expected to be able to read up until at least txid \");\n       UpgradeUtilities.createEmptyDirs(nameNodeDirs);\n       \n       log(\"NameNode rollback with no image file\", numDirs);"
    },
    {
        "commit_id": "190dc1c91b0ae0f3f128cc6603e354a3ec83288a",
        "commit_message": "HDFS-2753. Fix standby getting stuck in safemode when blocks are written while SBN is down. Contributed by Hari Mankude and Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1229898 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/190dc1c91b0ae0f3f128cc6603e354a3ec83288a",
        "buggy_code": "if (namesystem.isInStartupSafeMode() && node.numBlocks() > 0) {",
        "fixed_code": "if (namesystem.isInStartupSafeMode() && !node.isFirstBlockReport()) {",
        "patch": "@@ -1361,7 +1361,7 @@ public void processReport(final DatanodeID nodeID, final String poolId,\n \n       // To minimize startup time, we discard any second (or later) block reports\n       // that we receive while still in startup phase.\n-      if (namesystem.isInStartupSafeMode() && node.numBlocks() > 0) {\n+      if (namesystem.isInStartupSafeMode() && !node.isFirstBlockReport()) {\n         NameNode.stateChangeLog.info(\"BLOCK* processReport: \"\n             + \"discarded non-initial block report from \" + nodeID.getName()\n             + \" because namenode still in startup phase\");"
    },
    {
        "commit_id": "526efb48a6d3a44a753ee9fcb6333eba046193ca",
        "commit_message": "HDFS-2762. Fix TestCheckpoint timing out on HA branch. Contributed by Uma Maheswara Rao G.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1229464 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/526efb48a6d3a44a753ee9fcb6333eba046193ca",
        "buggy_code": "NameNode.format(conf);",
        "fixed_code": "DFSTestUtil.formatNameNode(conf);",
        "patch": "@@ -632,7 +632,7 @@ private void createNameNodesAndSetConf(MiniDFSNNTopology nnTopology,\n         \n         nnCounterForFormat++;\n         if (formatThisOne) {\n-          NameNode.format(conf);\n+          DFSTestUtil.formatNameNode(conf);\n         }\n         prevNNDirs = FSNamesystem.getNamespaceDirs(conf);\n       }"
    },
    {
        "commit_id": "8feb26f116b8255853224a74aedb479b9d11b6ae",
        "commit_message": "HADOOP-7963. Fix ViewFS to catch a null canonical service-name and pass tests TestViewFileSystem* (Siddharth Seth via vinodkv)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1229379 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/8feb26f116b8255853224a74aedb479b9d11b6ae",
        "buggy_code": "if (seenServiceNames.contains(serviceName)) {",
        "fixed_code": "if (serviceName == null || seenServiceNames.contains(serviceName)) {",
        "patch": "@@ -514,7 +514,7 @@ public List<Token<?>> getDelegationTokens(String renewer,\n     for (int i = 0; i < mountPoints.size(); ++i) {\n       String serviceName =\n           mountPoints.get(i).target.targetFileSystem.getCanonicalServiceName();\n-      if (seenServiceNames.contains(serviceName)) {\n+      if (serviceName == null || seenServiceNames.contains(serviceName)) {\n         continue;\n       }\n       seenServiceNames.add(serviceName);"
    },
    {
        "commit_id": "9a07ba8945407cd8f63169faf9e0faa4311d38c7",
        "commit_message": "HDFS-2709. Appropriately handle error conditions in EditLogTailer. Contributed by Aaron T. Myers.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1228390 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/9a07ba8945407cd8f63169faf9e0faa4311d38c7",
        "buggy_code": "int numEditsThisLog = loader.loadFSEdits(new EditLogFileInputStream(editFile),",
        "fixed_code": "long numEditsThisLog = loader.loadFSEdits(new EditLogFileInputStream(editFile),",
        "patch": "@@ -237,7 +237,7 @@ private long verifyEditLogs(FSNamesystem namesystem, FSImage fsimage,\n         \n       System.out.println(\"Verifying file: \" + editFile);\n       FSEditLogLoader loader = new FSEditLogLoader(namesystem);\n-      int numEditsThisLog = loader.loadFSEdits(new EditLogFileInputStream(editFile), \n+      long numEditsThisLog = loader.loadFSEdits(new EditLogFileInputStream(editFile), \n           startTxId);\n       \n       System.out.println(\"Number of edits: \" + numEditsThisLog);"
    },
    {
        "commit_id": "9a07ba8945407cd8f63169faf9e0faa4311d38c7",
        "commit_message": "HDFS-2709. Appropriately handle error conditions in EditLogTailer. Contributed by Aaron T. Myers.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1228390 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/9a07ba8945407cd8f63169faf9e0faa4311d38c7",
        "buggy_code": "int numEdits = loader.loadFSEdits(",
        "fixed_code": "long numEdits = loader.loadFSEdits(",
        "patch": "@@ -141,7 +141,7 @@ public void testEditLog() throws IOException {\n         System.out.println(\"Verifying file: \" + editFile);\n         \n         FSEditLogLoader loader = new FSEditLogLoader(namesystem);        \n-        int numEdits = loader.loadFSEdits(\n+        long numEdits = loader.loadFSEdits(\n             new EditLogFileInputStream(editFile), 1);\n         assertEquals(\"Verification for \" + editFile, expectedTransactions, numEdits);\n       }"
    },
    {
        "commit_id": "a2bcb867e17b76eb3ad1136bac92dac8069ebf6d",
        "commit_message": "MAPREDUCE-3615. Fix some ant test failures. (Contributed by Thomas Graves)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1227741 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/a2bcb867e17b76eb3ad1136bac92dac8069ebf6d",
        "buggy_code": "this(id, taskType, status, finishTime, hostname, -1, null, error, null);",
        "fixed_code": "this(id, taskType, status, finishTime, hostname, -1, \"\", error, null);",
        "patch": "@@ -103,7 +103,7 @@ public class TaskAttemptUnsuccessfulCompletionEvent implements HistoryEvent {\n        (TaskAttemptID id, TaskType taskType,\n         String status, long finishTime, \n         String hostname, String error) {\n-    this(id, taskType, status, finishTime, hostname, -1, null, error, null);\n+    this(id, taskType, status, finishTime, hostname, -1, \"\", error, null);\n   }\n \n   TaskAttemptUnsuccessfulCompletionEvent() {}"
    },
    {
        "commit_id": "a2bcb867e17b76eb3ad1136bac92dac8069ebf6d",
        "commit_message": "MAPREDUCE-3615. Fix some ant test failures. (Contributed by Thomas Graves)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1227741 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/a2bcb867e17b76eb3ad1136bac92dac8069ebf6d",
        "buggy_code": "taskTrackerHostName, -1, null, diagInfo,",
        "fixed_code": "taskTrackerHostName, -1, \"\", diagInfo,",
        "patch": "@@ -3210,7 +3210,7 @@ private void failedTask(TaskInProgress tip, TaskAttemptID taskid,\n             (taskid, \n              taskType, taskStatus.getRunState().toString(),\n              finishTime, \n-             taskTrackerHostName, -1, null, diagInfo,\n+             taskTrackerHostName, -1, \"\", diagInfo,\n              splits.burst());\n     jobHistory.logEvent(tue, taskid.getJobID());\n         "
    },
    {
        "commit_id": "41737432c07fb1e1d208b5125fd0fd5205c588cd",
        "commit_message": "HDFS-2726. Fix a logging issue under DFSClient's createBlockOutputStream method (harsh)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1225456 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/41737432c07fb1e1d208b5125fd0fd5205c588cd",
        "buggy_code": "DFSClient.LOG.info(\"Exception in createBlockOutputStream \" + ie);",
        "fixed_code": "DFSClient.LOG.info(\"Exception in createBlockOutputStream\", ie);",
        "patch": "@@ -1056,7 +1056,7 @@ private boolean createBlockOutputStream(DatanodeInfo[] nodes, long newGS,\n \n       } catch (IOException ie) {\n \n-        DFSClient.LOG.info(\"Exception in createBlockOutputStream \" + ie);\n+        DFSClient.LOG.info(\"Exception in createBlockOutputStream\", ie);\n \n         // find the datanode that matches\n         if (firstBadLink.length() != 0) {"
    },
    {
        "commit_id": "371f4228e86f5ebffb3d8647fb30b8bdc2b777c4",
        "commit_message": "HDFS-2684. Fix up some failing unit tests on HA branch. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1215241 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/371f4228e86f5ebffb3d8647fb30b8bdc2b777c4",
        "buggy_code": "private static final int EXPECTED_TXID = 33;",
        "fixed_code": "private static final int EXPECTED_TXID = 49;",
        "patch": "@@ -52,7 +52,7 @@\n */\n public class TestDFSUpgrade {\n  \n-  private static final int EXPECTED_TXID = 33;\n+  private static final int EXPECTED_TXID = 49;\n   private static final Log LOG = LogFactory.getLog(TestDFSUpgrade.class.getName());\n   private Configuration conf;\n   private int testCounter = 0;"
    },
    {
        "commit_id": "371f4228e86f5ebffb3d8647fb30b8bdc2b777c4",
        "commit_message": "HDFS-2684. Fix up some failing unit tests on HA branch. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1215241 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/371f4228e86f5ebffb3d8647fb30b8bdc2b777c4",
        "buggy_code": "newGS = newBlock.getGenerationStamp() + 1;",
        "fixed_code": "newGS = firstBlock.getGenerationStamp() + 1;",
        "patch": "@@ -302,7 +302,7 @@ private void testWrite(ExtendedBlock block, BlockConstructionStage stage, long n\n         testWrite(firstBlock, BlockConstructionStage.PIPELINE_SETUP_CREATE, 0L,\n             \"Cannot create a RBW block\", true);\n         // test PIPELINE_SETUP_APPEND on an existing block\n-        newGS = newBlock.getGenerationStamp() + 1;\n+        newGS = firstBlock.getGenerationStamp() + 1;\n         testWrite(firstBlock, BlockConstructionStage.PIPELINE_SETUP_APPEND,\n             newGS, \"Cannot append to a RBW replica\", true);\n         // test PIPELINE_SETUP_APPEND on an existing block"
    },
    {
        "commit_id": "371f4228e86f5ebffb3d8647fb30b8bdc2b777c4",
        "commit_message": "HDFS-2684. Fix up some failing unit tests on HA branch. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1215241 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/371f4228e86f5ebffb3d8647fb30b8bdc2b777c4",
        "buggy_code": "assertEquals(null, cmds);",
        "fixed_code": "assertEquals(0, cmds.length);",
        "patch": "@@ -110,7 +110,7 @@ public void testHeartbeat() throws Exception {\n \n           cmds = NameNodeAdapter.sendHeartBeat(nodeReg, dd, namesystem)\n               .getCommands();\n-          assertEquals(null, cmds);\n+          assertEquals(0, cmds.length);\n         }\n       } finally {\n         namesystem.writeUnlock();"
    },
    {
        "commit_id": "cdb9f01ad4e6084ddf83e40eb3ec18a89fbbae42",
        "commit_message": "HDFS-2667. Fix transition from active to standby. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1215037 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/cdb9f01ad4e6084ddf83e40eb3ec18a89fbbae42",
        "buggy_code": "throw new IOException(\"Asked for fromTxId \" + fromTxId",
        "fixed_code": "throw new IllegalStateException(\"Asked for fromTxId \" + fromTxId",
        "patch": "@@ -304,7 +304,7 @@ private List<EditLogFile> getLogFiles(long fromTxId) throws IOException {\n     for (EditLogFile elf : allLogFiles) {\n       if (fromTxId > elf.getFirstTxId()\n           && fromTxId <= elf.getLastTxId()) {\n-        throw new IOException(\"Asked for fromTxId \" + fromTxId\n+        throw new IllegalStateException(\"Asked for fromTxId \" + fromTxId\n             + \" which is in middle of file \" + elf.file);\n       }\n       if (fromTxId <= elf.getFirstTxId()) {"
    },
    {
        "commit_id": "cdb9f01ad4e6084ddf83e40eb3ec18a89fbbae42",
        "commit_message": "HDFS-2667. Fix transition from active to standby. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1215037 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/cdb9f01ad4e6084ddf83e40eb3ec18a89fbbae42",
        "buggy_code": "getHaServiceClient(nnIndex).transitionToActive();",
        "fixed_code": "getHaServiceClient(nnIndex).transitionToStandby();",
        "patch": "@@ -1553,7 +1553,7 @@ public void transitionToActive(int nnIndex) throws IOException,\n   \n   public void transitionToStandby(int nnIndex) throws IOException,\n       ServiceFailedException {\n-    getHaServiceClient(nnIndex).transitionToActive();\n+    getHaServiceClient(nnIndex).transitionToStandby();\n   }\n \n   /** Wait until the given namenode gets registration from all the datanodes */"
    },
    {
        "commit_id": "739f8871f2301970f96c1ec0ab9586bd0393cb3a",
        "commit_message": "MAPREDUCE-3541. Fix broken TestJobQueueClient test. (Ravi Prakash via mahadev)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1214421 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/739f8871f2301970f96c1ec0ab9586bd0393cb3a",
        "buggy_code": "queueClient.printJobQueueInfo(parent, writer, \"\");",
        "fixed_code": "queueClient.printJobQueueInfo(parent, writer);",
        "patch": "@@ -45,7 +45,7 @@ public void testPrintJobQueueInfo() throws IOException {\n \n     ByteArrayOutputStream bbos = new ByteArrayOutputStream();\n     PrintWriter writer = new PrintWriter(bbos);\n-    queueClient.printJobQueueInfo(parent, writer, \"\");\n+    queueClient.printJobQueueInfo(parent, writer);\n \n     Assert.assertTrue(\"printJobQueueInfo did not print grandchild's name\",\n       bbos.toString().contains(\"GrandChildQueue\"));"
    },
    {
        "commit_id": "186df142cc19c2969c1f8b56df0d8f75e3db66b1",
        "commit_message": "    HADOOP-7913 Fix bug in ProtoBufRpcEngine  (sanjay)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1213619 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/186df142cc19c2969c1f8b56df0d8f75e3db66b1",
        "buggy_code": "super(bindAddress, port, RpcRequestWritable.class, numHandlers,",
        "fixed_code": "super(bindAddress, port, null, numHandlers,",
        "patch": "@@ -325,7 +325,7 @@ public Server(Class<?> protocolClass, Object protocolImpl,\n         int numReaders, int queueSizePerHandler, boolean verbose,\n         SecretManager<? extends TokenIdentifier> secretManager)\n         throws IOException {\n-      super(bindAddress, port, RpcRequestWritable.class, numHandlers,\n+      super(bindAddress, port, null, numHandlers,\n           numReaders, queueSizePerHandler, conf, classNameBase(protocolImpl\n               .getClass().getName()), secretManager);\n       this.verbose = verbose;  "
    },
    {
        "commit_id": "f611e1d1b116613a8fce0abc400e54e155d295e9",
        "commit_message": "MAPREDUCE-3537. Fix race condition in DefaultContainerExecutor which led to container localization occuring in wrong directories.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1213575 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/f611e1d1b116613a8fce0abc400e54e155d295e9",
        "buggy_code": "public void startLocalizer(Path nmPrivateContainerTokensPath,",
        "fixed_code": "public synchronized void startLocalizer(Path nmPrivateContainerTokensPath,",
        "patch": "@@ -75,7 +75,7 @@ public void init() throws IOException {\n   }\n   \n   @Override\n-  public void startLocalizer(Path nmPrivateContainerTokensPath,\n+  public synchronized void startLocalizer(Path nmPrivateContainerTokensPath,\n       InetSocketAddress nmAddr, String user, String appId, String locId,\n       List<String> localDirs, List<String> logDirs)\n       throws IOException, InterruptedException {"
    },
    {
        "commit_id": "e948247715ba001b00eafc5f801fa926c409ea5a",
        "commit_message": "MAPREDUCE-3485. DISKS_FAILED -101 error code should be defined in same location as ABORTED_CONTAINER_EXIT_STATUS. (Ravi Gummadi via mahadev)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1210192 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/e948247715ba001b00eafc5f801fa926c409ea5a",
        "buggy_code": "ret = ExitCode.DISKS_FAILED.getExitCode();",
        "fixed_code": "ret = YarnConfiguration.DISKS_FAILED;",
        "patch": "@@ -181,7 +181,7 @@ public Integer call() {\n       List<String> logDirs = dirsHandler.getLogDirs();\n \n       if (!dirsHandler.areDisksHealthy()) {\n-        ret = ExitCode.DISKS_FAILED.getExitCode();\n+        ret = YarnConfiguration.DISKS_FAILED;\n         throw new IOException(\"Most of the disks failed. \"\n             + dirsHandler.getDisksHealthReport());\n       }"
    },
    {
        "commit_id": "1972a76e5a4483b2da431cc7532207c2e6274c6c",
        "commit_message": "MAPREDUCE-3458. Fix findbugs warnings in hadoop-examples. (Devaraj K via mahadev)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1210190 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/1972a76e5a4483b2da431cc7532207c2e6274c6c",
        "buggy_code": "return rotations;",
        "fixed_code": "return rotations.clone();",
        "patch": "@@ -69,7 +69,7 @@ public String getName() {\n     }\n     \n     public int[] getRotations() {\n-      return rotations;\n+      return rotations.clone();\n     }\n     \n     public boolean getFlippable() {"
    },
    {
        "commit_id": "1972a76e5a4483b2da431cc7532207c2e6274c6c",
        "commit_message": "MAPREDUCE-3458. Fix findbugs warnings in hadoop-examples. (Devaraj K via mahadev)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1210190 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/1972a76e5a4483b2da431cc7532207c2e6274c6c",
        "buggy_code": "public static String NUM_ROWS = \"mapreduce.terasort.num-rows\";",
        "fixed_code": "public static final String NUM_ROWS = \"mapreduce.terasort.num-rows\";",
        "patch": "@@ -70,7 +70,7 @@ public class TeraGen extends Configured implements Tool {\n \n   public static enum Counters {CHECKSUM}\n \n-  public static String NUM_ROWS = \"mapreduce.terasort.num-rows\";\n+  public static final String NUM_ROWS = \"mapreduce.terasort.num-rows\";\n   /**\n    * An input format that assigns ranges of longs to each mapper.\n    */"
    },
    {
        "commit_id": "1b8511ef89613f6c9495d1bac177285b598596e8",
        "commit_message": "MAPREDUCE-3281. Fixed a bug in TestLinuxContainerExecutorWithMocks. (vinodkv) \n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1189818 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/1b8511ef89613f6c9495d1bac177285b598596e8",
        "buggy_code": "workDir.toString(), \"/bin/echo\", \"/dev/null\", pidFile),",
        "fixed_code": "workDir.toString(), \"/bin/echo\", \"/dev/null\", pidFile.toString()),",
        "patch": "@@ -117,7 +117,7 @@ public void testContainerLaunch() throws IOException {\n         appSubmitter, appId, workDir);\n     assertEquals(0, ret);\n     assertEquals(Arrays.asList(appSubmitter, cmd, appId, containerId,\n-        workDir.toString(), \"/bin/echo\", \"/dev/null\", pidFile),\n+        workDir.toString(), \"/bin/echo\", \"/dev/null\", pidFile.toString()),\n         readMockParams());\n   }\n "
    },
    {
        "commit_id": "237154982bd5853c6a374cb265520e0602adc52f",
        "commit_message": "MAPREDUCE-3205. Fix memory specifications to be physical rather than virtual, allowing for a ratio between the two to be configurable. Contributed by Todd Lipcon. \n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1189542 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/237154982bd5853c6a374cb265520e0602adc52f",
        "buggy_code": "new ContainerImpl(this.dispatcher, launchContext, credentials, metrics);",
        "fixed_code": "new ContainerImpl(getConfig(), this.dispatcher, launchContext, credentials, metrics);",
        "patch": "@@ -275,7 +275,7 @@ public StartContainerResponse startContainer(StartContainerRequest request)\n     // //////////// End of parsing credentials\n \n     Container container =\n-        new ContainerImpl(this.dispatcher, launchContext, credentials, metrics);\n+        new ContainerImpl(getConfig(), this.dispatcher, launchContext, credentials, metrics);\n     ContainerId containerID = launchContext.getContainerId();\n     ApplicationId applicationID = \n         containerID.getApplicationAttemptId().getApplicationId();"
    },
    {
        "commit_id": "237154982bd5853c6a374cb265520e0602adc52f",
        "commit_message": "MAPREDUCE-3205. Fix memory specifications to be physical rather than virtual, allowing for a ratio between the two to be configurable. Contributed by Todd Lipcon. \n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1189542 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/237154982bd5853c6a374cb265520e0602adc52f",
        "buggy_code": "new ContainerImpl(dispatcher, launchContext, null, metrics) {",
        "fixed_code": "new ContainerImpl(conf, dispatcher, launchContext, null, metrics) {",
        "patch": "@@ -107,7 +107,7 @@ public long getPmemAllocatedForContainers() {\n       launchContext.setContainerId(containerId);\n       launchContext.setUser(user);\n       Container container =\n-          new ContainerImpl(dispatcher, launchContext, null, metrics) {\n+          new ContainerImpl(conf, dispatcher, launchContext, null, metrics) {\n         @Override\n         public ContainerState getContainerState() {\n           return ContainerState.RUNNING;"
    },
    {
        "commit_id": "237154982bd5853c6a374cb265520e0602adc52f",
        "commit_message": "MAPREDUCE-3205. Fix memory specifications to be physical rather than virtual, allowing for a ratio between the two to be configurable. Contributed by Todd Lipcon. \n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1189542 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/237154982bd5853c6a374cb265520e0602adc52f",
        "buggy_code": "getConfig().setInt(YarnConfiguration.NM_VMEM_GB, 4); // By default AM + 2 containers",
        "fixed_code": "getConfig().setInt(YarnConfiguration.NM_PMEM_MB, 4*1024); // By default AM + 2 containers",
        "patch": "@@ -168,7 +168,7 @@ public synchronized void start() {\n         getConfig().set(YarnConfiguration.NM_LOG_DIRS, logDir.getAbsolutePath());\n         getConfig().set(YarnConfiguration.NM_REMOTE_APP_LOG_DIR,\n             remoteLogDir.getAbsolutePath());\n-        getConfig().setInt(YarnConfiguration.NM_VMEM_GB, 4); // By default AM + 2 containers\n+        getConfig().setInt(YarnConfiguration.NM_PMEM_MB, 4*1024); // By default AM + 2 containers\n         nodeManager = new NodeManager() {\n \n           @Override"
    },
    {
        "commit_id": "29c6c3ed328965a73fe7b68eb29cb30794beef38",
        "commit_message": "MAPREDUCE-2977. Fix ResourceManager to renew HDFS delegation tokens for applications. \n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1189012 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/29c6c3ed328965a73fe7b68eb29cb30794beef38",
        "buggy_code": "null);",
        "fixed_code": "null, null);",
        "patch": "@@ -96,7 +96,7 @@ public void setUp() {\n     dispatcher.register(RMNodeEventType.class,\n         new InlineDispatcher.EmptyEventHandler());\n     RMContext context = new RMContextImpl(new MemStore(), dispatcher, null,\n-        null);\n+        null, null);\n     NMLivelinessMonitor nmLivelinessMonitor = new TestNmLivelinessMonitor(\n         dispatcher);\n     nmLivelinessMonitor.init(conf);"
    },
    {
        "commit_id": "29c6c3ed328965a73fe7b68eb29cb30794beef38",
        "commit_message": "MAPREDUCE-2977. Fix ResourceManager to renew HDFS delegation tokens for applications. \n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1189012 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/29c6c3ed328965a73fe7b68eb29cb30794beef38",
        "buggy_code": "containerAllocationExpirer, amLivelinessMonitor);",
        "fixed_code": "containerAllocationExpirer, amLivelinessMonitor, null);",
        "patch": "@@ -119,7 +119,7 @@ public void setUp() throws Exception {\n         mock(ContainerAllocationExpirer.class);\n     AMLivelinessMonitor amLivelinessMonitor = mock(AMLivelinessMonitor.class);\n     this.rmContext = new RMContextImpl(new MemStore(), rmDispatcher,\n-        containerAllocationExpirer, amLivelinessMonitor);\n+        containerAllocationExpirer, amLivelinessMonitor, null);\n \n     rmDispatcher.register(RMAppAttemptEventType.class,\n         new TestApplicationAttemptEventDispatcher(this.rmContext));"
    },
    {
        "commit_id": "29c6c3ed328965a73fe7b68eb29cb30794beef38",
        "commit_message": "MAPREDUCE-2977. Fix ResourceManager to renew HDFS delegation tokens for applications. \n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1189012 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/29c6c3ed328965a73fe7b68eb29cb30794beef38",
        "buggy_code": "containerAllocationExpirer, amLivelinessMonitor);",
        "fixed_code": "containerAllocationExpirer, amLivelinessMonitor, null);",
        "patch": "@@ -138,7 +138,7 @@ public void setUp() throws Exception {\n         mock(ContainerAllocationExpirer.class);\n     AMLivelinessMonitor amLivelinessMonitor = mock(AMLivelinessMonitor.class);\n     rmContext = new RMContextImpl(new MemStore(), rmDispatcher,\n-      containerAllocationExpirer, amLivelinessMonitor);\n+      containerAllocationExpirer, amLivelinessMonitor, null);\n     \n     scheduler = mock(YarnScheduler.class);\n     masterService = mock(ApplicationMasterService.class);"
    },
    {
        "commit_id": "29c6c3ed328965a73fe7b68eb29cb30794beef38",
        "commit_message": "MAPREDUCE-2977. Fix ResourceManager to renew HDFS delegation tokens for applications. \n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1189012 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/29c6c3ed328965a73fe7b68eb29cb30794beef38",
        "buggy_code": "new RMContextImpl(null, nullDispatcher, cae, null);",
        "fixed_code": "new RMContextImpl(null, nullDispatcher, cae, null, null);",
        "patch": "@@ -75,7 +75,7 @@ public EventHandler getEventHandler() {\n         new ContainerAllocationExpirer(nullDispatcher);\n     \n     RMContext rmContext = \n-        new RMContextImpl(null, nullDispatcher, cae, null);\n+        new RMContextImpl(null, nullDispatcher, cae, null, null);\n     \n     return rmContext;\n   }"
    },
    {
        "commit_id": "29c6c3ed328965a73fe7b68eb29cb30794beef38",
        "commit_message": "MAPREDUCE-2977. Fix ResourceManager to renew HDFS delegation tokens for applications. \n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1189012 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/29c6c3ed328965a73fe7b68eb29cb30794beef38",
        "buggy_code": "return new RMContextImpl(new MemStore(), null, null, null) {",
        "fixed_code": "return new RMContextImpl(new MemStore(), null, null, null, null) {",
        "patch": "@@ -120,7 +120,7 @@ public static RMContext mockRMContext(int numApps, int racks, int numNodes,\n     for (RMNode node : nodes) {\n       nodesMap.put(node.getNodeID(), node);\n     }\n-   return new RMContextImpl(new MemStore(), null, null, null) {\n+   return new RMContextImpl(new MemStore(), null, null, null, null) {\n       @Override\n       public ConcurrentMap<ApplicationId, RMApp> getRMApps() {\n         return applicationsMaps;"
    },
    {
        "commit_id": "5795fcfd9904431ec075fdce7ab8559ff50eccd2",
        "commit_message": "MAPREDUCE-3058. Fixed MR YarnChild to report failure when task throws an error and thus prevent a hanging task and job. (vinodkv)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1187654 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/5795fcfd9904431ec075fdce7ab8559ff50eccd2",
        "buggy_code": "umbilical.reportDiagnosticInfo(taskid, baos.toString());",
        "fixed_code": "umbilical.fatalError(taskid, baos.toString());",
        "patch": "@@ -177,7 +177,7 @@ public Object run() throws Exception {\n       ByteArrayOutputStream baos = new ByteArrayOutputStream();\n       exception.printStackTrace(new PrintStream(baos));\n       if (taskid != null) {\n-        umbilical.reportDiagnosticInfo(taskid, baos.toString());\n+        umbilical.fatalError(taskid, baos.toString());\n       }\n     } catch (Throwable throwable) {\n       LOG.fatal(\"Error running child : \""
    },
    {
        "commit_id": "4186121c08cb3d86f775d333c637459a4fb19d1b",
        "commit_message": "MAPREDUCE-3239. Use new createSocketAddr API in MRv2 to give better error messages on misconfig (Todd Lipcon via mahadev)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1187556 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/4186121c08cb3d86f775d333c637459a4fb19d1b",
        "buggy_code": "conf.set(MRConfig.MASTER_ADDRESS, \"local\");",
        "fixed_code": "conf.set(MRConfig.MASTER_ADDRESS, \"local:invalid\");",
        "patch": "@@ -42,7 +42,7 @@ public void testGetMasterAddress() {\n     \n     // Trying invalid master address for classic \n     conf.set(MRConfig.FRAMEWORK_NAME, MRConfig.CLASSIC_FRAMEWORK_NAME);\n-    conf.set(MRConfig.MASTER_ADDRESS, \"local\");\n+    conf.set(MRConfig.MASTER_ADDRESS, \"local:invalid\");\n \n     // should throw an exception for invalid value\n     try {"
    },
    {
        "commit_id": "89e7ca6a5231a2c383e6256f04517ea1a5280352",
        "commit_message": "MAPREDUCE-3070. Fix NodeManager to use ephemeral ports by default. Contributed by Devaraj K.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1187496 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/89e7ca6a5231a2c383e6256f04517ea1a5280352",
        "buggy_code": "public static final String DEFAULT_NM_ADDRESS = \"0.0.0.0:45454\";",
        "fixed_code": "public static final String DEFAULT_NM_ADDRESS = \"0.0.0.0:0\";",
        "patch": "@@ -218,7 +218,7 @@ public class YarnConfiguration extends Configuration {\n   \n   /** address of node manager IPC.*/\n   public static final String NM_ADDRESS = NM_PREFIX + \"address\";\n-  public static final String DEFAULT_NM_ADDRESS = \"0.0.0.0:45454\";\n+  public static final String DEFAULT_NM_ADDRESS = \"0.0.0.0:0\";\n   \n   /** who will execute(launch) the containers.*/\n   public static final String NM_CONTAINER_EXECUTOR = "
    },
    {
        "commit_id": "02d5fa3e6265025912223e07d1bb5aac9eac0dc9",
        "commit_message": "MAPREDUCE-3226. Fix shutdown of fetcher threads. Contributed by Vinod K V.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1187116 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/02d5fa3e6265025912223e07d1bb5aac9eac0dc9",
        "buggy_code": "while (true) {",
        "fixed_code": "while (true && !Thread.currentThread().isInterrupted()) {",
        "patch": "@@ -135,7 +135,7 @@ public Fetcher(JobConf job, TaskAttemptID reduceId,\n   \n   public void run() {\n     try {\n-      while (true) {\n+      while (true && !Thread.currentThread().isInterrupted()) {\n         MapHost host = null;\n         try {\n           // If merge is on, block"
    },
    {
        "commit_id": "50cb2771e924d2d6d9d04e588cb0a94aefb25b70",
        "commit_message": "HDFS-2439. Fix NullPointerException in webhdfs when opening a non-existing file or creating a file without specifying the replication parameter.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1183554 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/50cb2771e924d2d6d9d04e588cb0a94aefb25b70",
        "buggy_code": "replication.getValue(), blockSize.getValue(conf), null, b), null);",
        "fixed_code": "replication.getValue(conf), blockSize.getValue(conf), null, b), null);",
        "patch": "@@ -131,7 +131,7 @@ public Response run() throws IOException, URISyntaxException {\n           fullpath, permission.getFsPermission(), \n           overwrite.getValue() ? EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE)\n               : EnumSet.of(CreateFlag.CREATE),\n-          replication.getValue(), blockSize.getValue(conf), null, b), null);\n+          replication.getValue(conf), blockSize.getValue(conf), null, b), null);\n       try {\n         IOUtils.copyBytes(in, out, b);\n       } finally {"
    },
    {
        "commit_id": "002dd6968b89ded6a77858ccb50c9b2df074c226",
        "commit_message": "MAPREDUCE-2764. Fix renewal of dfs delegation tokens. Contributed by Owen.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1183187 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/002dd6968b89ded6a77858ccb50c9b2df074c226",
        "buggy_code": "@SuppressWarnings(\"unchecked\")",
        "fixed_code": "@SuppressWarnings({ \"unchecked\", \"deprecation\" })",
        "patch": "@@ -105,7 +105,7 @@ public void testFcResolveAfs() throws IOException, InterruptedException {\n    * @throws IOException\n    * @throws InterruptedException\n    */\n-  @SuppressWarnings(\"unchecked\")\n+  @SuppressWarnings({ \"unchecked\", \"deprecation\" })\n   @Test\n   public void testFcDelegationToken() throws UnsupportedFileSystemException,\n       IOException, InterruptedException {"
    },
    {
        "commit_id": "002dd6968b89ded6a77858ccb50c9b2df074c226",
        "commit_message": "MAPREDUCE-2764. Fix renewal of dfs delegation tokens. Contributed by Owen.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1183187 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/002dd6968b89ded6a77858ccb50c9b2df074c226",
        "buggy_code": "static final Text MAPREDUCE_DELEGATION_KIND =",
        "fixed_code": "public static final Text MAPREDUCE_DELEGATION_KIND =",
        "patch": "@@ -30,7 +30,7 @@\n @InterfaceStability.Unstable\n public class DelegationTokenIdentifier \n     extends AbstractDelegationTokenIdentifier {\n-  static final Text MAPREDUCE_DELEGATION_KIND = \n+  public static final Text MAPREDUCE_DELEGATION_KIND = \n     new Text(\"MAPREDUCE_DELEGATION_TOKEN\");\n \n   /**"
    },
    {
        "commit_id": "42e93829e5310f3cbd905384cd0529f8fffa887f",
        "commit_message": "MAPREDUCE-3158. Fix test failures in MRv1 due to default framework being set to yarn. Contributed by Hitesh Shah. \n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1181310 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/42e93829e5310f3cbd905384cd0529f8fffa887f",
        "buggy_code": "if (framework != null && !framework.equals(\"local\")) {",
        "fixed_code": "if (framework != null && !framework.equals(MRConfig.LOCAL_FRAMEWORK_NAME)) {",
        "patch": "@@ -34,7 +34,7 @@ public class LocalClientProtocolProvider extends ClientProtocolProvider {\n   @Override\n   public ClientProtocol create(Configuration conf) throws IOException {\n     String framework = conf.get(MRConfig.FRAMEWORK_NAME);\n-    if (framework != null && !framework.equals(\"local\")) {\n+    if (framework != null && !framework.equals(MRConfig.LOCAL_FRAMEWORK_NAME)) {\n       return null;\n     }\n     String tracker = conf.get(JTConfig.JT_IPC_ADDRESS, \"local\");"
    },
    {
        "commit_id": "42e93829e5310f3cbd905384cd0529f8fffa887f",
        "commit_message": "MAPREDUCE-3158. Fix test failures in MRv1 due to default framework being set to yarn. Contributed by Hitesh Shah. \n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1181310 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/42e93829e5310f3cbd905384cd0529f8fffa887f",
        "buggy_code": "conf.set(MRConfig.FRAMEWORK_NAME, \"local\");",
        "fixed_code": "conf.set(MRConfig.FRAMEWORK_NAME, MRConfig.LOCAL_FRAMEWORK_NAME);",
        "patch": "@@ -43,7 +43,7 @@ public void testClusterWithLocalClientProvider() throws Exception {\n     }\r\n \r\n     try {\r\n-      conf.set(MRConfig.FRAMEWORK_NAME, \"local\");\r\n+      conf.set(MRConfig.FRAMEWORK_NAME, MRConfig.LOCAL_FRAMEWORK_NAME);\r\n       conf.set(JTConfig.JT_IPC_ADDRESS, \"127.0.0.1:0\");\r\n \r\n       new Cluster(conf);\r"
    },
    {
        "commit_id": "f48280ac56108ccb1c2903012542e2ae982647da",
        "commit_message": "HDFS-2298. Fix TestDfsOverAvroRpc by changing ClientProtocol to not include multiple methods of the same name.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1177757 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/f48280ac56108ccb1c2903012542e2ae982647da",
        "buggy_code": "np.rename(fullpath, dstPath.getValue(),",
        "fixed_code": "np.rename2(fullpath, dstPath.getValue(),",
        "patch": "@@ -257,7 +257,7 @@ public Response run() throws IOException, URISyntaxException {\n         final String js = JsonUtil.toJsonString(PutOpParam.Op.RENAME, b);\n         return Response.ok(js).type(MediaType.APPLICATION_JSON).build();\n       } else {\n-        np.rename(fullpath, dstPath.getValue(),\n+        np.rename2(fullpath, dstPath.getValue(),\n             s.toArray(new Options.Rename[s.size()]));\n         return Response.ok().type(MediaType.APPLICATION_JSON).build();\n       }"
    },
    {
        "commit_id": "87b969c83541c6719abcc1dabc38dc41704876ee",
        "commit_message": "MAPREDUCE-2999. Fix YARN webapp framework to properly filter servlet paths. Contributed by Thomas Graves.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1176469 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/87b969c83541c6719abcc1dabc38dc41704876ee",
        "buggy_code": "this.httpAddress = hostName + \":\" + httpPort;;",
        "fixed_code": "this.httpAddress = hostName + \":\" + httpPort;",
        "patch": "@@ -144,7 +144,7 @@ public RMNodeImpl(NodeId nodeId, RMContext context, String hostName,\n     this.httpPort = httpPort;\n     this.totalCapability = capability; \n     this.nodeAddress = hostName + \":\" + cmPort;\n-    this.httpAddress = hostName + \":\" + httpPort;;\n+    this.httpAddress = hostName + \":\" + httpPort;\n     this.node = node;\n     this.nodeHealthStatus.setIsNodeHealthy(true);\n     this.nodeHealthStatus.setHealthReport(\"Healthy\");"
    },
    {
        "commit_id": "1d067c6e2b14e08943a46129f4ed521890d3ca22",
        "commit_message": "MAPREDUCE-2998. Fixed a bug in TaskAttemptImpl which caused it to fork bin/mapred too many times. Contributed by Vinod K V. \n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1173456 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/1d067c6e2b14e08943a46129f4ed521890d3ca22",
        "buggy_code": "private final Object classpathLock = new Object();",
        "fixed_code": "private static final Object classpathLock = new Object();",
        "patch": "@@ -153,7 +153,7 @@ public abstract class TaskAttemptImpl implements\n   private Token<JobTokenIdentifier> jobToken;\n   private static AtomicBoolean initialClasspathFlag = new AtomicBoolean();\n   private static String initialClasspath = null;\n-  private final Object classpathLock = new Object();\n+  private static final Object classpathLock = new Object();\n   private long launchTime;\n   private long finishTime;\n   private WrappedProgressSplitsBlock progressSplitBlock;"
    },
    {
        "commit_id": "482d840bcf3e8332e513bcea61f6d4ea7ee8579d",
        "commit_message": "MAPREDUCE-3004. Fix ReduceTask to not assume 'local' mode in YARN. Contributed by Hitesh Shah.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1172893 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/482d840bcf3e8332e513bcea61f6d4ea7ee8579d",
        "buggy_code": "if (\"yarn\".equals(conf.get(MRConfig.FRAMEWORK_NAME))) {",
        "fixed_code": "if (MRConfig.YARN_FRAMEWORK_NAME.equals(conf.get(MRConfig.FRAMEWORK_NAME))) {",
        "patch": "@@ -31,7 +31,7 @@ public class YarnClientProtocolProvider extends ClientProtocolProvider {\n \n   @Override\n   public ClientProtocol create(Configuration conf) throws IOException {\n-    if (\"yarn\".equals(conf.get(MRConfig.FRAMEWORK_NAME))) {\n+    if (MRConfig.YARN_FRAMEWORK_NAME.equals(conf.get(MRConfig.FRAMEWORK_NAME))) {\n       return new YARNRunner(new YarnConfiguration(conf));\n     }\n     return null;"
    },
    {
        "commit_id": "482d840bcf3e8332e513bcea61f6d4ea7ee8579d",
        "commit_message": "MAPREDUCE-3004. Fix ReduceTask to not assume 'local' mode in YARN. Contributed by Hitesh Shah.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1172893 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/482d840bcf3e8332e513bcea61f6d4ea7ee8579d",
        "buggy_code": "conf.set(MRConfig.FRAMEWORK_NAME, \"yarn\");",
        "fixed_code": "conf.set(MRConfig.FRAMEWORK_NAME, MRConfig.YARN_FRAMEWORK_NAME);",
        "patch": "@@ -119,7 +119,7 @@ public class TestClientRedirect {\n   public void testRedirect() throws Exception {\n     \n     Configuration conf = new YarnConfiguration();\n-    conf.set(MRConfig.FRAMEWORK_NAME, \"yarn\");\n+    conf.set(MRConfig.FRAMEWORK_NAME, MRConfig.YARN_FRAMEWORK_NAME);\n     conf.set(YarnConfiguration.RM_ADDRESS, RMADDRESS);\n     conf.set(JHAdminConfig.MR_HISTORY_ADDRESS, HSHOSTADDRESS);\n     RMService rmService = new RMService(\"test\");"
    },
    {
        "commit_id": "482d840bcf3e8332e513bcea61f6d4ea7ee8579d",
        "commit_message": "MAPREDUCE-3004. Fix ReduceTask to not assume 'local' mode in YARN. Contributed by Hitesh Shah.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1172893 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/482d840bcf3e8332e513bcea61f6d4ea7ee8579d",
        "buggy_code": "conf.set(MRConfig.FRAMEWORK_NAME, \"yarn\");",
        "fixed_code": "conf.set(MRConfig.FRAMEWORK_NAME, MRConfig.YARN_FRAMEWORK_NAME);",
        "patch": "@@ -59,7 +59,7 @@ public MiniMRYarnCluster(String testName) {\n \n   @Override\n   public void init(Configuration conf) {\n-    conf.set(MRConfig.FRAMEWORK_NAME, \"yarn\");\n+    conf.set(MRConfig.FRAMEWORK_NAME, MRConfig.YARN_FRAMEWORK_NAME);\n     conf.set(MRJobConfig.USER_NAME, System.getProperty(\"user.name\"));\n     conf.set(MRJobConfig.MR_AM_STAGING_DIR, new File(getTestWorkDir(),\n         \"apps_staging_dir/${user.name}/\").getAbsolutePath());"
    },
    {
        "commit_id": "2e61ed306f1d525096a800f28546601ef585a832",
        "commit_message": "MAPREDUCE-3030. Fixed a bug in NodeId.equals() that was causing RM to reject all NMs. Contributed by Devaraj K.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1172638 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/2e61ed306f1d525096a800f28546601ef585a832",
        "buggy_code": "if (!super.equals(obj))",
        "fixed_code": "if (obj == null)",
        "patch": "@@ -76,7 +76,7 @@ public int hashCode() {\n   public boolean equals(Object obj) {\n     if (this == obj)\n       return true;\n-    if (!super.equals(obj))\n+    if (obj == null)\n       return false;\n     if (getClass() != obj.getClass())\n       return false;"
    },
    {
        "commit_id": "341a15a23d16f4ee6cbbd6fe7ad36d4c07b4e240",
        "commit_message": "MAPREDUCE-2994. Fixed a bug in ApplicationID parsing that affects RM UI. Contributed by Devaraj K.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1171485 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/341a15a23d16f4ee6cbbd6fe7ad36d4c07b4e240",
        "buggy_code": "public static final String APP = \"app\";",
        "fixed_code": "public static final String APP = \"application\";",
        "patch": "@@ -30,7 +30,7 @@\n  * Yarn application related utilities\n  */\n public class Apps {\n-  public static final String APP = \"app\";\n+  public static final String APP = \"application\";\n   public static final String ID = \"ID\";\n \n   public static ApplicationId toAppID(String aid) {"
    },
    {
        "commit_id": "be2b0921fa3d1d82fed75bccfceae007d9faaea6",
        "commit_message": "HDFS-2331. Fix WebHdfsFileSystem compilation problems for a bug in JDK version < 1.6.0_26.  Contributed by Abhijit Suresh Shingate\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1170996 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/be2b0921fa3d1d82fed75bccfceae007d9faaea6",
        "buggy_code": "return jsonParse(conn.getInputStream());",
        "fixed_code": "return WebHdfsFileSystem.<T>jsonParse(conn.getInputStream());",
        "patch": "@@ -206,7 +206,7 @@ private <T> T run(final HttpOpParam.Op op, final Path fspath,\n     final HttpURLConnection conn = httpConnect(op, fspath, parameters);\n     validateResponse(op, conn);\n     try {\n-      return jsonParse(conn.getInputStream());\n+      return WebHdfsFileSystem.<T>jsonParse(conn.getInputStream());\n     } finally {\n       conn.disconnect();\n     }"
    },
    {
        "commit_id": "7fa0bf355893890ba981050c6d94f458a08af68d",
        "commit_message": "MAPREDUCE-2874. Fix formatting of ApplicationId in web-ui. Contributed by Eric Payne.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1169973 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/7fa0bf355893890ba981050c6d94f458a08af68d",
        "buggy_code": "set(APP_ID, Apps.toString(app.context.getApplicationID()));",
        "fixed_code": "set(APP_ID, app.context.getApplicationID().toString());",
        "patch": "@@ -47,7 +47,7 @@ protected AppController(App app, Configuration conf, RequestContext ctx,\n       String title) {\n     super(ctx);\n     this.app = app;\n-    set(APP_ID, Apps.toString(app.context.getApplicationID()));\n+    set(APP_ID, app.context.getApplicationID().toString());\n     set(RM_WEB, YarnConfiguration.getRMWebAppURL(conf));\n   }\n "
    },
    {
        "commit_id": "7fa0bf355893890ba981050c6d94f458a08af68d",
        "commit_message": "MAPREDUCE-2874. Fix formatting of ApplicationId in web-ui. Contributed by Eric Payne.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1169973 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/7fa0bf355893890ba981050c6d94f458a08af68d",
        "buggy_code": "assertEquals(Apps.toString(ctx.appID), controller.get(APP_ID,\"\"));",
        "fixed_code": "assertEquals(ctx.appID.toString(), controller.get(APP_ID,\"\"));",
        "patch": "@@ -108,7 +108,7 @@ public long getStartTime() {\n     Injector injector = WebAppTests.createMockInjector(AppContext.class, ctx);\n     AppController controller = injector.getInstance(AppController.class);\n     controller.index();\n-    assertEquals(Apps.toString(ctx.appID), controller.get(APP_ID,\"\"));\n+    assertEquals(ctx.appID.toString(), controller.get(APP_ID,\"\"));\n   }\n \n   @Test public void testAppView() {"
    },
    {
        "commit_id": "7fa0bf355893890ba981050c6d94f458a08af68d",
        "commit_message": "MAPREDUCE-2874. Fix formatting of ApplicationId in web-ui. Contributed by Eric Payne.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1169973 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/7fa0bf355893890ba981050c6d94f458a08af68d",
        "buggy_code": "assertEquals(Apps.toString(ctx.appID), controller.get(APP_ID,\"\"));",
        "fixed_code": "assertEquals(ctx.appID.toString(), controller.get(APP_ID,\"\"));",
        "patch": "@@ -107,7 +107,7 @@ public long getStartTime() {\n     Injector injector = WebAppTests.createMockInjector(AppContext.class, ctx);\n     HsController controller = injector.getInstance(HsController.class);\n     controller.index();\n-    assertEquals(Apps.toString(ctx.appID), controller.get(APP_ID,\"\"));\n+    assertEquals(ctx.appID.toString(), controller.get(APP_ID,\"\"));\n   }\n \n   @Test public void testJobView() {"
    },
    {
        "commit_id": "7fa0bf355893890ba981050c6d94f458a08af68d",
        "commit_message": "MAPREDUCE-2874. Fix formatting of ApplicationId in web-ui. Contributed by Eric Payne.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1169973 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/7fa0bf355893890ba981050c6d94f458a08af68d",
        "buggy_code": "String appId = Apps.toString(app.getApplicationId());",
        "fixed_code": "String appId = app.getApplicationId().toString();",
        "patch": "@@ -56,7 +56,7 @@ class AppsBlock extends HtmlBlock {\n         tbody();\n     int i = 0;\n     for (RMApp app : list.apps.values()) {\n-      String appId = Apps.toString(app.getApplicationId());\n+      String appId = app.getApplicationId().toString();\n       String trackingUrl = app.getTrackingUrl();\n       String ui = trackingUrl == null || trackingUrl.isEmpty() ? \"UNASSIGNED\" :\n           (app.getFinishTime() == 0 ? \"ApplicationMaster\" : \"JobHistory\");"
    },
    {
        "commit_id": "7fa0bf355893890ba981050c6d94f458a08af68d",
        "commit_message": "MAPREDUCE-2874. Fix formatting of ApplicationId in web-ui. Contributed by Eric Payne.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1169973 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/7fa0bf355893890ba981050c6d94f458a08af68d",
        "buggy_code": "String appID = Apps.toString(app.getApplicationId());",
        "fixed_code": "String appID = app.getApplicationId().toString();",
        "patch": "@@ -60,7 +60,7 @@ void toDataTableArrays(PrintWriter out) {\n       } else {\n         out.append(\",\\n\");\n       }\n-      String appID = Apps.toString(app.getApplicationId());\n+      String appID = app.getApplicationId().toString();\n       String trackingUrl = app.getTrackingUrl();\n       String ui = trackingUrl == null ? \"UNASSIGNED\" :\n           (app.getFinishTime() == 0 ? \"ApplicationMaster\" : \"JobHistory\");"
    },
    {
        "commit_id": "ca853445e9a31e05278e9dceea9dbed734103f49",
        "commit_message": "MAPREDUCE-2953. Fix a race condition on submission which caused client to incorrectly assume application was gone by making submission synchronous for RMAppManager. Contributed by Thomas Graves.  \n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1166968 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/ca853445e9a31e05278e9dceea9dbed734103f49",
        "buggy_code": "protected void submitApplication(ApplicationSubmissionContext submissionContext) {",
        "fixed_code": "protected synchronized void submitApplication(ApplicationSubmissionContext submissionContext) {",
        "patch": "@@ -210,7 +210,7 @@ protected synchronized void checkAppNumCompletedLimit() {\n     }\n   }\n \n-  protected void submitApplication(ApplicationSubmissionContext submissionContext) {\n+  protected synchronized void submitApplication(ApplicationSubmissionContext submissionContext) {\n     ApplicationId applicationId = submissionContext.getApplicationId();\n     RMApp application = null;\n     try {"
    },
    {
        "commit_id": "1f46b991da9b91585608a0babd3eda39485dce09",
        "commit_message": "MAPREDUCE-2908. Fix all findbugs warnings. Contributed by Vinod K V. \n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1166838 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/1f46b991da9b91585608a0babd3eda39485dce09",
        "buggy_code": "public class ProcResourceValues {",
        "fixed_code": "public static class ProcResourceValues {",
        "patch": "@@ -97,7 +97,7 @@ public abstract class ResourceCalculatorPlugin extends Configured {\n   @InterfaceStability.Unstable\n   public abstract ProcResourceValues getProcResourceValues();\n \n-  public class ProcResourceValues {\n+  public static class ProcResourceValues {\n     private final long cumulativeCpuTime;\n     private final long physicalMemorySize;\n     private final long virtualMemorySize;"
    },
    {
        "commit_id": "1f46b991da9b91585608a0babd3eda39485dce09",
        "commit_message": "MAPREDUCE-2908. Fix all findbugs warnings. Contributed by Vinod K V. \n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1166838 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/1f46b991da9b91585608a0babd3eda39485dce09",
        "buggy_code": "public class ProcResourceValues {",
        "fixed_code": "public static class ProcResourceValues {",
        "patch": "@@ -97,7 +97,7 @@ public abstract class ResourceCalculatorPlugin extends Configured {\n   @InterfaceStability.Unstable\n   public abstract ProcResourceValues getProcResourceValues();\n \n-  public class ProcResourceValues {\n+  public static class ProcResourceValues {\n     private final long cumulativeCpuTime;\n     private final long physicalMemorySize;\n     private final long virtualMemorySize;"
    },
    {
        "commit_id": "1f46b991da9b91585608a0babd3eda39485dce09",
        "commit_message": "MAPREDUCE-2908. Fix all findbugs warnings. Contributed by Vinod K V. \n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1166838 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/1f46b991da9b91585608a0babd3eda39485dce09",
        "buggy_code": "public static String EXPIRE_APPLICATIONS_COMPLETED_MAX =",
        "fixed_code": "public static final String EXPIRE_APPLICATIONS_COMPLETED_MAX =",
        "patch": "@@ -87,7 +87,7 @@ public class RMConfig {\n   public static final String DEFAULT_RM_NODES_EXCLUDE_FILE = \"\";\n \n   // the maximum number of completed applications RM keeps \n-  public static String EXPIRE_APPLICATIONS_COMPLETED_MAX =\n+  public static final String EXPIRE_APPLICATIONS_COMPLETED_MAX =\n     YarnConfiguration.RM_PREFIX + \"expire.applications.completed.max\";\n   public static final int DEFAULT_EXPIRE_APPLICATIONS_COMPLETED_MAX = 10000;\n }"
    },
    {
        "commit_id": "1f46b991da9b91585608a0babd3eda39485dce09",
        "commit_message": "MAPREDUCE-2908. Fix all findbugs warnings. Contributed by Vinod K V. \n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1166838 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/1f46b991da9b91585608a0babd3eda39485dce09",
        "buggy_code": "public Queue hook(Queue queue) {",
        "fixed_code": "public CSQueue hook(CSQueue queue) {",
        "patch": "@@ -85,7 +85,7 @@ public EventHandler getEventHandler() {\n    */\n   static class SpyHook extends CapacityScheduler.QueueHook {\n     @Override\n-    public Queue hook(Queue queue) {\n+    public CSQueue hook(CSQueue queue) {\n       return spy(queue);\n     }\n   }"
    },
    {
        "commit_id": "58676edf98cf8e1c9179ac8e89923c16bc3df3b5",
        "commit_message": "MAPREDUCE-2879. Fix version for MR-279 to 0.23.0.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161705 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/58676edf98cf8e1c9179ac8e89923c16bc3df3b5",
        "buggy_code": "\"hadoop-mapreduce-client-app-1.0-SNAPSHOT.jar\";",
        "fixed_code": "\"hadoop-mapreduce-client-app-0.23.0-SNAPSHOT.jar\";",
        "patch": "@@ -40,7 +40,7 @@ public interface MRConstants {\n   public static final String JOB_JAR = \"job.jar\";\n \n   public static final String HADOOP_MAPREDUCE_CLIENT_APP_JAR_NAME =\n-      \"hadoop-mapreduce-client-app-1.0-SNAPSHOT.jar\";\n+      \"hadoop-mapreduce-client-app-0.23.0-SNAPSHOT.jar\";\n \n   public static final String YARN_MAPREDUCE_APP_JAR_PATH =\n     \"$YARN_HOME/modules/\" + HADOOP_MAPREDUCE_CLIENT_APP_JAR_NAME;"
    },
    {
        "commit_id": "f2b91a8367a762091482074505618b570a520b19",
        "commit_message": " MAPREDUCE-2807. Fix AM restart and client redirection. Contributed by Sharad Agarwal.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161408 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/f2b91a8367a762091482074505618b570a520b19",
        "buggy_code": "NEW, SUBMITTED, RUNNING, RESTARTING, SUCCEEDED, FAILED, KILLED",
        "fixed_code": "NEW, SUBMITTED, RUNNING, SUCCEEDED, FAILED, KILLED",
        "patch": "@@ -19,5 +19,5 @@\n package org.apache.hadoop.yarn.api.records;\n \n public enum ApplicationState {\n-  NEW, SUBMITTED, RUNNING, RESTARTING, SUCCEEDED, FAILED, KILLED\n+  NEW, SUBMITTED, RUNNING, SUCCEEDED, FAILED, KILLED\n }"
    },
    {
        "commit_id": "f2b91a8367a762091482074505618b570a520b19",
        "commit_message": " MAPREDUCE-2807. Fix AM restart and client redirection. Contributed by Sharad Agarwal.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161408 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/f2b91a8367a762091482074505618b570a520b19",
        "buggy_code": "NEW, SUBMITTED, ACCEPTED, RUNNING, RESTARTING, FINISHED, FAILED, KILLED",
        "fixed_code": "NEW, SUBMITTED, ACCEPTED, RUNNING, FINISHED, FAILED, KILLED",
        "patch": "@@ -1,5 +1,5 @@\n package org.apache.hadoop.yarn.server.resourcemanager.rmapp;\n \n public enum RMAppState {\n-  NEW, SUBMITTED, ACCEPTED, RUNNING, RESTARTING, FINISHED, FAILED, KILLED\n+  NEW, SUBMITTED, ACCEPTED, RUNNING, FINISHED, FAILED, KILLED\n }"
    },
    {
        "commit_id": "ded6f225a55517deedc2bd502f2b68f1ca2ddee8",
        "commit_message": "MAPREDUCE-2837. Ported bug fixes from y-merge to prepare for MAPREDUCE-279 merge. \n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1157249 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/ded6f225a55517deedc2bd502f2b68f1ca2ddee8",
        "buggy_code": "Counters getCounters() { return EventReader.fromAvro(datum.counters); }",
        "fixed_code": "public Counters getCounters() { return EventReader.fromAvro(datum.counters); }",
        "patch": "@@ -67,7 +67,7 @@ public void setDatum(Object datum) {\n   /** Get the task finish time */\n   public long getFinishTime() { return datum.finishTime; }\n   /** Get task counters */\n-  Counters getCounters() { return EventReader.fromAvro(datum.counters); }\n+  public Counters getCounters() { return EventReader.fromAvro(datum.counters); }\n   /** Get task type */\n   public TaskType getTaskType() {\n     return TaskType.valueOf(datum.taskType.toString());"
    },
    {
        "commit_id": "ded6f225a55517deedc2bd502f2b68f1ca2ddee8",
        "commit_message": "MAPREDUCE-2837. Ported bug fixes from y-merge to prepare for MAPREDUCE-279 merge. \n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1157249 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/ded6f225a55517deedc2bd502f2b68f1ca2ddee8",
        "buggy_code": "MapOutputFile mapOutputFile = new MapOutputFile();",
        "fixed_code": "MapOutputFile mapOutputFile = new MROutputFiles();",
        "patch": "@@ -293,7 +293,7 @@ public void reduce(WritableComparable key, Iterator values,\n                        ) throws IOException {\n       if (first) {\n         first = false;\n-        MapOutputFile mapOutputFile = new MapOutputFile();\n+        MapOutputFile mapOutputFile = new MROutputFiles();\n         mapOutputFile.setConf(conf);\n         Path input = mapOutputFile.getInputFile(0);\n         FileSystem fs = FileSystem.get(conf);"
    },
    {
        "commit_id": "ded6f225a55517deedc2bd502f2b68f1ca2ddee8",
        "commit_message": "MAPREDUCE-2837. Ported bug fixes from y-merge to prepare for MAPREDUCE-279 merge. \n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1157249 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/ded6f225a55517deedc2bd502f2b68f1ca2ddee8",
        "buggy_code": "String nnUri = dfsCluster.getURI().toString();",
        "fixed_code": "String nnUri = dfsCluster.getURI(0).toString();",
        "patch": "@@ -178,7 +178,7 @@ public void testBinaryTokenFile() throws IOException {\n     jConf = mrCluster.createJobConf();\n     \n     // provide namenodes names for the job to get the delegation tokens for\n-    String nnUri = dfsCluster.getURI().toString();\n+    String nnUri = dfsCluster.getURI(0).toString();\n     jConf.set(MRJobConfig.JOB_NAMENODES, nnUri + \",\" + nnUri);\n     // job tracker principla id..\n     jConf.set(JTConfig.JT_USER_NAME, \"jt_id\");"
    },
    {
        "commit_id": "ded6f225a55517deedc2bd502f2b68f1ca2ddee8",
        "commit_message": "MAPREDUCE-2837. Ported bug fixes from y-merge to prepare for MAPREDUCE-279 merge. \n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1157249 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/ded6f225a55517deedc2bd502f2b68f1ca2ddee8",
        "buggy_code": "URI uri = cluster.getURI();",
        "fixed_code": "URI uri = cluster.getURI(0);",
        "patch": "@@ -108,7 +108,7 @@ public void setUp() throws Exception {\n     cluster = new MiniDFSCluster(0, config, 1, true, true, true,  null, null, \n         null, null);\n     cluster.waitActive();\n-    URI uri = cluster.getURI();\n+    URI uri = cluster.getURI(0);\n     \n     MiniMRCluster miniMRCluster = new MiniMRCluster(0, uri.toString() , \n       3, null, null, config);"
    },
    {
        "commit_id": "961bae4ec599966c47aa1db06f9d9ba8719bfcd1",
        "commit_message": "Fix bad import in HADOOP-615.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1156860 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/961bae4ec599966c47aa1db06f9d9ba8719bfcd1",
        "buggy_code": "import org.apache.hadoop.util.CyclicIteration;",
        "fixed_code": "import org.apache.hadoop.hdfs.util.CyclicIteration;",
        "patch": "@@ -60,12 +60,12 @@\n import org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol;\n import org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration;\n import org.apache.hadoop.hdfs.server.protocol.DisallowedDatanodeException;\n+import org.apache.hadoop.hdfs.util.CyclicIteration;\n import org.apache.hadoop.ipc.Server;\n import org.apache.hadoop.net.CachedDNSToSwitchMapping;\n import org.apache.hadoop.net.DNSToSwitchMapping;\n import org.apache.hadoop.net.NetworkTopology;\n import org.apache.hadoop.net.ScriptBasedMapping;\n-import org.apache.hadoop.util.CyclicIteration;\n import org.apache.hadoop.util.Daemon;\n import org.apache.hadoop.util.HostsFileReader;\n import org.apache.hadoop.util.ReflectionUtils;"
    },
    {
        "commit_id": "c316d43a0a9e6e803550d5873b5b6cb0bd032117",
        "commit_message": "HDFS-1776 Bug in Bug in Concat code. Contributed by Bharath Mundlapudi\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1150247 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/c316d43a0a9e6e803550d5873b5b6cb0bd032117",
        "buggy_code": "for(BlockInfo bi: this.blocks) {",
        "fixed_code": "for(BlockInfo bi: newlist) {",
        "patch": "@@ -127,7 +127,7 @@ void appendBlocks(INodeFile [] inodes, int totalAddedBlocks) {\n       size += in.blocks.length;\n     }\n     \n-    for(BlockInfo bi: this.blocks) {\n+    for(BlockInfo bi: newlist) {\n       bi.setINode(this);\n     }\n     this.blocks = newlist;"
    },
    {
        "commit_id": "c3f6575ca44e8ad803d0b46991472465b595cdeb",
        "commit_message": "HDFS-2147. Move cluster network topology to block management and fix some javac warnings.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1148112 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/c3f6575ca44e8ad803d0b46991472465b595cdeb",
        "buggy_code": "BlocksMap(int initialCapacity, float loadFactor) {",
        "fixed_code": "BlocksMap(final float loadFactor) {",
        "patch": "@@ -57,7 +57,7 @@ public void remove()  {\n   \n   private GSet<Block, BlockInfo> blocks;\n \n-  BlocksMap(int initialCapacity, float loadFactor) {\n+  BlocksMap(final float loadFactor) {\n     this.capacity = computeCapacity();\n     this.blocks = new LightWeightGSet<Block, BlockInfo>(capacity);\n   }"
    },
    {
        "commit_id": "c3f6575ca44e8ad803d0b46991472465b595cdeb",
        "commit_message": "HDFS-2147. Move cluster network topology to block management and fix some javac warnings.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1148112 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/c3f6575ca44e8ad803d0b46991472465b595cdeb",
        "buggy_code": "return nn.getNamesystem().getRandomDatanode();",
        "fixed_code": "return NamenodeJspHelper.getRandomDatanode(nn);",
        "patch": "@@ -86,7 +86,7 @@ private DatanodeID pickSrcDatanode(LocatedBlocks blks, HdfsFileStatus i)\n     if (i.getLen() == 0 || blks.getLocatedBlocks().size() <= 0) {\n       // pick a random datanode\n       NameNode nn = (NameNode)getServletContext().getAttribute(\"name.node\");\n-      return nn.getNamesystem().getRandomDatanode();\n+      return NamenodeJspHelper.getRandomDatanode(nn);\n     }\n     return JspHelper.bestNode(blks);\n   }"
    },
    {
        "commit_id": "a7a3653b7006297958e79146aa46011d6060099f",
        "commit_message": "HADOOP-7341. Fix options parsing in CommandFormat. Contributed by Daryn Sharp.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1132505 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/a7a3653b7006297958e79146aa46011d6060099f",
        "buggy_code": "CommandFormat cf = new CommandFormat(NAME, 1, Integer.MAX_VALUE, \"q\");",
        "fixed_code": "CommandFormat cf = new CommandFormat(1, Integer.MAX_VALUE, \"q\");",
        "patch": "@@ -73,7 +73,7 @@ public Count(String[] cmd, int pos, Configuration conf) {\n \n   @Override\n   protected void processOptions(LinkedList<String> args) {\n-    CommandFormat cf = new CommandFormat(NAME, 1, Integer.MAX_VALUE, \"q\");\n+    CommandFormat cf = new CommandFormat(1, Integer.MAX_VALUE, \"q\");\n     cf.parse(args);\n     if (args.isEmpty()) { // default path is the current working directory\n       args.add(\".\");"
    },
    {
        "commit_id": "a7a3653b7006297958e79146aa46011d6060099f",
        "commit_message": "HADOOP-7341. Fix options parsing in CommandFormat. Contributed by Daryn Sharp.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1132505 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/a7a3653b7006297958e79146aa46011d6060099f",
        "buggy_code": "CommandFormat cf = new CommandFormat(null, 1, Integer.MAX_VALUE, \"ignoreCrc\");",
        "fixed_code": "CommandFormat cf = new CommandFormat(1, Integer.MAX_VALUE, \"ignoreCrc\");",
        "patch": "@@ -66,7 +66,7 @@ public static class Cat extends Display {\n     @Override\n     protected void processOptions(LinkedList<String> args)\n     throws IOException {\n-      CommandFormat cf = new CommandFormat(null, 1, Integer.MAX_VALUE, \"ignoreCrc\");\n+      CommandFormat cf = new CommandFormat(1, Integer.MAX_VALUE, \"ignoreCrc\");\n       cf.parse(args);\n       verifyChecksum = !cf.getOpt(\"ignoreCrc\");\n     }"
    },
    {
        "commit_id": "a7a3653b7006297958e79146aa46011d6060099f",
        "commit_message": "HADOOP-7341. Fix options parsing in CommandFormat. Contributed by Daryn Sharp.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1132505 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/a7a3653b7006297958e79146aa46011d6060099f",
        "buggy_code": "CommandFormat cf = new CommandFormat(null, 0, Integer.MAX_VALUE, \"R\");",
        "fixed_code": "CommandFormat cf = new CommandFormat(0, Integer.MAX_VALUE, \"R\");",
        "patch": "@@ -62,7 +62,7 @@ public static void registerCommands(CommandFactory factory) {\n   @Override\n   protected void processOptions(LinkedList<String> args)\n   throws IOException {\n-    CommandFormat cf = new CommandFormat(null, 0, Integer.MAX_VALUE, \"R\");\n+    CommandFormat cf = new CommandFormat(0, Integer.MAX_VALUE, \"R\");\n     cf.parse(args);\n     setRecursive(cf.getOpt(\"R\"));\n     if (args.isEmpty()) args.add(Path.CUR_DIR);"
    },
    {
        "commit_id": "a7a3653b7006297958e79146aa46011d6060099f",
        "commit_message": "HADOOP-7341. Fix options parsing in CommandFormat. Contributed by Daryn Sharp.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1132505 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/a7a3653b7006297958e79146aa46011d6060099f",
        "buggy_code": "CommandFormat cf = new CommandFormat(null, 1, Integer.MAX_VALUE);",
        "fixed_code": "CommandFormat cf = new CommandFormat(1, Integer.MAX_VALUE);",
        "patch": "@@ -45,7 +45,7 @@ public static void registerCommands(CommandFactory factory) {\n \n   @Override\n   protected void processOptions(LinkedList<String> args) {\n-    CommandFormat cf = new CommandFormat(null, 1, Integer.MAX_VALUE);\n+    CommandFormat cf = new CommandFormat(1, Integer.MAX_VALUE);\n     cf.parse(args);\n   }\n "
    },
    {
        "commit_id": "a7a3653b7006297958e79146aa46011d6060099f",
        "commit_message": "HADOOP-7341. Fix options parsing in CommandFormat. Contributed by Daryn Sharp.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1132505 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/a7a3653b7006297958e79146aa46011d6060099f",
        "buggy_code": "CommandFormat cf = new CommandFormat(null, 2, Integer.MAX_VALUE);",
        "fixed_code": "CommandFormat cf = new CommandFormat(2, Integer.MAX_VALUE);",
        "patch": "@@ -78,7 +78,7 @@ public static class Rename extends CommandWithDestination {\n \n     @Override\n     protected void processOptions(LinkedList<String> args) throws IOException {\n-      CommandFormat cf = new CommandFormat(null, 2, Integer.MAX_VALUE);\n+      CommandFormat cf = new CommandFormat(2, Integer.MAX_VALUE);\n       cf.parse(args);\n       getRemoteDestination(args);\n     }"
    },
    {
        "commit_id": "a7a3653b7006297958e79146aa46011d6060099f",
        "commit_message": "HADOOP-7341. Fix options parsing in CommandFormat. Contributed by Daryn Sharp.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1132505 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/a7a3653b7006297958e79146aa46011d6060099f",
        "buggy_code": "CommandFormat cf = new CommandFormat(null, 1, Integer.MAX_VALUE, \"R\");",
        "fixed_code": "CommandFormat cf = new CommandFormat(1, Integer.MAX_VALUE, \"R\");",
        "patch": "@@ -64,7 +64,7 @@ public static void registerCommands(CommandFactory factory) {\n \n   @Override\n   protected void processOptions(LinkedList<String> args) throws IOException {\n-    CommandFormat cf = new CommandFormat(null, 1, Integer.MAX_VALUE, \"R\");\n+    CommandFormat cf = new CommandFormat(1, Integer.MAX_VALUE, \"R\");\n     cf.parse(args);\n     setRecursive(cf.getOpt(\"R\"));\n     if (args.getFirst().contains(\"%\")) format = args.removeFirst();"
    },
    {
        "commit_id": "a7a3653b7006297958e79146aa46011d6060099f",
        "commit_message": "HADOOP-7341. Fix options parsing in CommandFormat. Contributed by Daryn Sharp.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1132505 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/a7a3653b7006297958e79146aa46011d6060099f",
        "buggy_code": "CommandFormat cf = new CommandFormat(null, 1, 1, \"f\");",
        "fixed_code": "CommandFormat cf = new CommandFormat(1, 1, \"f\");",
        "patch": "@@ -51,7 +51,7 @@ public static void registerCommands(CommandFactory factory) {\n   \n   @Override\n   protected void processOptions(LinkedList<String> args) throws IOException {\n-    CommandFormat cf = new CommandFormat(null, 1, 1, \"f\");\n+    CommandFormat cf = new CommandFormat(1, 1, \"f\");\n     cf.parse(args);\n     follow = cf.getOpt(\"f\");\n   }"
    },
    {
        "commit_id": "a7a3653b7006297958e79146aa46011d6060099f",
        "commit_message": "HADOOP-7341. Fix options parsing in CommandFormat. Contributed by Daryn Sharp.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1132505 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/a7a3653b7006297958e79146aa46011d6060099f",
        "buggy_code": "CommandFormat cf = new CommandFormat(null, 1, 1, \"e\", \"d\", \"z\");",
        "fixed_code": "CommandFormat cf = new CommandFormat(1, 1, \"e\", \"d\", \"z\");",
        "patch": "@@ -46,7 +46,7 @@ public static void registerCommands(CommandFactory factory) {\n   \n   @Override\n   protected void processOptions(LinkedList<String> args) {\n-    CommandFormat cf = new CommandFormat(null, 1, 1, \"e\", \"d\", \"z\");\n+    CommandFormat cf = new CommandFormat(1, 1, \"e\", \"d\", \"z\");\n     cf.parse(args);\n     \n     String[] opts = cf.getOpts().toArray(new String[0]);"
    },
    {
        "commit_id": "a7a3653b7006297958e79146aa46011d6060099f",
        "commit_message": "HADOOP-7341. Fix options parsing in CommandFormat. Contributed by Daryn Sharp.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1132505 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/a7a3653b7006297958e79146aa46011d6060099f",
        "buggy_code": "CommandFormat cf = new CommandFormat(null, 1, Integer.MAX_VALUE);",
        "fixed_code": "CommandFormat cf = new CommandFormat(1, Integer.MAX_VALUE);",
        "patch": "@@ -52,7 +52,7 @@ public static class Touchz extends Touch {\n \n     @Override\n     protected void processOptions(LinkedList<String> args) {\n-      CommandFormat cf = new CommandFormat(null, 1, Integer.MAX_VALUE);\n+      CommandFormat cf = new CommandFormat(1, Integer.MAX_VALUE);\n       cf.parse(args);\n     }\n "
    },
    {
        "commit_id": "a5290c9eca69027cff2448d05fee6983cbb54cd7",
        "commit_message": "HADOOP-7271. Standardize shell command error messages.  Contributed by Daryn Sharp\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1101653 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/a5290c9eca69027cff2448d05fee6983cbb54cd7",
        "buggy_code": "assertTrue(\" Not displaying the intended message \",results.contains(\"get: \"+args[1]+\": No such file or directory\"));",
        "fixed_code": "assertTrue(\" Not displaying the intended message \",results.contains(\"get: `\"+args[1]+\"': No such file or directory\"));",
        "patch": "@@ -288,7 +288,7 @@ public void testGetWithInvalidSourcePathShouldNotDisplayNullInConsole()\n       results = bytes.toString();\n       assertTrue(\"Return code should be -1\", run == -1);\n       assertTrue(\" Null is coming when source path is invalid. \",!results.contains(\"get: null\"));\n-      assertTrue(\" Not displaying the intended message \",results.contains(\"get: \"+args[1]+\": No such file or directory\"));\n+      assertTrue(\" Not displaying the intended message \",results.contains(\"get: `\"+args[1]+\"': No such file or directory\"));\n     } finally {\n       IOUtils.closeStream(out);\n       System.setErr(oldErr);"
    },
    {
        "commit_id": "e0cc26093c99162f3a78859ecd4c6337cfd49d80",
        "commit_message": "HADOOP-7241. fix typo of command 'hadoop fs -help tail'. Contributed by Wei Yongjun\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1096522 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/e0cc26093c99162f3a78859ecd4c6337cfd49d80",
        "buggy_code": "+ \"\\t\\tThe -f option shows apended data as the file grows. \\n\";",
        "fixed_code": "+ \"\\t\\tThe -f option shows appended data as the file grows. \\n\";",
        "patch": "@@ -1436,7 +1436,7 @@ private void printHelp(String cmd) {\n \n     String tail = TAIL_USAGE\n       + \":  Show the last 1KB of the file. \\n\"\n-      + \"\\t\\tThe -f option shows apended data as the file grows. \\n\";\n+      + \"\\t\\tThe -f option shows appended data as the file grows. \\n\";\n \n     String chmod = FsShellPermissions.CHMOD_USAGE + \"\\n\" +\n       \"\\t\\tChanges permissions of a file.\\n\" +"
    },
    {
        "commit_id": "3b57b151c9e69fe82c4928e967eceb627074fe3c",
        "commit_message": "HADOOP-7231. Fix synopsis for -count. Contributed by Daryn Sharp.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1095121 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/3b57b151c9e69fe82c4928e967eceb627074fe3c",
        "buggy_code": "public static final String USAGE = \"-\" + NAME + \"[-q] <path>\";",
        "fixed_code": "public static final String USAGE = \"-\" + NAME + \" [-q] <path> ...\";",
        "patch": "@@ -43,7 +43,7 @@ public static void registerCommands(CommandFactory factory) {\n   }\n \n   public static final String NAME = \"count\";\n-  public static final String USAGE = \"-\" + NAME + \"[-q] <path>\";\n+  public static final String USAGE = \"-\" + NAME + \" [-q] <path> ...\";\n   public static final String DESCRIPTION = CommandUtils.formatDescription(USAGE, \n       \"Count the number of directories, files and bytes under the paths\",\n       \"that match the specified file pattern.  The output columns are:\","
    },
    {
        "commit_id": "e82df7e7f7360942ddc99b542c465c4716b2e775",
        "commit_message": "HADOOP-7126. Fix file permission setting for RawLocalFileSystem on Windows. Contributed by Po Cheung.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1065901 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/e82df7e7f7360942ddc99b542c465c4716b2e775",
        "buggy_code": "args[cmd.length] = f.getCanonicalPath();",
        "fixed_code": "args[cmd.length] = FileUtil.makeShellPath(f, true);",
        "patch": "@@ -567,7 +567,7 @@ public void setPermission(Path p, FsPermission permission)\n   private static String execCommand(File f, String... cmd) throws IOException {\n     String[] args = new String[cmd.length + 1];\n     System.arraycopy(cmd, 0, args, 0, cmd.length);\n-    args[cmd.length] = f.getCanonicalPath();\n+    args[cmd.length] = FileUtil.makeShellPath(f, true);\n     String output = Shell.execCommand(args);\n     return output;\n   }"
    },
    {
        "commit_id": "3460a5e345f50ffcdd03a896b4410acbbe3b7711",
        "commit_message": "HADOOP-6758. MapFile.fix does not allow index interval definition. Contributed by Gianmarco De Francisci Morales.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1031743 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/3460a5e345f50ffcdd03a896b4410acbbe3b7711",
        "buggy_code": "int indexInterval = 128;",
        "fixed_code": "int indexInterval = conf.getInt(Writer.INDEX_INTERVAL, 128);",
        "patch": "@@ -771,7 +771,7 @@ public static long fix(FileSystem fs, Path dir,\n     String dr = (dryrun ? \"[DRY RUN ] \" : \"\");\n     Path data = new Path(dir, DATA_FILE_NAME);\n     Path index = new Path(dir, INDEX_FILE_NAME);\n-    int indexInterval = 128;\n+    int indexInterval = conf.getInt(Writer.INDEX_INTERVAL, 128);\n     if (!fs.exists(data)) {\n       // there's nothing we can do to fix this!\n       throw new Exception(dr + \"Missing data file in \" + dir + \", impossible to fix this.\");"
    },
    {
        "commit_id": "dc2a3d1ca33102b30a7c2eea67df1ba447813081",
        "commit_message": "HADOOP-6900. Make the iterator returned by FileSystem#listLocatedStatus to throw IOException rather than RuntimeException when there is an IO error fetching the next file. Contributed by Hairong Kuang. \n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@984301 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/dc2a3d1ca33102b30a7c2eea67df1ba447813081",
        "buggy_code": "public Iterator<LocatedFileStatus> listLocatedStatus(Path f)",
        "fixed_code": "public RemoteIterator<LocatedFileStatus> listLocatedStatus(Path f)",
        "patch": "@@ -504,7 +504,7 @@ public FileStatus[] listStatus(Path f) throws IOException {\n    * @throws IOException\n    */\n   @Override\n-  public Iterator<LocatedFileStatus> listLocatedStatus(Path f)\n+  public RemoteIterator<LocatedFileStatus> listLocatedStatus(Path f)\n   throws IOException {\n     return fs.listLocatedStatus(f, DEFAULT_FILTER);\n   }"
    },
    {
        "commit_id": "dc2a3d1ca33102b30a7c2eea67df1ba447813081",
        "commit_message": "HADOOP-6900. Make the iterator returned by FileSystem#listLocatedStatus to throw IOException rather than RuntimeException when there is an IO error fetching the next file. Contributed by Hairong Kuang. \n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@984301 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/dc2a3d1ca33102b30a7c2eea67df1ba447813081",
        "buggy_code": "public Iterator<LocatedFileStatus> listLocatedStatus(Path f)",
        "fixed_code": "public RemoteIterator<LocatedFileStatus> listLocatedStatus(Path f)",
        "patch": "@@ -167,7 +167,7 @@ public FileStatus[] listStatus(Path f) throws IOException {\n   }\n   \n   /** List files and its block locations in a directory. */\n-  public Iterator<LocatedFileStatus> listLocatedStatus(Path f)\n+  public RemoteIterator<LocatedFileStatus> listLocatedStatus(Path f)\n   throws IOException {\n     return fs.listLocatedStatus(f);\n   }"
    },
    {
        "commit_id": "dc2a3d1ca33102b30a7c2eea67df1ba447813081",
        "commit_message": "HADOOP-6900. Make the iterator returned by FileSystem#listLocatedStatus to throw IOException rather than RuntimeException when there is an IO error fetching the next file. Contributed by Hairong Kuang. \n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@984301 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/dc2a3d1ca33102b30a7c2eea67df1ba447813081",
        "buggy_code": "Iterator<FileStatus> pathsIterator =",
        "fixed_code": "RemoteIterator<FileStatus> pathsIterator =",
        "patch": "@@ -282,7 +282,7 @@ public void testListStatus() throws Exception {\n     Assert.assertEquals(0, paths.length);\n     \n     // test listStatus that returns an iterator\n-    Iterator<FileStatus> pathsIterator = \n+    RemoteIterator<FileStatus> pathsIterator = \n       fc.listStatus(getTestRootPath(fc, \"test\"));\n     Assert.assertEquals(getTestRootPath(fc, \"test/hadoop\"), \n         pathsIterator.next().getPath());"
    },
    {
        "commit_id": "00cb89215017707e96dd1cff51d2b323b92ca512",
        "commit_message": "HADOOP-6634. Fix AccessControlList to use short names to verify access control. Contributed by Vinod Kumar Vavilapalli.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@939242 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/00cb89215017707e96dd1cff51d2b323b92ca512",
        "buggy_code": "if (allAllowed || users.contains(ugi.getUserName())) {",
        "fixed_code": "if (allAllowed || users.contains(ugi.getShortUserName())) {",
        "patch": "@@ -93,7 +93,7 @@ Set<String> getGroups() {\n   }\n \n   public boolean isUserAllowed(UserGroupInformation ugi) {\n-    if (allAllowed || users.contains(ugi.getUserName())) {\n+    if (allAllowed || users.contains(ugi.getShortUserName())) {\n       return true;\n     } else {\n       for(String group: ugi.getGroupNames()) {"
    },
    {
        "commit_id": "a90d3205d2a23945eaab8b756cfbeeb4377c3c04",
        "commit_message": "HADOOP-6489. Fix 3 findbugs warnings. Contributed by Erik Steffl.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@899856 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/a90d3205d2a23945eaab8b756cfbeeb4377c3c04",
        "buggy_code": "private static FileContext localFsSingleton = null;",
        "fixed_code": "volatile private static FileContext localFsSingleton = null;",
        "patch": "@@ -156,6 +156,7 @@ public final class FileContext {\n   \n   public static final Log LOG = LogFactory.getLog(FileContext.class);\n   public static final FsPermission DEFAULT_PERM = FsPermission.getDefault();\n+  volatile private static FileContext localFsSingleton = null;\n   \n   /**\n    * List of files that should be deleted on JVM shutdown.\n@@ -342,7 +343,6 @@ public static FileContext getFileContext() throws IOException {\n     return getFileContext(new Configuration());\n   } \n   \n-  private static FileContext localFsSingleton = null;\n   /**\n    * \n    * @return a FileContext for the local filesystem using the default config."
    },
    {
        "commit_id": "a90d3205d2a23945eaab8b756cfbeeb4377c3c04",
        "commit_message": "HADOOP-6489. Fix 3 findbugs warnings. Contributed by Erik Steffl.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@899856 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/a90d3205d2a23945eaab8b756cfbeeb4377c3c04",
        "buggy_code": "private static SerializationFactory serialFactory = null;",
        "fixed_code": "volatile private static SerializationFactory serialFactory = null;",
        "patch": "@@ -47,7 +47,7 @@\n public class ReflectionUtils {\n     \n   private static final Class<?>[] EMPTY_ARRAY = new Class[]{};\n-  private static SerializationFactory serialFactory = null;\n+  volatile private static SerializationFactory serialFactory = null;\n \n   /** \n    * Cache of constructors for each class. Pins the classes so they"
    },
    {
        "commit_id": "cdcb8514a030cfd0934071b426274a8b7838021c",
        "commit_message": "HADOOP-6314. Fix \"fs -help\" for the \"-count\" commond.  Contributed by Ravi Phulari\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@836019 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/cdcb8514a030cfd0934071b426274a8b7838021c",
        "buggy_code": "} else if (Count.matches(cmd)) {",
        "fixed_code": "} else if (Count.NAME.equals(cmd)) {",
        "patch": "@@ -1568,7 +1568,7 @@ private void printHelp(String cmd) {\n       System.out.println(chown);\n     } else if (\"chgrp\".equals(cmd)) {\n       System.out.println(chgrp);\n-    } else if (Count.matches(cmd)) {\n+    } else if (Count.NAME.equals(cmd)) {\n       System.out.println(Count.DESCRIPTION);\n     } else if (\"help\".equals(cmd)) {\n       System.out.println(help);"
    },
    {
        "commit_id": "d09ade4d0b64c1f0f629fe9361d511ed905732c6",
        "commit_message": "HADOOP-6203. FsShell rm/rmr error message indicates exceeding Trash quota and suggests using -skpTrash, when moving to trash fails. Contributed by Boris Shkolnik.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@812317 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/d09ade4d0b64c1f0f629fe9361d511ed905732c6",
        "buggy_code": "LOG.warn(\"Can't create trash directory: \"+baseTrashPath);",
        "fixed_code": "LOG.warn(\"Can't create(mkdir) trash directory: \"+baseTrashPath);",
        "patch": "@@ -123,7 +123,7 @@ public boolean moveToTrash(Path path) throws IOException {\n     for (int i = 0; i < 2; i++) {\n       try {\n         if (!fs.mkdirs(baseTrashPath, PERMISSION)) {      // create current\n-          LOG.warn(\"Can't create trash directory: \"+baseTrashPath);\n+          LOG.warn(\"Can't create(mkdir) trash directory: \"+baseTrashPath);\n           return false;\n         }\n       } catch (IOException e) {"
    },
    {
        "commit_id": "aa10f303e3cb5b8b533e3407a6be6d2b4d81217c",
        "commit_message": "HADOOP-6215. fix GenericOptionParser to deal with -D with '=' in the value. Contributed by Amar Kamat.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@808415 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/aa10f303e3cb5b8b533e3407a6be6d2b4d81217c",
        "buggy_code": "String[] keyval = prop.split(\"=\");",
        "fixed_code": "String[] keyval = prop.split(\"=\", 2);",
        "patch": "@@ -281,7 +281,7 @@ private void processGeneralOptions(Configuration conf,\n     if (line.hasOption('D')) {\n       String[] property = line.getOptionValues('D');\n       for(String prop : property) {\n-        String[] keyval = prop.split(\"=\");\n+        String[] keyval = prop.split(\"=\", 2);\n         if (keyval.length == 2) {\n           conf.set(keyval[0], keyval[1]);\n         }"
    },
    {
        "commit_id": "244380a885f5cd009fa16cb9a6774337eab52e1d",
        "commit_message": "Fix core test failrues \n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/core/branches/HADOOP-4687/core@780977 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/244380a885f5cd009fa16cb9a6774337eab52e1d",
        "buggy_code": "HttpServer http = new HttpServer(\"datanode\", \"localhost\", 0, true, conf);",
        "fixed_code": "HttpServer http = new HttpServer(\"..\", \"localhost\", 0, true, conf);",
        "patch": "@@ -101,7 +101,7 @@ public void testServletFilter() throws Exception {\n     //start a http server with CountingFilter\n     conf.set(HttpServer.FILTER_INITIALIZER_PROPERTY,\n         RecordingFilter.Initializer.class.getName());\n-    HttpServer http = new HttpServer(\"datanode\", \"localhost\", 0, true, conf);\n+    HttpServer http = new HttpServer(\"..\", \"localhost\", 0, true, conf);\n     http.start();\n \n     final String fsckURL = \"/fsck\";"
    },
    {
        "commit_id": "244380a885f5cd009fa16cb9a6774337eab52e1d",
        "commit_message": "Fix core test failrues \n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/core/branches/HADOOP-4687/core@780977 13f79535-47bb-0310-9956-ffa450edef68",
        "commit_url": "https://github.com/apache/hadoop/commit/244380a885f5cd009fa16cb9a6774337eab52e1d",
        "buggy_code": "HttpServer http = new HttpServer(\"datanode\", \"localhost\", 0, true, conf);",
        "fixed_code": "HttpServer http = new HttpServer(\"..\", \"localhost\", 0, true, conf);",
        "patch": "@@ -99,7 +99,7 @@ public void testServletFilter() throws Exception {\n     //start a http server with CountingFilter\n     conf.set(HttpServer.FILTER_INITIALIZER_PROPERTY,\n         SimpleFilter.Initializer.class.getName());\n-    HttpServer http = new HttpServer(\"datanode\", \"localhost\", 0, true, conf);\n+    HttpServer http = new HttpServer(\"..\", \"localhost\", 0, true, conf);\n     http.start();\n \n     final String fsckURL = \"/fsck\";"
    }
]